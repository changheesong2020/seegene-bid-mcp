"""
Seegene Bid Information MCP Server
FastMCPë¥¼ ì‚¬ìš©í•œ ë©”ì¸ ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import asyncio
from datetime import datetime
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

try:
    from fastmcp import FastMCP
except ImportError:
    print("FastMCPê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install fastmcpë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    FastMCP = None

from src.config import settings
from src.database.connection import init_database, DatabaseManager
from src.models.filters import BidFilter
from src.models.bid_info import BidInfo
from src.models.crawler_api import (
    CrawlerRequest, CrawlerExecutionResponse, AllCrawlerExecutionResponse,
    CrawlerResultsResponse, SiteCrawlerResultResponse, ScheduledJobsResponse,
    ScheduleRequest, ScheduleResponse, BidListResponse, BidDetailResponse,
    BidSearchResponse, BidStatisticsResponse
)
from src.models.advanced_filters import (
    AdvancedBidSearchRequest, AdvancedSearchResponse, KeywordSuggestionsResponse,
    KeywordExpansion, AdvancedSearchQuery, KeywordGroup
)
from src.services.advanced_search import advanced_search_service
from src.services.site_compliance import list_site_compliance, get_site_compliance
from src.utils.keyword_expansion import keyword_engine
from src.utils.logger import get_logger
from src.crawler.manager import crawler_manager
from src.models.site_compliance import (
    SiteComplianceListResponse,
    SiteComplianceResponse,
)

logger = get_logger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    try:
        await init_database()
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        # í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ì„¤ì •ì— ë”°ë¼)
        if settings.ENABLE_SCHEDULER:
            await crawler_manager.start_scheduler()
            logger.info("âœ… í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ")
        else:
            logger.info("â¸ï¸ í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ë¹„í™œì„±í™”ë¨ (ENABLE_SCHEDULER=False)")
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    yield

    # ì¢…ë£Œ ì‹œ - ì•ˆì „í•œ ì¢…ë£Œ
    try:
        if settings.ENABLE_SCHEDULER:
            await crawler_manager.stop_scheduler()
        logger.info("ğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
    except Exception as e:
        logger.warning(f"ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë¦¬
    try:
        from src.database.connection import async_engine
        if async_engine:
            await async_engine.dispose()
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Seegene Bid Information MCP Server",
    description="ì”¨ì  ì„ ìœ„í•œ ê¸€ë¡œë²Œ ì…ì°° ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œìŠ¤í…œ",
    version="2.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì • - MCP ë° Copilot Studio í˜¸í™˜
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "*",  # ê°œë°œìš©
        "https://copilotstudio.microsoft.com",
        "https://make.powerplatform.com",
        "https://apps.powerapps.com",
        "https://flow.microsoft.com",
        "http://localhost:*",
        "https://localhost:*"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD"],
    allow_headers=[
        "*",
        "Content-Type",
        "Authorization",
        "Accept",
        "Origin",
        "X-Requested-With",
        "Access-Control-Request-Method",
        "Access-Control-Request-Headers"
    ],
    expose_headers=["*"]
)

# ì¶”ê°€ CORS ë¯¸ë“¤ì›¨ì–´ (MCP ì „ìš©)
@app.middleware("http")
async def add_cors_headers(request: Request, call_next):
    """MCP ìš”ì²­ì„ ìœ„í•œ ì¶”ê°€ CORS í—¤ë”"""
    response = await call_next(request)

    # MCP ê´€ë ¨ ê²½ë¡œì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
    if request.url.path.startswith("/mcp") or request.url.path in ["/api/mcp", "/v1/mcp", "/jsonrpc"]:
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS, HEAD"
        response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization, Accept, Origin, X-Requested-With"
        response.headers["Access-Control-Expose-Headers"] = "*"
        response.headers["Access-Control-Allow-Credentials"] = "true"

        # OPTIONS ìš”ì²­ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
        if request.method == "OPTIONS":
            response.headers["Access-Control-Max-Age"] = "86400"

    return response

# FastMCP ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
if FastMCP:
    mcp = FastMCP("Seegene Bid Information Server")
    
    @mcp.tool()
    async def search_bids(
        keywords: List[str],
        days_range: int = 7,
        countries: List[str] = ["KR", "US"],
        limit: int = 50
    ) -> Dict[str, Any]:
        """ì…ì°° ì •ë³´ ê²€ìƒ‰"""
        try:
            logger.info(f"ì…ì°° ì •ë³´ ê²€ìƒ‰: í‚¤ì›Œë“œ={keywords}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰
            results = await DatabaseManager.search_bids(keywords, limit)
            
            # ê²°ê³¼ ë³€í™˜
            bid_list = []
            for bid in results:
                bid_dict = {
                    "id": getattr(bid, 'id', 0),
                    "title": getattr(bid, 'title', ''),
                    "organization": getattr(bid, 'organization', ''),
                    "source_site": getattr(bid, 'source_site', ''),
                    "country": getattr(bid, 'country', ''),
                    "relevance_score": getattr(bid, 'relevance_score', 0.0),
                    "source_url": getattr(bid, 'source_url', '')
                }
                bid_list.append(bid_dict)
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(bid_list)}ê±´ ë°œê²¬")
            
            return {
                "success": True,
                "total_found": len(bid_list),
                "results": bid_list
            }
            
        except Exception as e:
            logger.error(f"ì…ì°° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }

    @mcp.tool()
    async def get_database_stats() -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            stats = await DatabaseManager.get_database_stats()
            
            return {
                "success": True,
                "database_statistics": stats,
                "last_updated": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def run_crawler(
        site_name: str,
        keywords: List[str] = None
    ) -> Dict[str, Any]:
        """íŠ¹ì • ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            logger.info(f"í¬ë¡¤ë§ ì‹¤í–‰: {site_name}")

            result = await crawler_manager.run_crawler(site_name, keywords)

            return {
                "success": True,
                "crawler_result": result
            }

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def run_all_crawlers(
        keywords: List[str] = None
    ) -> Dict[str, Any]:
        """ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            logger.info("ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰")

            result = await crawler_manager.run_all_crawlers(keywords)

            return {
                "success": True,
                "crawling_result": result
            }

        except Exception as e:
            logger.error(f"ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def get_crawler_status() -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            status = crawler_manager.get_crawler_status()

            return {
                "success": True,
                "crawler_status": status
            }

        except Exception as e:
            logger.error(f"í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def get_scheduled_jobs() -> Dict[str, Any]:
        """ì˜ˆì•½ëœ í¬ë¡¤ë§ ì‘ì—… ì¡°íšŒ"""
        try:
            jobs = crawler_manager.get_scheduled_jobs()

            return {
                "success": True,
                "scheduled_jobs": jobs
            }

        except Exception as e:
            logger.error(f"ì˜ˆì•½ëœ ì‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def add_crawler_schedule(
        site_name: str,
        cron_expression: str,
        job_id: str = None
    ) -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ ì¶”ê°€"""
        try:
            success = await crawler_manager.add_custom_schedule(site_name, cron_expression, job_id)

            return {
                "success": success,
                "message": "ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì„±ê³µ" if success else "ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì‹¤íŒ¨"
            }

        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def advanced_search_bids(
        keywords: List[str],
        countries: List[str] = None,
        enable_expansion: bool = True,
        min_relevance: float = None,
        limit: int = 50
    ) -> Dict[str, Any]:
        """ê³ ê¸‰ í‚¤ì›Œë“œ í™•ì¥ ê²€ìƒ‰"""
        try:
            logger.info(f"MCP ê³ ê¸‰ ê²€ìƒ‰: í‚¤ì›Œë“œ={keywords}")

            # í™•ì¥ ì„¤ì •
            expansion_config = None
            if enable_expansion:
                expansion_config = KeywordExpansion(
                    enable_synonyms=True,
                    enable_related_terms=True,
                    enable_translations=True,
                    enable_abbreviations=True
                )

            # í‚¤ì›Œë“œ ê·¸ë£¹ ìƒì„±
            keyword_groups = [KeywordGroup(
                keywords=keywords,
                operator="or",
                weight=1.0
            )]

            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = AdvancedSearchQuery(
                keyword_groups=keyword_groups,
                countries=countries or ["KR", "US"],
                min_relevance_score=min_relevance,
                limit=limit
            )

            # ìš”ì²­ ìƒì„± ë° ê²€ìƒ‰
            request = AdvancedBidSearchRequest(
                query=search_query,
                expansion=expansion_config,
                include_metadata=True,
                explain_relevance=True
            )

            result = await advanced_search_service.search_bids(request)

            return {
                "success": True,
                "search_result": {
                    "total_found": result.total_found,
                    "search_time": result.search_time,
                    "query_summary": result.query_summary,
                    "filters_applied": result.filters_applied,
                    "results": [
                        {
                            "title": r.title,
                            "organization": r.organization,
                            "country": r.country,
                            "relevance_score": r.relevance_score,
                            "matched_keywords": r.matched_keywords,
                            "source_url": r.source_url,
                            "urgency_level": r.urgency_level
                        } for r in result.results[:10]  # ì²˜ìŒ 10ê°œë§Œ
                    ]
                }
            }

        except Exception as e:
            logger.error(f"MCP ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def get_keyword_suggestions(
        keywords: List[str],
        max_suggestions: int = 10
    ) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ì œì•ˆ ë°›ê¸°"""
        try:
            suggestions = keyword_engine.get_keyword_suggestions(keywords, max_suggestions)

            return {
                "success": True,
                "original_keywords": keywords,
                "suggestions": [
                    {
                        "keyword": s.keyword,
                        "relevance": s.relevance,
                        "source": s.source
                    } for s in suggestions
                ]
            }

        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì œì•ˆ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def reset_database() -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ ì´ˆê¸°í™” (ëª¨ë“  ë°ì´í„° ì‚­ì œ)"""
        try:
            from src.database.connection import get_db_session, BidInfoModel
            from sqlalchemy import delete

            async with get_db_session() as session:
                # ëª¨ë“  ì…ì°° ì •ë³´ ì‚­ì œ
                result = await session.execute(delete(BidInfoModel))
                deleted_count = result.rowcount
                await session.commit()

                logger.info(f"MCP ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {deleted_count}ê±´ ì‚­ì œ")

                return {
                    "success": True,
                    "message": "ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤",
                    "deleted_records": deleted_count,
                    "timestamp": datetime.now().isoformat()
                }

        except Exception as e:
            logger.error(f"MCP ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def clean_dummy_data() -> Dict[str, Any]:
        """ë”ë¯¸ ë°ì´í„°ë§Œ ì‚­ì œ"""
        try:
            from src.database.connection import get_db_session, BidInfoModel
            from sqlalchemy import delete, select

            async with get_db_session() as session:
                # ë”ë¯¸ ë°ì´í„° í™•ì¸
                dummy_result = await session.execute(
                    select(BidInfoModel).where(
                        BidInfoModel.extra_data.contains('"dummy": true') |
                        BidInfoModel.bid_number.like('%DUMMY%') |
                        BidInfoModel.bid_number.like('%TEST%')
                    )
                )
                dummy_bids = dummy_result.scalars().all()
                dummy_count = len(dummy_bids)

                if dummy_count > 0:
                    # ë”ë¯¸ ë°ì´í„° ì‚­ì œ
                    await session.execute(
                        delete(BidInfoModel).where(
                            BidInfoModel.extra_data.contains('"dummy": true') |
                            BidInfoModel.bid_number.like('%DUMMY%') |
                            BidInfoModel.bid_number.like('%TEST%')
                        )
                    )
                    await session.commit()

                logger.info(f"MCP ë”ë¯¸ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {dummy_count}ê±´ ì‚­ì œ")

                return {
                    "success": True,
                    "message": "ë”ë¯¸ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤",
                    "deleted_dummy_records": dummy_count,
                    "timestamp": datetime.now().isoformat()
                }

        except Exception as e:
            logger.error(f"MCP ë”ë¯¸ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def get_database_info() -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ë° í†µê³„ ì¡°íšŒ"""
        try:
            from src.database.connection import get_db_session, BidInfoModel
            from sqlalchemy import select, func

            async with get_db_session() as session:
                # ì „ì²´ ë ˆì½”ë“œ ìˆ˜
                total_result = await session.execute(select(func.count(BidInfoModel.id)))
                total_count = total_result.scalar()

                # ì†ŒìŠ¤ë³„ í†µê³„
                source_result = await session.execute(
                    select(BidInfoModel.source_site, func.count(BidInfoModel.id))
                    .group_by(BidInfoModel.source_site)
                )
                source_stats = {site: count for site, count in source_result.fetchall()}

                # ë”ë¯¸ ë°ì´í„° ìˆ˜
                dummy_result = await session.execute(
                    select(func.count(BidInfoModel.id)).where(
                        BidInfoModel.extra_data.contains('"dummy": true') |
                        BidInfoModel.bid_number.like('%DUMMY%') |
                        BidInfoModel.bid_number.like('%TEST%')
                    )
                )
                dummy_count = dummy_result.scalar()

                # ìµœê·¼ ë°ì´í„°
                recent_result = await session.execute(
                    select(BidInfoModel.created_at, BidInfoModel.source_site, BidInfoModel.title)
                    .order_by(BidInfoModel.created_at.desc())
                    .limit(5)
                )
                recent_data = [
                    {
                        "created_at": row[0].isoformat() if row[0] else None,
                        "source": row[1],
                        "title": row[2][:50] + "..." if len(row[2]) > 50 else row[2]
                    }
                    for row in recent_result.fetchall()
                ]

                return {
                    "success": True,
                    "database_info": {
                        "total_records": total_count,
                        "dummy_records": dummy_count,
                        "real_records": total_count - dummy_count,
                        "source_breakdown": source_stats,
                        "recent_entries": recent_data
                    },
                    "timestamp": datetime.now().isoformat()
                }

        except Exception as e:
            logger.error(f"MCP ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    # MCP ì„œë²„ëŠ” ì•± ì„¤ì • í›„ ë§ˆìš´íŠ¸ë¨ (ì•„ë˜ì—ì„œ)

# OPTIONS ìš”ì²­ ì²˜ë¦¬ (CORS preflight)
@app.options("/mcp")
@app.options("/mcp/")
@app.options("/api/mcp")
@app.options("/v1/mcp")
@app.options("/jsonrpc")
async def mcp_options():
    """CORS preflight ìš”ì²­ ì²˜ë¦¬"""
    return {"status": "ok"}

# ì¼ë°˜ì ì¸ MCP ê²½ë¡œë“¤ì— ëŒ€í•œ ì •ë³´ ì œê³µ
@app.get("/api/mcp")
@app.get("/v1/mcp")
@app.get("/jsonrpc")
async def mcp_alternative_paths():
    """ë‹¤ë¥¸ MCP ê²½ë¡œë“¤ì— ëŒ€í•œ ì•ˆë‚´"""
    return {
        "message": "MCP endpoint is available at /mcp",
        "correct_endpoint": "/mcp",
        "server_info": {
            "name": "Seegene Bid Information Server",
            "protocol": "MCP via FastMCP",
            "transport": "SSE"
        }
    }

# FastAPI ë¼ìš°íŠ¸
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Seegene Bid Information MCP Server",
        "version": "2.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "docs": "/docs",
            "mcp": "/mcp" if FastMCP else "Not available (install fastmcp)",
            "crawler_apis": {
                "run_single": "POST /crawl/{site_name}",
                "run_all": "POST /crawl-all",
                "get_results": "GET /crawl-results",
                "get_site_results": "GET /crawl-results/{site_name}",
                "scheduled_jobs": "GET /scheduled-jobs",
                "add_schedule": "POST /schedule-crawler",
                "remove_schedule": "DELETE /schedule-crawler/{job_id}"
            },
            "advanced_search_apis": {
                "advanced_search": "POST /search/advanced",
                "keyword_suggestions": "GET /search/keyword-suggestions",
                "search_with_expansion": "POST /search/expanded"
            },
            "bid_data_apis": {
                "get_all_bids": "GET /bids",
                "get_bid_by_id": "GET /bids/{bid_id}",
                "search_bids": "GET /bids/search",
                "get_statistics": "GET /bids/stats"
            }
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        stats = await DatabaseManager.get_database_stats()
        db_status = "ok"
    except:
        db_status = "error"
    
    return {
        "status": "healthy" if db_status == "ok" else "degraded",
        "timestamp": datetime.now().isoformat(),
        "database": db_status,
        "version": "2.0.0"
    }


@app.get("/compliance/sites", response_model=SiteComplianceListResponse)
async def get_site_compliance_catalog():
    """ì§€ì› ëŒ€ìƒ ì¡°ë‹¬ ì‚¬ì´íŠ¸ì˜ í¬ë¡¤ë§ ë° ë²•ì  ìœ ì˜ì‚¬í•­ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""

    entries = list_site_compliance()
    return {
        "success": True,
        "total": len(entries),
        "data": entries,
    }


@app.get("/compliance/sites/{slug}", response_model=SiteComplianceResponse)
async def get_site_compliance_detail(slug: str):
    """íŠ¹ì • ì¡°ë‹¬ ì‚¬ì´íŠ¸ì˜ í¬ë¡¤ë§ ê°€ì´ë“œë¼ì¸ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""

    entry = get_site_compliance(slug)
    if not entry:
        raise HTTPException(status_code=404, detail="Site compliance entry not found")

    return {
        "success": True,
        "data": entry,
    }

@app.get("/crawler-status")
async def crawler_status_endpoint():
    """í¬ë¡¤ëŸ¬ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        status = crawler_manager.get_crawler_status()
        return status
    except Exception as e:
        logger.error(f"í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "error": str(e),
            "scheduler_running": False,
            "crawlers": {}
        }

@app.post("/crawl/{site_name}", response_model=CrawlerExecutionResponse)
async def run_single_crawler(site_name: str, request: CrawlerRequest = None):
    """íŠ¹ì • ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        logger.info(f"ìˆ˜ë™ í¬ë¡¤ë§ ì‹¤í–‰ ìš”ì²­: {site_name}")
        logger.info(f"ğŸ” ìš”ì²­ ê°ì²´: {request}")

        keywords = request.keywords if request else None
        logger.info(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
        result = await crawler_manager.run_crawler(site_name, keywords)

        return CrawlerExecutionResponse(
            success=True,
            message=f"{site_name} í¬ë¡¤ë§ ì™„ë£Œ",
            result=result
        )

    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/crawl-all", response_model=AllCrawlerExecutionResponse)
async def run_all_crawlers_endpoint(request: CrawlerRequest = None):
    """ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        logger.info("ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰ ìš”ì²­")

        keywords = request.keywords if request else None
        result = await crawler_manager.run_all_crawlers(keywords)

        return AllCrawlerExecutionResponse(
            success=True,
            message="ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ",
            result=result
        )

    except Exception as e:
        logger.error(f"ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/crawl-results", response_model=CrawlerResultsResponse)
async def get_crawler_results():
    """ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    try:
        results = crawler_manager.last_run_results

        return CrawlerResultsResponse(
            success=True,
            last_run_results=results,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/crawl-results/{site_name}", response_model=SiteCrawlerResultResponse)
async def get_site_crawler_results(site_name: str):
    """íŠ¹ì • ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    try:
        if site_name not in crawler_manager.last_run_results:
            raise HTTPException(status_code=404, detail=f"{site_name}ì˜ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        result = crawler_manager.last_run_results[site_name]

        return SiteCrawlerResultResponse(
            success=True,
            site=site_name,
            result=result
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/scheduled-jobs", response_model=ScheduledJobsResponse)
async def get_scheduled_jobs_endpoint():
    """ì˜ˆì•½ëœ í¬ë¡¤ë§ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        jobs = crawler_manager.get_scheduled_jobs()

        return ScheduledJobsResponse(
            success=True,
            scheduled_jobs=jobs,
            scheduler_running=crawler_manager.is_running
        )

    except Exception as e:
        logger.error(f"ì˜ˆì•½ëœ ì‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/schedule-crawler", response_model=ScheduleResponse)
async def add_crawler_schedule_endpoint(request: ScheduleRequest):
    """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ ì¶”ê°€"""
    try:
        if request.site_name not in ["G2B", "SAM.gov"]:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤. G2B ë˜ëŠ” SAM.govë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        success = await crawler_manager.add_custom_schedule(
            request.site_name,
            request.cron_expression,
            request.job_id
        )

        if success:
            return ScheduleResponse(
                success=True,
                message=f"{request.site_name} ìŠ¤ì¼€ì¤„ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤",
                site_name=request.site_name,
                cron_expression=request.cron_expression,
                job_id=request.job_id
            )
        else:
            raise HTTPException(status_code=400, detail="ìŠ¤ì¼€ì¤„ ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/schedule-crawler/{job_id}", response_model=ScheduleResponse)
async def remove_crawler_schedule_endpoint(job_id: str):
    """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ ì œê±°"""
    try:
        success = crawler_manager.remove_scheduled_job(job_id)

        if success:
            return ScheduleResponse(
                success=True,
                message=f"ìŠ¤ì¼€ì¤„ ì‘ì—… '{job_id}'ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤",
                job_id=job_id
            )
        else:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ ì œê±° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search/advanced", response_model=AdvancedSearchResponse)
async def advanced_search_endpoint(request: AdvancedBidSearchRequest):
    """ê³ ê¸‰ ì…ì°° ê²€ìƒ‰"""
    try:
        logger.info("ê³ ê¸‰ ê²€ìƒ‰ ìš”ì²­")

        result = await advanced_search_service.search_bids(request)

        return result

    except Exception as e:
        logger.error(f"ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/keyword-suggestions", response_model=KeywordSuggestionsResponse)
async def get_keyword_suggestions(
    keywords: str,
    max_suggestions: int = 20
):
    """í‚¤ì›Œë“œ ì œì•ˆ"""
    try:
        keyword_list = [k.strip() for k in keywords.split(",")]
        logger.info(f"í‚¤ì›Œë“œ ì œì•ˆ ìš”ì²­: {keyword_list}")

        suggestions = keyword_engine.get_keyword_suggestions(keyword_list, max_suggestions)

        return KeywordSuggestionsResponse(
            success=True,
            original_keywords=keyword_list,
            suggestions=suggestions,
            total_suggestions=len(suggestions)
        )

    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì œì•ˆ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search/expanded", response_model=AdvancedSearchResponse)
async def search_with_keyword_expansion(
    keywords: List[str],
    expansion_config: KeywordExpansion = None,
    countries: List[str] = None,
    limit: int = 50
):
    """í‚¤ì›Œë“œ í™•ì¥ì„ í¬í•¨í•œ ê°„ë‹¨í•œ ê²€ìƒ‰"""
    try:
        logger.info(f"í™•ì¥ ê²€ìƒ‰ ìš”ì²­: í‚¤ì›Œë“œ={keywords}")

        # ê¸°ë³¸ í™•ì¥ ì„¤ì •
        if not expansion_config:
            expansion_config = KeywordExpansion()

        # í‚¤ì›Œë“œ ê·¸ë£¹ ìƒì„±
        keyword_groups = [KeywordGroup(
            keywords=keywords,
            operator="or",
            weight=1.0
        )]

        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = AdvancedSearchQuery(
            keyword_groups=keyword_groups,
            countries=countries,
            limit=limit
        )

        # ìš”ì²­ ê°ì²´ ìƒì„±
        request = AdvancedBidSearchRequest(
            query=search_query,
            expansion=expansion_config,
            include_metadata=True,
            explain_relevance=True
        )

        # ê²€ìƒ‰ ì‹¤í–‰
        result = await advanced_search_service.search_bids(request)

        return result

    except Exception as e:
        logger.error(f"í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/filters-help")
async def get_filters_help():
    """ê³ ê¸‰ í•„í„°ë§ ë„ì›€ë§"""
    return {
        "available_operators": [
            "eq (ê°™ìŒ)", "ne (ë‹¤ë¦„)", "gt (ì´ˆê³¼)", "lt (ë¯¸ë§Œ)",
            "gte (ì´ìƒ)", "lte (ì´í•˜)", "in (í¬í•¨)", "not_in (ì œì™¸)",
            "contains (ë¬¸ìì—´ í¬í•¨)", "starts_with (ì‹œì‘)", "ends_with (ëë‚¨)"
        ],
        "available_fields": [
            "title", "organization", "country", "source_site",
            "relevance_score", "urgency_level", "currency",
            "announcement_date", "deadline_date"
        ],
        "relevance_levels": ["low (1-3ì )", "medium (4-6ì )", "high (7-8ì )", "very_high (9-10ì )"],
        "urgency_levels": ["low", "medium", "high", "urgent"],
        "sort_options": ["relevance", "date", "announcement_date", "deadline_date", "price", "urgency"],
        "expansion_features": [
            "synonyms (ë™ì˜ì–´)", "related_terms (ê´€ë ¨ìš©ì–´)",
            "translations (ë²ˆì—­)", "abbreviations (ì•½ì–´)"
        ]
    }

@app.get("/bids", response_model=BidListResponse)
async def get_all_bids(
    limit: int = 50,
    offset: int = 0,
    site: str = None,
    country: str = None,
    min_relevance: float = None
):
    """ì €ì¥ëœ ì…ì°° ì •ë³´ ëª©ë¡ ì¡°íšŒ"""
    try:
        from src.database.connection import get_db_session, BidInfoModel
        from sqlalchemy import select, desc

        async with get_db_session() as session:
            # ì¿¼ë¦¬ ë¹Œë“œ
            query = select(BidInfoModel)

            # í•„í„° ì ìš©
            if site:
                query = query.where(BidInfoModel.source_site == site)
            if country:
                query = query.where(BidInfoModel.country == country)
            if min_relevance:
                query = query.where(BidInfoModel.relevance_score >= min_relevance)

            # ì •ë ¬ ë° í˜ì´ì§€ë„¤ì´ì…˜
            query = query.order_by(desc(BidInfoModel.created_at)).offset(offset).limit(limit)

            # ì‹¤í–‰
            result = await session.execute(query)
            bids = result.scalars().all()

            # ê²°ê³¼ ë³€í™˜
            bid_list = []
            for bid in bids:
                # ì•ˆì „í•œ ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
                def safe_date_format(date_value):
                    if date_value is None:
                        return None
                    if hasattr(date_value, 'isoformat'):
                        return date_value.isoformat()
                    elif isinstance(date_value, str):
                        return date_value
                    else:
                        return str(date_value)

                bid_dict = {
                    "id": bid.id,
                    "title": bid.title,
                    "organization": bid.organization,
                    "bid_number": bid.bid_number,
                    "announcement_date": safe_date_format(bid.announcement_date),
                    "deadline_date": safe_date_format(bid.deadline_date),
                    "estimated_price": bid.estimated_price,
                    "currency": bid.currency,
                    "source_url": bid.source_url,
                    "source_site": bid.source_site,
                    "country": bid.country,
                    "relevance_score": bid.relevance_score,
                    "urgency_level": bid.urgency_level,
                    "status": bid.status,
                    "keywords": bid.keywords,
                    "created_at": safe_date_format(bid.created_at)
                }
                bid_list.append(bid_dict)

            # ì´ ê°œìˆ˜ ì¡°íšŒ
            count_query = select(BidInfoModel)
            if site:
                count_query = count_query.where(BidInfoModel.source_site == site)
            if country:
                count_query = count_query.where(BidInfoModel.country == country)
            if min_relevance:
                count_query = count_query.where(BidInfoModel.relevance_score >= min_relevance)

            from sqlalchemy import func
            count_result = await session.execute(select(func.count()).select_from(count_query.subquery()))
            total_count = count_result.scalar()

            return {
                "success": True,
                "data": bid_list,
                "pagination": {
                    "total": total_count,
                    "limit": limit,
                    "offset": offset,
                    "has_next": offset + limit < total_count
                },
                "filters": {
                    "site": site,
                    "country": country,
                    "min_relevance": min_relevance
                }
            }

    except Exception as e:
        logger.error(f"ì…ì°° ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/bids/{bid_id}", response_model=BidDetailResponse)
async def get_bid_by_id(bid_id: int):
    """íŠ¹ì • ì…ì°° ì •ë³´ ìƒì„¸ ì¡°íšŒ"""
    try:
        from src.database.connection import get_db_session, BidInfoModel
        from sqlalchemy import select

        async with get_db_session() as session:
            result = await session.execute(select(BidInfoModel).where(BidInfoModel.id == bid_id))
            bid = result.scalar_one_or_none()

            if not bid:
                raise HTTPException(status_code=404, detail="í•´ë‹¹ ì…ì°° ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ì•ˆì „í•œ ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
            def safe_date_format(date_value):
                if date_value is None:
                    return None
                if hasattr(date_value, 'isoformat'):
                    return date_value.isoformat()
                elif isinstance(date_value, str):
                    return date_value
                else:
                    return str(date_value)

            bid_detail = {
                "id": bid.id,
                "title": bid.title,
                "organization": bid.organization,
                "bid_number": bid.bid_number,
                "announcement_date": safe_date_format(bid.announcement_date),
                "deadline_date": safe_date_format(bid.deadline_date),
                "estimated_price": bid.estimated_price,
                "currency": bid.currency,
                "source_url": bid.source_url,
                "source_site": bid.source_site,
                "country": bid.country,
                "relevance_score": bid.relevance_score,
                "urgency_level": bid.urgency_level,
                "status": bid.status,
                "keywords": bid.keywords,
                "extra_data": bid.extra_data,
                "created_at": safe_date_format(bid.created_at),
                "updated_at": safe_date_format(bid.updated_at)
            }

            return {
                "success": True,
                "data": bid_detail
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì…ì°° ì •ë³´ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/bids/search", response_model=BidSearchResponse)
async def search_bids_endpoint(
    q: str,
    limit: int = 50,
    offset: int = 0,
    site: str = None,
    country: str = None
):
    """ì…ì°° ì •ë³´ ê²€ìƒ‰"""
    try:
        keywords = [k.strip() for k in q.split(",") if k.strip()]
        logger.info(f"ì…ì°° ê²€ìƒ‰ ìš”ì²­: í‚¤ì›Œë“œ={keywords}")

        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰
        results = await DatabaseManager.search_bids(keywords, limit + offset)

        # ì˜¤í”„ì…‹ ì ìš© (ê°„ë‹¨í•œ êµ¬í˜„)
        paginated_results = results[offset:offset + limit] if results else []

        # ì¶”ê°€ í•„í„° ì ìš©
        if site:
            paginated_results = [r for r in paginated_results if getattr(r, 'source_site', '') == site]
        if country:
            paginated_results = [r for r in paginated_results if getattr(r, 'country', '') == country]

        # ê²°ê³¼ ë³€í™˜
        bid_list = []
        for bid in paginated_results:
            bid_dict = {
                "id": getattr(bid, 'id', 0),
                "title": getattr(bid, 'title', ''),
                "organization": getattr(bid, 'organization', ''),
                "source_site": getattr(bid, 'source_site', ''),
                "country": getattr(bid, 'country', ''),
                "relevance_score": getattr(bid, 'relevance_score', 0.0),
                "source_url": getattr(bid, 'source_url', ''),
                "estimated_price": getattr(bid, 'estimated_price', ''),
                "deadline_date": getattr(bid, 'deadline_date', None),
                "urgency_level": getattr(bid, 'urgency_level', 'low')
            }
            bid_list.append(bid_dict)

        return {
            "success": True,
            "query": q,
            "keywords": keywords,
            "data": bid_list,
            "pagination": {
                "total": len(results),
                "limit": limit,
                "offset": offset,
                "returned": len(bid_list)
            }
        }

    except Exception as e:
        logger.error(f"ì…ì°° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/bids/stats", response_model=BidStatisticsResponse)
async def get_bid_statistics():
    """ì…ì°° ì •ë³´ í†µê³„"""
    try:
        stats = await DatabaseManager.get_database_stats()

        return {
            "success": True,
            "statistics": stats,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/test/db")
async def test_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    from src.database.connection import get_db_session, BidInfoModel
    from sqlalchemy import select

    try:
        async with get_db_session() as session:
            # ê°„ë‹¨í•œ select ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
            result = await session.execute(select(BidInfoModel).limit(1))
            bids = result.scalars().all()
            return {
                "success": True,
                "message": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ",
                "found_bids": len(bids)
            }
    except Exception as e:
        return {
            "success": False,
            "message": f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}",
            "error": str(e)
        }

@app.post("/admin/reset-database")
async def reset_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ëª¨ë“  ë°ì´í„° ì‚­ì œ)"""
    try:
        from src.database.connection import get_db_session, BidInfoModel
        from sqlalchemy import delete

        async with get_db_session() as session:
            # ëª¨ë“  ì…ì°° ì •ë³´ ì‚­ì œ
            result = await session.execute(delete(BidInfoModel))
            deleted_count = result.rowcount
            await session.commit()

            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {deleted_count}ê±´ ì‚­ì œ")

            return {
                "success": True,
                "message": "ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤",
                "deleted_records": deleted_count,
                "timestamp": datetime.now().isoformat()
            }

    except Exception as e:
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "message": f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}",
            "error": str(e)
        }

@app.post("/admin/clean-dummy-data")
async def clean_dummy_data():
    """ë”ë¯¸ ë°ì´í„°ë§Œ ì‚­ì œ"""
    try:
        from src.database.connection import get_db_session, BidInfoModel
        from sqlalchemy import delete, select

        async with get_db_session() as session:
            # ë”ë¯¸ ë°ì´í„° í™•ì¸
            dummy_result = await session.execute(
                select(BidInfoModel).where(
                    BidInfoModel.extra_data.contains('"dummy": true') |
                    BidInfoModel.bid_number.like('%DUMMY%') |
                    BidInfoModel.bid_number.like('%TEST%')
                )
            )
            dummy_bids = dummy_result.scalars().all()
            dummy_count = len(dummy_bids)

            if dummy_count > 0:
                # ë”ë¯¸ ë°ì´í„° ì‚­ì œ
                await session.execute(
                    delete(BidInfoModel).where(
                        BidInfoModel.extra_data.contains('"dummy": true') |
                        BidInfoModel.bid_number.like('%DUMMY%') |
                        BidInfoModel.bid_number.like('%TEST%')
                    )
                )
                await session.commit()

            logger.info(f"ë”ë¯¸ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {dummy_count}ê±´ ì‚­ì œ")

            return {
                "success": True,
                "message": "ë”ë¯¸ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤",
                "deleted_dummy_records": dummy_count,
                "timestamp": datetime.now().isoformat()
            }

    except Exception as e:
        logger.error(f"ë”ë¯¸ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "message": f"ë”ë¯¸ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}",
            "error": str(e)
        }

@app.get("/admin/database-info")
async def get_database_info():
    """ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        from src.database.connection import get_db_session, BidInfoModel
        from sqlalchemy import select, func

        async with get_db_session() as session:
            # ì „ì²´ ë ˆì½”ë“œ ìˆ˜
            total_result = await session.execute(select(func.count(BidInfoModel.id)))
            total_count = total_result.scalar()

            # ì†ŒìŠ¤ë³„ í†µê³„
            source_result = await session.execute(
                select(BidInfoModel.source_site, func.count(BidInfoModel.id))
                .group_by(BidInfoModel.source_site)
            )
            source_stats = {site: count for site, count in source_result.fetchall()}

            # ë”ë¯¸ ë°ì´í„° ìˆ˜
            dummy_result = await session.execute(
                select(func.count(BidInfoModel.id)).where(
                    BidInfoModel.extra_data.contains('"dummy": true') |
                    BidInfoModel.bid_number.like('%DUMMY%') |
                    BidInfoModel.bid_number.like('%TEST%')
                )
            )
            dummy_count = dummy_result.scalar()

            # ìµœê·¼ ë°ì´í„°
            recent_result = await session.execute(
                select(BidInfoModel.created_at, BidInfoModel.source_site)
                .order_by(BidInfoModel.created_at.desc())
                .limit(5)
            )
            recent_data = [
                {"created_at": row[0].isoformat() if row[0] else None, "source": row[1]}
                for row in recent_result.fetchall()
            ]

            return {
                "success": True,
                "database_info": {
                    "total_records": total_count,
                    "dummy_records": dummy_count,
                    "real_records": total_count - dummy_count,
                    "source_breakdown": source_stats,
                    "recent_entries": recent_data
                },
                "timestamp": datetime.now().isoformat()
            }

    except Exception as e:
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "message": f"ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}",
            "error": str(e)
        }

# MCP ì„œë²„ ë§ˆìš´íŠ¸ (FastAPI ì„¤ì • ì™„ë£Œ í›„)
if FastMCP:
    # MCP ìƒíƒœ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë¨¼ì € ì •ì˜
    @app.get("/mcp-status")
    async def mcp_status():
        """MCP ì„œë²„ ìƒíƒœ í™•ì¸"""
        return {
            "status": "active",
            "protocol": "MCP (Model Context Protocol)",
            "transport": "SSE (Server-Sent Events)",
            "server_name": "Seegene Bid Information Server",
            "tools_count": len(mcp._tools) if hasattr(mcp, '_tools') else 0,
            "note": "âš ï¸ SSE transport deprecated after Aug 2025"
        }

    # MCP ì„œë²„ ë§ˆìš´íŠ¸ (ë¼ìš°íŠ¸ ì •ì˜ í›„)
    app.mount("/mcp", mcp.sse_app())

if __name__ == "__main__":
    import uvicorn
    import os

    # SSL ì„¤ì •
    ssl_config = {}
    if settings.SSL_ENABLED:
        cert_path = os.path.join(os.getcwd(), settings.SSL_CERTFILE)
        key_path = os.path.join(os.getcwd(), settings.SSL_KEYFILE)

        if os.path.exists(cert_path) and os.path.exists(key_path):
            ssl_config = {
                "ssl_keyfile": key_path,
                "ssl_certfile": cert_path
            }
            print(f"[SSL] HTTPS enabled with SSL certificates")
            print(f"[INFO] Server will run on https://{settings.HOST}:{settings.PORT}")
        else:
            print(f"[WARNING] SSL certificates not found, running HTTP instead")
            print(f"[INFO] Server will run on http://{settings.HOST}:{settings.PORT}")
    else:
        print(f"[INFO] Server will run on http://{settings.HOST}:{settings.PORT}")

    # SSLê³¼ reloadëŠ” ì˜ ì‘ë™í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ SSLì´ í™œì„±í™”ëœ ê²½ìš° reload ë¹„í™œì„±í™”
    reload_mode = settings.DEBUG and not ssl_config

    uvicorn.run(
        "src.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=reload_mode,
        **ssl_config
    )
