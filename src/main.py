"""
Seegene Bid Information MCP Server
FastMCPë¥¼ ì‚¬ìš©í•œ ë©”ì¸ ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import asyncio
from datetime import datetime
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

try:
    from fastmcp import FastMCP
except ImportError:
    print("FastMCPê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install fastmcpë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    FastMCP = None

from src.config import settings
from src.database.connection import init_database, DatabaseManager
from src.models.filters import BidFilter
from src.models.bid_info import BidInfo
from src.models.crawler_api import (
    CrawlerRequest, CrawlerExecutionResponse, AllCrawlerExecutionResponse,
    CrawlerResultsResponse, SiteCrawlerResultResponse, ScheduledJobsResponse,
    ScheduleRequest, ScheduleResponse, BidListResponse, BidDetailResponse,
    BidSearchResponse, BidStatisticsResponse
)
from src.models.advanced_filters import (
    AdvancedBidSearchRequest, AdvancedSearchResponse, KeywordSuggestionsResponse,
    KeywordExpansion, AdvancedSearchQuery, KeywordGroup
)
from src.services.advanced_search import advanced_search_service
from src.utils.keyword_expansion import keyword_engine
from src.utils.logger import get_logger
from src.crawler.manager import crawler_manager

logger = get_logger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œìž‘ ì‹œ
    try:
        await init_database()
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        # í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œìž‘
        await crawler_manager.start_scheduler()
        logger.info("âœ… í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œìž‘ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    yield
    
    # ì¢…ë£Œ ì‹œ
    await crawler_manager.stop_scheduler()
    logger.info("ðŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Seegene Bid Information MCP Server",
    description="ì”¨ì  ì„ ìœ„í•œ ê¸€ë¡œë²Œ ìž…ì°° ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œìŠ¤í…œ",
    version="2.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# FastMCP ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
if FastMCP:
    mcp = FastMCP("Seegene Bid Information Server")
    
    @mcp.tool()
    async def search_bids(
        keywords: List[str],
        days_range: int = 7,
        countries: List[str] = ["KR", "US"],
        limit: int = 50
    ) -> Dict[str, Any]:
        """ìž…ì°° ì •ë³´ ê²€ìƒ‰"""
        try:
            logger.info(f"ìž…ì°° ì •ë³´ ê²€ìƒ‰: í‚¤ì›Œë“œ={keywords}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰
            results = await DatabaseManager.search_bids(keywords, limit)
            
            # ê²°ê³¼ ë³€í™˜
            bid_list = []
            for bid in results:
                bid_dict = {
                    "id": getattr(bid, 'id', 0),
                    "title": getattr(bid, 'title', ''),
                    "organization": getattr(bid, 'organization', ''),
                    "source_site": getattr(bid, 'source_site', ''),
                    "country": getattr(bid, 'country', ''),
                    "relevance_score": getattr(bid, 'relevance_score', 0.0),
                    "source_url": getattr(bid, 'source_url', '')
                }
                bid_list.append(bid_dict)
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(bid_list)}ê±´ ë°œê²¬")
            
            return {
                "success": True,
                "total_found": len(bid_list),
                "results": bid_list
            }
            
        except Exception as e:
            logger.error(f"ìž…ì°° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }

    @mcp.tool()
    async def get_database_stats() -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            stats = await DatabaseManager.get_database_stats()
            
            return {
                "success": True,
                "database_statistics": stats,
                "last_updated": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def run_crawler(
        site_name: str,
        keywords: List[str] = None
    ) -> Dict[str, Any]:
        """íŠ¹ì • ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            logger.info(f"í¬ë¡¤ë§ ì‹¤í–‰: {site_name}")

            result = await crawler_manager.run_crawler(site_name, keywords)

            return {
                "success": True,
                "crawler_result": result
            }

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def run_all_crawlers(
        keywords: List[str] = None
    ) -> Dict[str, Any]:
        """ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            logger.info("ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰")

            result = await crawler_manager.run_all_crawlers(keywords)

            return {
                "success": True,
                "crawling_result": result
            }

        except Exception as e:
            logger.error(f"ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def get_crawler_status() -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            status = crawler_manager.get_crawler_status()

            return {
                "success": True,
                "crawler_status": status
            }

        except Exception as e:
            logger.error(f"í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def get_scheduled_jobs() -> Dict[str, Any]:
        """ì˜ˆì•½ëœ í¬ë¡¤ë§ ìž‘ì—… ì¡°íšŒ"""
        try:
            jobs = crawler_manager.get_scheduled_jobs()

            return {
                "success": True,
                "scheduled_jobs": jobs
            }

        except Exception as e:
            logger.error(f"ì˜ˆì•½ëœ ìž‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def add_crawler_schedule(
        site_name: str,
        cron_expression: str,
        job_id: str = None
    ) -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ ì¶”ê°€"""
        try:
            success = await crawler_manager.add_custom_schedule(site_name, cron_expression, job_id)

            return {
                "success": success,
                "message": "ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì„±ê³µ" if success else "ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì‹¤íŒ¨"
            }

        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def advanced_search_bids(
        keywords: List[str],
        countries: List[str] = None,
        enable_expansion: bool = True,
        min_relevance: float = None,
        limit: int = 50
    ) -> Dict[str, Any]:
        """ê³ ê¸‰ í‚¤ì›Œë“œ í™•ìž¥ ê²€ìƒ‰"""
        try:
            logger.info(f"MCP ê³ ê¸‰ ê²€ìƒ‰: í‚¤ì›Œë“œ={keywords}")

            # í™•ìž¥ ì„¤ì •
            expansion_config = None
            if enable_expansion:
                expansion_config = KeywordExpansion(
                    enable_synonyms=True,
                    enable_related_terms=True,
                    enable_translations=True,
                    enable_abbreviations=True
                )

            # í‚¤ì›Œë“œ ê·¸ë£¹ ìƒì„±
            keyword_groups = [KeywordGroup(
                keywords=keywords,
                operator="or",
                weight=1.0
            )]

            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = AdvancedSearchQuery(
                keyword_groups=keyword_groups,
                countries=countries or ["KR", "US"],
                min_relevance_score=min_relevance,
                limit=limit
            )

            # ìš”ì²­ ìƒì„± ë° ê²€ìƒ‰
            request = AdvancedBidSearchRequest(
                query=search_query,
                expansion=expansion_config,
                include_metadata=True,
                explain_relevance=True
            )

            result = await advanced_search_service.search_bids(request)

            return {
                "success": True,
                "search_result": {
                    "total_found": result.total_found,
                    "search_time": result.search_time,
                    "query_summary": result.query_summary,
                    "filters_applied": result.filters_applied,
                    "results": [
                        {
                            "title": r.title,
                            "organization": r.organization,
                            "country": r.country,
                            "relevance_score": r.relevance_score,
                            "matched_keywords": r.matched_keywords,
                            "source_url": r.source_url,
                            "urgency_level": r.urgency_level
                        } for r in result.results[:10]  # ì²˜ìŒ 10ê°œë§Œ
                    ]
                }
            }

        except Exception as e:
            logger.error(f"MCP ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    @mcp.tool()
    async def get_keyword_suggestions(
        keywords: List[str],
        max_suggestions: int = 10
    ) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ì œì•ˆ ë°›ê¸°"""
        try:
            suggestions = keyword_engine.get_keyword_suggestions(keywords, max_suggestions)

            return {
                "success": True,
                "original_keywords": keywords,
                "suggestions": [
                    {
                        "keyword": s.keyword,
                        "relevance": s.relevance,
                        "source": s.source
                    } for s in suggestions
                ]
            }

        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì œì•ˆ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    # MCP ì„œë²„ ë§ˆìš´íŠ¸
    app.mount("/mcp", mcp.sse_app())

# FastAPI ë¼ìš°íŠ¸
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Seegene Bid Information MCP Server",
        "version": "2.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "docs": "/docs",
            "mcp": "/mcp" if FastMCP else "Not available (install fastmcp)",
            "crawler_apis": {
                "run_single": "POST /crawl/{site_name}",
                "run_all": "POST /crawl-all",
                "get_results": "GET /crawl-results",
                "get_site_results": "GET /crawl-results/{site_name}",
                "scheduled_jobs": "GET /scheduled-jobs",
                "add_schedule": "POST /schedule-crawler",
                "remove_schedule": "DELETE /schedule-crawler/{job_id}"
            },
            "advanced_search_apis": {
                "advanced_search": "POST /search/advanced",
                "keyword_suggestions": "GET /search/keyword-suggestions",
                "search_with_expansion": "POST /search/expanded"
            },
            "bid_data_apis": {
                "get_all_bids": "GET /bids",
                "get_bid_by_id": "GET /bids/{bid_id}",
                "search_bids": "GET /bids/search",
                "get_statistics": "GET /bids/stats"
            }
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        stats = await DatabaseManager.get_database_stats()
        db_status = "ok"
    except:
        db_status = "error"
    
    return {
        "status": "healthy" if db_status == "ok" else "degraded",
        "timestamp": datetime.now().isoformat(),
        "database": db_status,
        "version": "2.0.0"
    }

@app.get("/crawler-status")
async def crawler_status_endpoint():
    """í¬ë¡¤ëŸ¬ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        status = crawler_manager.get_crawler_status()
        return status
    except Exception as e:
        logger.error(f"í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "error": str(e),
            "scheduler_running": False,
            "crawlers": {}
        }

@app.post("/crawl/{site_name}", response_model=CrawlerExecutionResponse)
async def run_single_crawler(site_name: str, request: CrawlerRequest = None):
    """íŠ¹ì • ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        logger.info(f"ìˆ˜ë™ í¬ë¡¤ë§ ì‹¤í–‰ ìš”ì²­: {site_name}")

        keywords = request.keywords if request else None
        result = await crawler_manager.run_crawler(site_name, keywords)

        return CrawlerExecutionResponse(
            success=True,
            message=f"{site_name} í¬ë¡¤ë§ ì™„ë£Œ",
            result=result
        )

    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/crawl-all", response_model=AllCrawlerExecutionResponse)
async def run_all_crawlers_endpoint(request: CrawlerRequest = None):
    """ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        logger.info("ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰ ìš”ì²­")

        keywords = request.keywords if request else None
        result = await crawler_manager.run_all_crawlers(keywords)

        return AllCrawlerExecutionResponse(
            success=True,
            message="ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ",
            result=result
        )

    except Exception as e:
        logger.error(f"ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/crawl-results", response_model=CrawlerResultsResponse)
async def get_crawler_results():
    """ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    try:
        results = crawler_manager.last_run_results

        return CrawlerResultsResponse(
            success=True,
            last_run_results=results,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/crawl-results/{site_name}", response_model=SiteCrawlerResultResponse)
async def get_site_crawler_results(site_name: str):
    """íŠ¹ì • ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    try:
        if site_name not in crawler_manager.last_run_results:
            raise HTTPException(status_code=404, detail=f"{site_name}ì˜ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        result = crawler_manager.last_run_results[site_name]

        return SiteCrawlerResultResponse(
            success=True,
            site=site_name,
            result=result
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/scheduled-jobs", response_model=ScheduledJobsResponse)
async def get_scheduled_jobs_endpoint():
    """ì˜ˆì•½ëœ í¬ë¡¤ë§ ìž‘ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        jobs = crawler_manager.get_scheduled_jobs()

        return ScheduledJobsResponse(
            success=True,
            scheduled_jobs=jobs,
            scheduler_running=crawler_manager.is_running
        )

    except Exception as e:
        logger.error(f"ì˜ˆì•½ëœ ìž‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/schedule-crawler", response_model=ScheduleResponse)
async def add_crawler_schedule_endpoint(request: ScheduleRequest):
    """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ ì¶”ê°€"""
    try:
        if request.site_name not in ["G2B", "SAM.gov"]:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ìž…ë‹ˆë‹¤. G2B ë˜ëŠ” SAM.govë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        success = await crawler_manager.add_custom_schedule(
            request.site_name,
            request.cron_expression,
            request.job_id
        )

        if success:
            return ScheduleResponse(
                success=True,
                message=f"{request.site_name} ìŠ¤ì¼€ì¤„ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤",
                site_name=request.site_name,
                cron_expression=request.cron_expression,
                job_id=request.job_id
            )
        else:
            raise HTTPException(status_code=400, detail="ìŠ¤ì¼€ì¤„ ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/schedule-crawler/{job_id}", response_model=ScheduleResponse)
async def remove_crawler_schedule_endpoint(job_id: str):
    """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ ì œê±°"""
    try:
        success = crawler_manager.remove_scheduled_job(job_id)

        if success:
            return ScheduleResponse(
                success=True,
                message=f"ìŠ¤ì¼€ì¤„ ìž‘ì—… '{job_id}'ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤",
                job_id=job_id
            )
        else:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ìž‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ ì œê±° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search/advanced", response_model=AdvancedSearchResponse)
async def advanced_search_endpoint(request: AdvancedBidSearchRequest):
    """ê³ ê¸‰ ìž…ì°° ê²€ìƒ‰"""
    try:
        logger.info("ê³ ê¸‰ ê²€ìƒ‰ ìš”ì²­")

        result = await advanced_search_service.search_bids(request)

        return result

    except Exception as e:
        logger.error(f"ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/keyword-suggestions", response_model=KeywordSuggestionsResponse)
async def get_keyword_suggestions(
    keywords: str,
    max_suggestions: int = 20
):
    """í‚¤ì›Œë“œ ì œì•ˆ"""
    try:
        keyword_list = [k.strip() for k in keywords.split(",")]
        logger.info(f"í‚¤ì›Œë“œ ì œì•ˆ ìš”ì²­: {keyword_list}")

        suggestions = keyword_engine.get_keyword_suggestions(keyword_list, max_suggestions)

        return KeywordSuggestionsResponse(
            success=True,
            original_keywords=keyword_list,
            suggestions=suggestions,
            total_suggestions=len(suggestions)
        )

    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì œì•ˆ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search/expanded", response_model=AdvancedSearchResponse)
async def search_with_keyword_expansion(
    keywords: List[str],
    expansion_config: KeywordExpansion = None,
    countries: List[str] = None,
    limit: int = 50
):
    """í‚¤ì›Œë“œ í™•ìž¥ì„ í¬í•¨í•œ ê°„ë‹¨í•œ ê²€ìƒ‰"""
    try:
        logger.info(f"í™•ìž¥ ê²€ìƒ‰ ìš”ì²­: í‚¤ì›Œë“œ={keywords}")

        # ê¸°ë³¸ í™•ìž¥ ì„¤ì •
        if not expansion_config:
            expansion_config = KeywordExpansion()

        # í‚¤ì›Œë“œ ê·¸ë£¹ ìƒì„±
        keyword_groups = [KeywordGroup(
            keywords=keywords,
            operator="or",
            weight=1.0
        )]

        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = AdvancedSearchQuery(
            keyword_groups=keyword_groups,
            countries=countries,
            limit=limit
        )

        # ìš”ì²­ ê°ì²´ ìƒì„±
        request = AdvancedBidSearchRequest(
            query=search_query,
            expansion=expansion_config,
            include_metadata=True,
            explain_relevance=True
        )

        # ê²€ìƒ‰ ì‹¤í–‰
        result = await advanced_search_service.search_bids(request)

        return result

    except Exception as e:
        logger.error(f"í™•ìž¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/filters-help")
async def get_filters_help():
    """ê³ ê¸‰ í•„í„°ë§ ë„ì›€ë§"""
    return {
        "available_operators": [
            "eq (ê°™ìŒ)", "ne (ë‹¤ë¦„)", "gt (ì´ˆê³¼)", "lt (ë¯¸ë§Œ)",
            "gte (ì´ìƒ)", "lte (ì´í•˜)", "in (í¬í•¨)", "not_in (ì œì™¸)",
            "contains (ë¬¸ìžì—´ í¬í•¨)", "starts_with (ì‹œìž‘)", "ends_with (ëë‚¨)"
        ],
        "available_fields": [
            "title", "organization", "country", "source_site",
            "relevance_score", "urgency_level", "currency",
            "announcement_date", "deadline_date"
        ],
        "relevance_levels": ["low (1-3ì )", "medium (4-6ì )", "high (7-8ì )", "very_high (9-10ì )"],
        "urgency_levels": ["low", "medium", "high", "urgent"],
        "sort_options": ["relevance", "date", "announcement_date", "deadline_date", "price", "urgency"],
        "expansion_features": [
            "synonyms (ë™ì˜ì–´)", "related_terms (ê´€ë ¨ìš©ì–´)",
            "translations (ë²ˆì—­)", "abbreviations (ì•½ì–´)"
        ]
    }

@app.get("/bids", response_model=BidListResponse)
async def get_all_bids(
    limit: int = 50,
    offset: int = 0,
    site: str = None,
    country: str = None,
    min_relevance: float = None
):
    """ì €ìž¥ëœ ìž…ì°° ì •ë³´ ëª©ë¡ ì¡°íšŒ"""
    try:
        from src.database.connection import get_db_session, BidInfoModel
        from sqlalchemy import select, desc

        async with get_db_session() as session:
            # ì¿¼ë¦¬ ë¹Œë“œ
            query = select(BidInfoModel)

            # í•„í„° ì ìš©
            if site:
                query = query.where(BidInfoModel.source_site == site)
            if country:
                query = query.where(BidInfoModel.country == country)
            if min_relevance:
                query = query.where(BidInfoModel.relevance_score >= min_relevance)

            # ì •ë ¬ ë° íŽ˜ì´ì§€ë„¤ì´ì…˜
            query = query.order_by(desc(BidInfoModel.created_at)).offset(offset).limit(limit)

            # ì‹¤í–‰
            result = await session.execute(query)
            bids = result.scalars().all()

            # ê²°ê³¼ ë³€í™˜
            bid_list = []
            for bid in bids:
                # ì•ˆì „í•œ ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
                def safe_date_format(date_value):
                    if date_value is None:
                        return None
                    if hasattr(date_value, 'isoformat'):
                        return date_value.isoformat()
                    elif isinstance(date_value, str):
                        return date_value
                    else:
                        return str(date_value)

                bid_dict = {
                    "id": bid.id,
                    "title": bid.title,
                    "organization": bid.organization,
                    "bid_number": bid.bid_number,
                    "announcement_date": safe_date_format(bid.announcement_date),
                    "deadline_date": safe_date_format(bid.deadline_date),
                    "estimated_price": bid.estimated_price,
                    "currency": bid.currency,
                    "source_url": bid.source_url,
                    "source_site": bid.source_site,
                    "country": bid.country,
                    "relevance_score": bid.relevance_score,
                    "urgency_level": bid.urgency_level,
                    "status": bid.status,
                    "keywords": bid.keywords,
                    "created_at": safe_date_format(bid.created_at)
                }
                bid_list.append(bid_dict)

            # ì´ ê°œìˆ˜ ì¡°íšŒ
            count_query = select(BidInfoModel)
            if site:
                count_query = count_query.where(BidInfoModel.source_site == site)
            if country:
                count_query = count_query.where(BidInfoModel.country == country)
            if min_relevance:
                count_query = count_query.where(BidInfoModel.relevance_score >= min_relevance)

            from sqlalchemy import func
            count_result = await session.execute(select(func.count()).select_from(count_query.subquery()))
            total_count = count_result.scalar()

            return {
                "success": True,
                "data": bid_list,
                "pagination": {
                    "total": total_count,
                    "limit": limit,
                    "offset": offset,
                    "has_next": offset + limit < total_count
                },
                "filters": {
                    "site": site,
                    "country": country,
                    "min_relevance": min_relevance
                }
            }

    except Exception as e:
        logger.error(f"ìž…ì°° ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/bids/{bid_id}", response_model=BidDetailResponse)
async def get_bid_by_id(bid_id: int):
    """íŠ¹ì • ìž…ì°° ì •ë³´ ìƒì„¸ ì¡°íšŒ"""
    try:
        from src.database.connection import get_db_session, BidInfoModel
        from sqlalchemy import select

        async with get_db_session() as session:
            result = await session.execute(select(BidInfoModel).where(BidInfoModel.id == bid_id))
            bid = result.scalar_one_or_none()

            if not bid:
                raise HTTPException(status_code=404, detail="í•´ë‹¹ ìž…ì°° ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ì•ˆì „í•œ ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
            def safe_date_format(date_value):
                if date_value is None:
                    return None
                if hasattr(date_value, 'isoformat'):
                    return date_value.isoformat()
                elif isinstance(date_value, str):
                    return date_value
                else:
                    return str(date_value)

            bid_detail = {
                "id": bid.id,
                "title": bid.title,
                "organization": bid.organization,
                "bid_number": bid.bid_number,
                "announcement_date": safe_date_format(bid.announcement_date),
                "deadline_date": safe_date_format(bid.deadline_date),
                "estimated_price": bid.estimated_price,
                "currency": bid.currency,
                "source_url": bid.source_url,
                "source_site": bid.source_site,
                "country": bid.country,
                "relevance_score": bid.relevance_score,
                "urgency_level": bid.urgency_level,
                "status": bid.status,
                "keywords": bid.keywords,
                "extra_data": bid.extra_data,
                "created_at": safe_date_format(bid.created_at),
                "updated_at": safe_date_format(bid.updated_at)
            }

            return {
                "success": True,
                "data": bid_detail
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìž…ì°° ì •ë³´ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/bids/search", response_model=BidSearchResponse)
async def search_bids_endpoint(
    q: str,
    limit: int = 50,
    offset: int = 0,
    site: str = None,
    country: str = None
):
    """ìž…ì°° ì •ë³´ ê²€ìƒ‰"""
    try:
        keywords = [k.strip() for k in q.split(",") if k.strip()]
        logger.info(f"ìž…ì°° ê²€ìƒ‰ ìš”ì²­: í‚¤ì›Œë“œ={keywords}")

        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰
        results = await DatabaseManager.search_bids(keywords, limit + offset)

        # ì˜¤í”„ì…‹ ì ìš© (ê°„ë‹¨í•œ êµ¬í˜„)
        paginated_results = results[offset:offset + limit] if results else []

        # ì¶”ê°€ í•„í„° ì ìš©
        if site:
            paginated_results = [r for r in paginated_results if getattr(r, 'source_site', '') == site]
        if country:
            paginated_results = [r for r in paginated_results if getattr(r, 'country', '') == country]

        # ê²°ê³¼ ë³€í™˜
        bid_list = []
        for bid in paginated_results:
            bid_dict = {
                "id": getattr(bid, 'id', 0),
                "title": getattr(bid, 'title', ''),
                "organization": getattr(bid, 'organization', ''),
                "source_site": getattr(bid, 'source_site', ''),
                "country": getattr(bid, 'country', ''),
                "relevance_score": getattr(bid, 'relevance_score', 0.0),
                "source_url": getattr(bid, 'source_url', ''),
                "estimated_price": getattr(bid, 'estimated_price', ''),
                "deadline_date": getattr(bid, 'deadline_date', None),
                "urgency_level": getattr(bid, 'urgency_level', 'low')
            }
            bid_list.append(bid_dict)

        return {
            "success": True,
            "query": q,
            "keywords": keywords,
            "data": bid_list,
            "pagination": {
                "total": len(results),
                "limit": limit,
                "offset": offset,
                "returned": len(bid_list)
            }
        }

    except Exception as e:
        logger.error(f"ìž…ì°° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/bids/stats", response_model=BidStatisticsResponse)
async def get_bid_statistics():
    """ìž…ì°° ì •ë³´ í†µê³„"""
    try:
        stats = await DatabaseManager.get_database_stats()

        return {
            "success": True,
            "statistics": stats,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/test/db")
async def test_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    from src.database.connection import get_db_session, BidInfoModel
    from sqlalchemy import select

    try:
        async with get_db_session() as session:
            # ê°„ë‹¨í•œ select ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
            result = await session.execute(select(BidInfoModel).limit(1))
            bids = result.scalars().all()
            return {
                "success": True,
                "message": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ",
                "found_bids": len(bids)
            }
    except Exception as e:
        return {
            "success": False,
            "message": f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}",
            "error": str(e)
        }

if __name__ == "__main__":
    import uvicorn
    import os

    # SSL ì„¤ì •
    ssl_config = {}
    if settings.SSL_ENABLED:
        cert_path = os.path.join(os.getcwd(), settings.SSL_CERTFILE)
        key_path = os.path.join(os.getcwd(), settings.SSL_KEYFILE)

        if os.path.exists(cert_path) and os.path.exists(key_path):
            ssl_config = {
                "ssl_keyfile": key_path,
                "ssl_certfile": cert_path
            }
            print(f"[SSL] HTTPS enabled with SSL certificates")
            print(f"[INFO] Server will run on https://{settings.HOST}:{settings.PORT}")
        else:
            print(f"[WARNING] SSL certificates not found, running HTTP instead")
            print(f"[INFO] Server will run on http://{settings.HOST}:{settings.PORT}")
    else:
        print(f"[INFO] Server will run on http://{settings.HOST}:{settings.PORT}")

    # SSLê³¼ reloadëŠ” ìž˜ ìž‘ë™í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ SSLì´ í™œì„±í™”ëœ ê²½ìš° reload ë¹„í™œì„±í™”
    reload_mode = settings.DEBUG and not ssl_config

    uvicorn.run(
        "src.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=reload_mode,
        **ssl_config
    )
