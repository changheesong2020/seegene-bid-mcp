"""
German Vergabestellen í¬ë¡¤ëŸ¬
ë…ì¼ ê³µê³µì¡°ë‹¬ í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import ssl
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import quote, urljoin

import aiohttp
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta

from ..config import settings
from ..crawler.base import BaseCrawler
from ..models.tender_notice import (
    TenderNotice, TenderStatus, TenderType, ProcurementMethod,
    TenderValue, Organization, Classification, TenderDocument,
    CurrencyCode,
)
from ..utils.cpv_filter import cpv_filter
from ..utils.logger import get_logger
from ..database.connection import DatabaseManager

logger = get_logger(__name__)


DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": (
        "text/html,application/xhtml+xml,application/xml;q=0.9,"
        "application/rss+xml;q=0.9,*/*;q=0.8"
    ),
    "Accept-Language": "en-US,en;q=0.9",
    "Connection": "keep-alive",
}


def create_ssl_context():
    """SSL ê²€ì¦ ìš°íšŒë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE

    custom_ca_bundle = getattr(settings, "SSL_CUSTOM_CA_BUNDLE", None)
    if custom_ca_bundle:
        ca_path = Path(custom_ca_bundle)
        if not ca_path.is_absolute():
            ca_path = Path.cwd() / ca_path

        if ca_path.exists():
            try:
                ssl_context.load_verify_locations(cafile=str(ca_path))
                ssl_context.check_hostname = True
                ssl_context.verify_mode = ssl.CERT_REQUIRED
                logger.info(f"ì»¤ìŠ¤í…€ CA ë²ˆë“¤ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {ca_path}")
            except Exception as exc:
                logger.warning(f"ì»¤ìŠ¤í…€ CA ë²ˆë“¤ ë¡œë“œ ì‹¤íŒ¨: {exc}")
        else:
            logger.warning(f"ì§€ì •ëœ CA ë²ˆë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ca_path}")

    return ssl_context


class GermanyVergabestellenCrawler(BaseCrawler):
    """ë…ì¼ Vergabestellen ê³µê³µì¡°ë‹¬ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        super().__init__("DE_VERGABESTELLEN", "DE")

        # ë…ì¼ ì£¼ìš” ì¡°ë‹¬ í¬í„¸ë“¤
        self.portals = {
            "deutsches_vergabeportal": "https://www.deutsches-vergabeportal.de",
            "evergabe": "https://www.evergabe.de",
            "bund": "https://www.evergabe-online.de",
            "bayern": "https://www.vergabe24.bayern.de",
            "nrw": "https://www.vergabe.nrw.de"
        }

        # RSS/XML í”¼ë“œ URLë“¤
        # RSS í”¼ë“œ URLë“¤ (ì—°ê²° ì˜¤ë¥˜ ë•Œë¬¸ì— ì£¼ì„ ì²˜ë¦¬)
        self.rss_feeds = [
            # ì—°ê²° ì‹¤íŒ¨ë¡œ ì¸í•´ ì£¼ì„ ì²˜ë¦¬
            # "https://www.deutsches-vergabeportal.de/rss",
            # "https://www.evergabe.de/api/rss"
        ]

        # ì˜ë£Œê¸°ê¸° ê´€ë ¨ CPV ì½”ë“œ (ë…ì¼ íŠ¹í™”)
        self.healthcare_cpv_codes = [
            "33100000",  # ì˜ë£Œê¸°ê¸°
            "33140000",  # ì˜ë£Œìš©í’ˆ
            "33183000",  # ì§„ë‹¨ì¥ë¹„
            "33184000",  # ì‹¤í—˜ì‹¤ ì¥ë¹„
            "33600000",  # ì˜ì•½í’ˆ
            "33700000",  # ê°œì¸ë³´í˜¸ì¥ë¹„
        ]

        # ë…ì¼ì–´ ì˜ë£Œ í‚¤ì›Œë“œ
        self.medical_keywords_de = [
            "medizin", "medizinisch", "krankenhaus", "klinik", "labor",
            "diagnose", "diagnostik", "medizinprodukt", "medizintechnik",
            "gesundheit", "arzt", "pflege", "therapie", "chirurgie"
        ]

    async def crawl(self, keywords: List[str] = None) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"ë…ì¼ Vergabestellen í¬ë¡¤ë§ ì‹œì‘ - í‚¤ì›Œë“œ: {keywords}")

        results = []
        had_failures = False

        try:
            # RSS í”¼ë“œ ìˆ˜ì§‘
            rss_results, rss_failed = await self._crawl_rss_feeds(keywords)
            results.extend(rss_results)
            had_failures = had_failures or rss_failed

            # ì£¼ìš” í¬í„¸ í¬ë¡¤ë§
            for portal_name, portal_url in self.portals.items():
                try:
                    portal_results, portal_failed = await self._crawl_portal(portal_name, portal_url, keywords)
                    results.extend(portal_results)
                    had_failures = had_failures or portal_failed
                except Exception as e:
                    logger.warning(f"{portal_name} í¬í„¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    had_failures = True

            if not results and had_failures:
                logger.warning("ë…ì¼ Vergabestellen í¬í„¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

            # ê²°ê³¼ ì¤‘ë³µ ì œê±°
            unique_results = self._remove_duplicates(results)

            logger.info(f"ë…ì¼ Vergabestellen í¬ë¡¤ë§ ì™„ë£Œ - ì´ {len(unique_results)}ê±´ ìˆ˜ì§‘")

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if unique_results:
                try:
                    await DatabaseManager.save_bid_info(unique_results)
                    logger.info(f"ğŸ’¾ DE_VERGABESTELLEN ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(unique_results)}ê±´")
                except Exception as e:
                    logger.error(f"âŒ DE_VERGABESTELLEN ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            else:
                logger.info("ğŸ“ DE_VERGABESTELLEN ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

            return {
                "success": True,
                "total_collected": len(unique_results),
                "results": unique_results,
                "source": "DE_VERGABESTELLEN",
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"ë…ì¼ Vergabestellen í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": results,
                "source": "DE_VERGABESTELLEN",
                "timestamp": datetime.now().isoformat()
            }

    async def _crawl_rss_feeds(self, keywords: List[str] = None) -> Tuple[List[Dict[str, Any]], bool]:
        """RSS í”¼ë“œì—ì„œ ê³µê³  ìˆ˜ì§‘"""
        results = []
        had_failures = False

        if not self.rss_feeds:
            logger.info("RSS í”¼ë“œ URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ìŠ¤í‚µ")
            return results, False

        connector = aiohttp.TCPConnector(ssl=create_ssl_context())
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=connector,
            headers=DEFAULT_HEADERS,
        ) as session:

            for feed_url in self.rss_feeds:
                try:
                    logger.info(f"ë…ì¼ RSS í”¼ë“œ í¬ë¡¤ë§: {feed_url}")

                    async with session.get(feed_url, headers=DEFAULT_HEADERS) as response:
                        if response.status == 200:
                            content = await response.text()
                            feed_results = await self._parse_rss_feed(content, keywords)
                            results.extend(feed_results)
                            logger.info(f"RSSì—ì„œ {len(feed_results)}ê±´ ìˆ˜ì§‘")
                        else:
                            logger.warning(f"RSS í”¼ë“œ ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")
                            had_failures = True

                except Exception as e:
                    logger.warning(f"RSS í”¼ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜ {feed_url}: {e}")
                    had_failures = True

        return results, had_failures

    async def _crawl_portal(
        self,
        portal_name: str,
        portal_url: str,
        keywords: List[str] = None
    ) -> Tuple[List[Dict[str, Any]], bool]:
        """ê°œë³„ í¬í„¸ í¬ë¡¤ë§"""
        results = []
        had_failures = False

        try:
            logger.info(f"ë…ì¼ í¬í„¸ í¬ë¡¤ë§: {portal_name}")

            connector = aiohttp.TCPConnector(ssl=create_ssl_context())
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=45),
                connector=connector,
                headers=DEFAULT_HEADERS,
            ) as session:

                # ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼
                async with session.get(portal_url, headers=DEFAULT_HEADERS) as response:
                    if response.status == 200:
                        html_content = await response.text()

                        # ê³µê³  ëª©ë¡ í˜ì´ì§€ ì°¾ê¸°
                        search_results = await self._parse_portal_page(html_content, portal_name, keywords)
                        results.extend(search_results)

                        logger.info(f"{portal_name}ì—ì„œ {len(search_results)}ê±´ ìˆ˜ì§‘")
                    else:
                        logger.warning(f"{portal_name} ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")
                        had_failures = True

                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                await asyncio.sleep(3)

        except Exception as e:
            logger.warning(f"{portal_name} í¬í„¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            had_failures = True

        return results, had_failures

    async def _parse_rss_feed(self, content: str, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        results = []

        try:
            # XML íŒŒì‹±
            root = ET.fromstring(content)
            items = root.findall(".//item")

            for item in items:
                try:
                    title = item.find("title")
                    title_text = title.text if title is not None else ""

                    description = item.find("description")
                    description_text = description.text if description is not None else ""

                    link = item.find("link")
                    link_url = link.text if link is not None else ""

                    pub_date = item.find("pubDate")
                    pub_date_text = pub_date.text if pub_date is not None else ""

                    # í‚¤ì›Œë“œ í•„í„°ë§ (ë…ì¼ì–´ í¬í•¨)
                    if keywords and not self._matches_keywords_de(title_text + " " + description_text, keywords):
                        continue

                    # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ê³µê³  ì •ë³´ êµ¬ì„±
                    tender_info = {
                        "title": title_text.strip()[:500],  # ê¸¸ì´ ì œí•œ
                        "organization": self._extract_organization_de(description_text) or "Deutsche BehÃ¶rde",
                        "bid_number": f"DE-RSS-{datetime.now().strftime('%Y%m%d')}-{len(results)+1:03d}",
                        "announcement_date": self._parse_date(pub_date_text),
                        "deadline_date": self._extract_deadline_de(description_text) or self._estimate_deadline_date_de(),
                        "estimated_price": str(self._extract_value_de(description_text)) if self._extract_value_de(description_text) else "",
                        "currency": "EUR",
                        "source_url": link_url.strip(),
                        "source_site": "DE_VERGABESTELLEN",
                        "country": "DE",
                        "keywords": keywords or [],
                        "relevance_score": self._calculate_relevance_score_de(title_text, keywords[0] if keywords else ""),
                        "urgency_level": "medium",
                        "status": "active",
                        "extra_data": {
                            "description": description_text.strip()[:1000],  # ê¸¸ì´ ì œí•œ
                            "tender_type": self._determine_tender_type_de(title_text),
                            "cpv_codes": self._extract_cpv_codes(description_text),
                            "notice_type": "RSS",
                            "language": "de",
                            "crawled_at": datetime.now().isoformat()
                        }
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í•„í„°ë§
                    if self._is_healthcare_related_de(tender_info):
                        results.append(tender_info)

                except Exception as e:
                    logger.warning(f"RSS ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except ET.ParseError as e:
            logger.warning(f"RSS XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    async def _parse_portal_page(self, html_content: str, portal_name: str, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """í¬í„¸ í˜ì´ì§€ íŒŒì‹±"""
        results = []

        try:
            import re

            # ê³µê³  ì œëª© íŒ¨í„´ (ë…ì¼ì–´)
            title_patterns = [
                r'<h[2-4][^>]*>([^<]*(?:Ausschreibung|Vergabe|Auftrag|Beschaffung)[^<]*)</h[2-4]>',
                r'title="([^"]*(?:Ausschreibung|Vergabe|Auftrag|Beschaffung)[^"]*)"'
            ]

            # ë§í¬ íŒ¨í„´
            link_patterns = [
                r'href="([^"]*(?:vergabe|ausschreibung|auftrag)[^"]*)"',
                r'href="([^"]*tender[^"]*)"'
            ]

            titles = []
            for pattern in title_patterns:
                titles.extend(re.findall(pattern, html_content, re.IGNORECASE))

            links = []
            for pattern in link_patterns:
                links.extend(re.findall(pattern, html_content))

            # ì œëª©ê³¼ ë§í¬ ë§¤ì¹­
            for i, title in enumerate(titles[:8]):  # ìµœëŒ€ 8ê°œ
                try:
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if keywords and not self._matches_keywords_de(title, keywords):
                        continue

                    link_url = ""
                    if i < len(links):
                        base_url = self.portals.get(portal_name, portal_url)
                        link_url = urljoin(base_url, links[i])

                    tender_info = {
                        "title": title.strip()[:500],
                        "organization": self._extract_organization_from_title_de(title) or "Deutsche BehÃ¶rde",
                        "bid_number": f"DE-WEB-{datetime.now().strftime('%Y%m%d')}-{i+1:03d}",
                        "announcement_date": datetime.now().date().isoformat(),
                        "deadline_date": self._estimate_deadline_date_de(),
                        "estimated_price": "",
                        "currency": "EUR",
                        "source_url": link_url,
                        "source_site": "DE_VERGABESTELLEN",
                        "country": "DE",
                        "keywords": [],
                        "relevance_score": self._calculate_relevance_score_de(title, ""),
                        "urgency_level": "medium",
                        "status": "active",
                        "extra_data": {
                            "description": f"í¬í„¸: {portal_name}",
                            "tender_type": self._determine_tender_type_de(title),
                            "notice_type": "WEB_CRAWL",
                            "language": "de",
                            "portal_name": portal_name,
                            "crawled_at": datetime.now().isoformat()
                        }
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í™•ì¸
                    if self._is_healthcare_related_de(tender_info):
                        results.append(tender_info)

                except Exception as e:
                    logger.warning(f"í¬í„¸ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
            logger.warning(f"í¬í„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results


    def _matches_keywords_de(self, text: str, keywords: List[str]) -> bool:
        """ë…ì¼ì–´ í‚¤ì›Œë“œ ë§¤ì¹­"""
        if not keywords:
            return True

        text_lower = text.lower()

        # ì˜ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return True

        # ë…ì¼ì–´ ì˜ë£Œ í‚¤ì›Œë“œ ë§¤ì¹­
        for med_keyword in self.medical_keywords_de:
            if med_keyword in text_lower:
                return True

        return False

    def _determine_tender_type_de(self, title: str) -> str:
        """ë…ì¼ì–´ ê³µê³  ìœ í˜• íŒë‹¨"""
        title_lower = title.lower()

        if "ausschreibung" in title_lower or "Ã¶ffentlich" in title_lower:
            return "OPEN"
        elif "beschrÃ¤nkt" in title_lower or "begrenzt" in title_lower:
            return "RESTRICTED"
        elif "auftrag" in title_lower or "vertrag" in title_lower:
            return "CONTRACT"
        elif "rahmen" in title_lower:
            return "FRAMEWORK"
        else:
            return "OTHER"

    def _extract_organization_de(self, text: str) -> str:
        """ë…ì¼ì–´ ë°œì£¼ê¸°ê´€ ì¶”ì¶œ"""
        import re

        org_patterns = [
            r"(Bundesministerium[^,\n]+)",
            r"(Landesregierung[^,\n]+)",
            r"(Stadt[^,\n]+)",
            r"(Gemeinde[^,\n]+)",
            r"(Klinikum[^,\n]+)",
            r"(Krankenhaus[^,\n]+)",
            r"(UniversitÃ¤ts[^,\n]+)",
            r"(CharitÃ©[^,\n]*)",
            r"(UniversitÃ¤tsklinikum[^,\n]+)"
        ]

        for pattern in org_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return "Deutsche BehÃ¶rde"

    def _extract_organization_from_title_de(self, title: str) -> str:
        """ì œëª©ì—ì„œ ë°œì£¼ê¸°ê´€ ì¶”ì¶œ"""
        import re

        # ì œëª©ì—ì„œ ê¸°ê´€ëª… íŒ¨í„´ ì°¾ê¸°
        if "klinik" in title.lower() or "krankenhaus" in title.lower():
            return "Deutsches Krankenhaus"
        elif "universitÃ¤t" in title.lower():
            return "Deutsche UniversitÃ¤t"
        elif "stadt" in title.lower():
            return "Deutsche Stadtverwaltung"
        elif "bund" in title.lower():
            return "BundesbehÃ¶rde"
        else:
            return "Deutsche BehÃ¶rde"

    def _extract_value_de(self, text: str) -> Optional[float]:
        """ë…ì¼ì–´ ì¶”ì •ê°€ê²© ì¶”ì¶œ"""
        import re

        # ë…ì¼ ê¸ˆì•¡ íŒ¨í„´
        value_patterns = [
            r"(\d+(?:\.\d+)*(?:,\d+)?)\s*â‚¬",
            r"â‚¬\s*(\d+(?:\.\d+)*(?:,\d+)?)",
            r"(\d+(?:\.\d+)*(?:,\d+)?)\s*Euro",
            r"Wert:\s*(\d+(?:\.\d+)*(?:,\d+)?)"
        ]

        for pattern in value_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    value_str = match.group(1).replace(".", "").replace(",", ".")
                    return float(value_str)
                except ValueError:
                    continue

        return None

    def _extract_deadline_de(self, text: str) -> Optional[str]:
        """ë…ì¼ì–´ ë§ˆê°ì¼ ì¶”ì¶œ"""
        import re

        # ë…ì¼ ë‚ ì§œ íŒ¨í„´
        date_patterns = [
            r"(\d{1,2}\.\d{1,2}\.\d{4})",
            r"(\d{1,2}/\d{1,2}/\d{4})",
            r"(\d{4}-\d{1,2}-\d{1,2})",
            r"Frist:\s*(\d{1,2}\.\d{1,2}\.\d{4})",
            r"bis\s*(\d{1,2}\.\d{1,2}\.\d{4})"
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)

        return None

    def _parse_date(self, date_str: str) -> str:
        """ë…ì¼ ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
        try:
            from datetime import datetime

            # ë…ì¼ì–´ ë‚ ì§œ í˜•ì‹ë“¤
            formats = [
                "%a, %d %b %Y %H:%M:%S %Z",
                "%a, %d %b %Y %H:%M:%S %z",
                "%d.%m.%Y %H:%M:%S",
                "%d.%m.%Y",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d",
            ]

            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.date().isoformat()
                except ValueError:
                    continue

            return datetime.now().date().isoformat()

        except Exception:
            return datetime.now().date().isoformat()

    def _is_healthcare_related_de(self, tender_info: Dict[str, Any]) -> bool:
        """ë…ì¼ì–´ ì˜ë£Œê¸°ê¸° ê´€ë ¨ ê³µê³  í™•ì¸"""
        # CPV ì½”ë“œ í™•ì¸
        cpv_codes = tender_info.get("cpv_codes", [])
        if any(cpv.startswith(hc) for cpv in cpv_codes for hc in ["331", "336", "337"]):
            return True

        # ë…ì¼ì–´ ì˜ë£Œ í‚¤ì›Œë“œ í™•ì¸
        text = f"{tender_info.get('title', '')} {tender_info.get('description', '')}".lower()

        return any(keyword in text for keyword in self.medical_keywords_de)

    def _extract_cpv_codes(self, text: str) -> List[str]:
        """CPV ì½”ë“œ ì¶”ì¶œ"""
        import re

        cpv_pattern = r"CPV[:\s]*(\d{8})"
        matches = re.findall(cpv_pattern, text, re.IGNORECASE)

        return matches if matches else []

    def _remove_duplicates(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±°"""
        seen_urls = set()
        unique_results = []

        for result in results:
            url = result.get("source_url", "")
            title = result.get("title", "")

            key = url if url else title
            if key and key not in seen_urls:
                seen_urls.add(key)
                unique_results.append(result)

        return unique_results

    async def login(self) -> bool:
        """ë¡œê·¸ì¸ - ë…ì¼ Vergabestellen ëŒ€ë¶€ë¶„ ê³µê°œ ì‚¬ì´íŠ¸ì´ë¯€ë¡œ ë¡œê·¸ì¸ ë¶ˆí•„ìš”"""
        return True

    async def search_bids(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ì…ì°° ì •ë³´ ê²€ìƒ‰ - crawl ë©”ì„œë“œë¥¼ í˜¸ì¶œ"""
        result = await self.crawl(keywords)
        return result.get("results", [])

    def _estimate_deadline_date_de(self) -> str:
        """ë§ˆê°ì¼ ì¶”ì • (ë…ì¼ ê¸°ì¤€ 30ì¼ í›„)"""
        try:
            estimated_date = datetime.now() + timedelta(days=30)
            return estimated_date.date().isoformat()
        except Exception:
            return datetime.now().date().isoformat()

    def _calculate_relevance_score_de(self, title: str, keyword: str) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë…ì¼ì–´)"""
        if not keyword or not title:
            return 5.0

        title_lower = title.lower()
        keyword_lower = keyword.lower()

        # ì™„ì „ ì¼ì¹˜
        if keyword_lower in title_lower:
            return 8.0

        # ë…ì¼ì–´ ì˜ë£Œ í‚¤ì›Œë“œ ë¶€ë¶„ ì¼ì¹˜
        german_medical_keywords = [
            "medizinisch", "medizinische", "krankenhaus", "klinik", "diagnostik",
            "labor", "medizingerÃ¤te", "gesundheitswesen", "gesundheit",
            "therapie", "chirurgie", "radiologie", "kardiologie", "onkologie"
        ]

        for medical_kw in german_medical_keywords:
            if medical_kw.lower() in title_lower:
                return 7.0

        return 5.0