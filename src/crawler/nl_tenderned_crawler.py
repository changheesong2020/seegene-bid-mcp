"""
Dutch TenderNed í¬ë¡¤ëŸ¬
ë„¤ëœë€ë“œ ê³µê³µì¡°ë‹¬ í”Œë«í¼ TenderNed ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import aiohttp
import json
import ssl
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, quote

from bs4 import BeautifulSoup


def create_ssl_context():
    """SSL ê²€ì¦ ìš°íšŒë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    return ssl_context

from ..utils.logger import get_logger
from ..crawler.base import BaseCrawler
from ..models.tender_notice import (
    TenderNotice, TenderStatus, TenderType, ProcurementMethod,
    TenderValue, Organization, Classification, TenderDocument,
    CurrencyCode
)
from ..utils.cpv_filter import cpv_filter
from ..database.connection import DatabaseManager

logger = get_logger(__name__)


class NetherlandsTenderNedCrawler(BaseCrawler):
    """ë„¤ëœë€ë“œ TenderNed ê³µê³µì¡°ë‹¬ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        super().__init__("NL_TENDERNED", "NL")

        # TenderNed í”Œë«í¼ URLë“¤
        self.tenderned_base_url = "https://www.tenderned.nl"
        self.search_url = f"{self.tenderned_base_url}/tenderned-web/search"
        self.api_url = f"{self.tenderned_base_url}/api/search"

        # RSS í”¼ë“œ URLë“¤ (XML íŒŒì‹± ì˜¤ë¥˜ ë•Œë¬¸ì— ì£¼ì„ ì²˜ë¦¬)
        self.rss_feeds = [
            # XML íŒŒì‹± ì˜¤ë¥˜ë¡œ ì¸í•´ ì£¼ì„ ì²˜ë¦¬
            # f"{self.tenderned_base_url}/aankondigingen/overzicht.rss",
            # f"{self.tenderned_base_url}/aankondigingen/zoeken.rss",
        ]

        # ê³¼ê±°ì— ì‚¬ìš©ë˜ë˜ RSS í”¼ë“œ URL (í•„ìš” ì‹œ í´ë°±ìœ¼ë¡œë§Œ ì‹œë„)
        self.legacy_rss_feeds = [
            f"{self.tenderned_base_url}/rss/aanbestedingen.xml",
            f"{self.tenderned_base_url}/feeds/tender.rss",
        ]

        # HTTP ìš”ì²­ì— ì‚¬ìš©í•  ê³µí†µ í—¤ë” êµ¬ì„±
        self._user_agent = (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
        )

        # ë„¤ëœë€ë“œì–´ ì˜ë£Œ í‚¤ì›Œë“œ
        self.medical_keywords_nl = [
            "medisch", "medische", "ziekenhuis", "kliniek", "gezondheidszorg",
            "diagnostiek", "laboratorium", "medische apparatuur",
            "medische hulpmiddelen", "farmaceutisch", "gezondheid", "zorg",
            "therapie", "chirurgie", "radiologie", "cardiologie", "oncologie",
            "UMC", "academisch ziekenhuis", "GGD", "huisarts"
        ]

        # ì˜ë£Œê¸°ê¸° ê´€ë ¨ CPV ì½”ë“œ
        self.healthcare_cpv_codes = [
            "33100000",  # ì˜ë£Œê¸°ê¸°
            "33140000",  # ì˜ë£Œìš©í’ˆ
            "33183000",  # ì§„ë‹¨ì¥ë¹„
            "33184000",  # ì‹¤í—˜ì‹¤ ì¥ë¹„
            "33600000",  # ì˜ì•½í’ˆ
            "33700000",  # ê°œì¸ë³´í˜¸ì¥ë¹„
        ]

    def _build_headers(self, accept: Optional[str] = None) -> Dict[str, str]:
        """TenderNed ìš”ì²­ì—ì„œ ì‚¬ìš©í•  ê¸°ë³¸ í—¤ë” ìƒì„±"""
        headers: Dict[str, str] = {
            "User-Agent": self._user_agent,
            "Accept-Language": "en-US,en;q=0.9,nl;q=0.8",
            "Connection": "keep-alive",
        }

        if accept:
            headers["Accept"] = accept

        return headers

    async def _discover_rss_feeds(self, session: aiohttp.ClientSession) -> List[str]:
        """TenderNed í¬í„¸ì—ì„œ RSS í”¼ë“œ URLì„ ë™ì ìœ¼ë¡œ íƒìƒ‰"""

        discovery_pages = [
            f"{self.tenderned_base_url}/aankondigingen",
            f"{self.tenderned_base_url}/aankondigingen/overzicht",
        ]

        discovered: List[str] = []
        headers = self._build_headers(
            "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        )

        for page_url in discovery_pages:
            try:
                logger.debug(f"ë„¤ëœë€ë“œ RSS í”¼ë“œ ìë™ íƒìƒ‰ ì‹œë„: {page_url}")

                async with session.get(page_url, headers=headers) as response:
                    if response.status != 200:
                        logger.debug(
                            "RSS í”¼ë“œ íƒìƒ‰ ì‹¤íŒ¨ - ìƒíƒœì½”ë“œ %s (%s)",
                            response.status,
                            page_url,
                        )
                        continue

                    html_content = await response.text()
                    soup = BeautifulSoup(html_content, "html.parser")

                    for link_tag in soup.find_all(
                        "link", attrs={"type": "application/rss+xml"}
                    ):
                        href = link_tag.get("href")
                        if not href:
                            continue

                        full_url = urljoin(self.tenderned_base_url, href)
                        discovered.append(full_url)

            except Exception as e:
                logger.debug(f"RSS í”¼ë“œ ìë™ íƒìƒ‰ ì˜¤ë¥˜ {page_url}: {e}")

        unique_feeds = list(dict.fromkeys(discovered))

        if unique_feeds:
            logger.info(
                "ë„¤ëœë€ë“œ RSS í”¼ë“œ ìë™ íƒìƒ‰ ì„±ê³µ - ë°œê²¬ëœ í”¼ë“œ: %s",
                unique_feeds,
            )

        return unique_feeds

    async def crawl(self, keywords: List[str] = None) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"ë„¤ëœë€ë“œ TenderNed í¬ë¡¤ë§ ì‹œì‘ - í‚¤ì›Œë“œ: {keywords}")

        results = []

        try:
            # RSS í”¼ë“œ ìˆ˜ì§‘
            rss_results = await self._crawl_rss_feeds(keywords)
            results.extend(rss_results)

            # ì›¹ ê²€ìƒ‰ í¬ë¡¤ë§
            if keywords:
                web_results = await self._crawl_web_search(keywords)
                results.extend(web_results)

            # API ê²€ìƒ‰ ì‹œë„
            api_results = await self._crawl_api_search(keywords)
            results.extend(api_results)

            # ë©”ì¸ í¬í„¸ í¬ë¡¤ë§
            portal_results = await self._crawl_main_portal(keywords)
            results.extend(portal_results)

            # ê²°ê³¼ ì¤‘ë³µ ì œê±°
            unique_results = self._remove_duplicates(results)

            logger.info(f"ë„¤ëœë€ë“œ TenderNed í¬ë¡¤ë§ ì™„ë£Œ - ì´ {len(unique_results)}ê±´ ìˆ˜ì§‘")

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if unique_results:
                try:
                    await DatabaseManager.save_bid_info(unique_results)
                    logger.info(f"ğŸ’¾ NL_TENDERNED ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(unique_results)}ê±´")
                except Exception as e:
                    logger.error(f"âŒ NL_TENDERNED ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            else:
                logger.info("ğŸ“ NL_TENDERNED ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

            return {
                "success": True,
                "total_collected": len(unique_results),
                "results": unique_results,
                "source": "NL_TENDERNED",
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"ë„¤ëœë€ë“œ TenderNed í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": results,
                "source": "NL_TENDERNED",
                "timestamp": datetime.now().isoformat()
            }

    async def _crawl_rss_feeds(self, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œì—ì„œ ê³µê³  ìˆ˜ì§‘"""
        results = []

        if not self.rss_feeds:
            logger.info("RSS í”¼ë“œ URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ìŠ¤í‚µ")
            return results

        connector = aiohttp.TCPConnector(ssl=create_ssl_context())
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=connector
        ) as session:

            discovered_feeds = await self._discover_rss_feeds(session)
            feed_candidates = list(dict.fromkeys(discovered_feeds + self.rss_feeds))

            rss_headers = self._build_headers(
                "application/rss+xml,application/xml;q=0.9,*/*;q=0.8"
            )

            successful_response = False
            attempted_feeds = set()

            for feed_url in feed_candidates:
                if not feed_url:
                    continue

                attempted_feeds.add(feed_url)

                try:
                    logger.info(f"ë„¤ëœë€ë“œ RSS í”¼ë“œ í¬ë¡¤ë§: {feed_url}")

                    async with session.get(feed_url, headers=rss_headers) as response:
                        if response.status == 200:
                            successful_response = True
                            content = await response.text()
                            feed_results = await self._parse_rss_feed(content, keywords)
                            results.extend(feed_results)
                            logger.info(f"RSSì—ì„œ {len(feed_results)}ê±´ ìˆ˜ì§‘")
                        elif response.status == 404:
                            logger.info(f"RSS í”¼ë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ(404): {feed_url}")
                        else:
                            logger.warning(
                                "RSS í”¼ë“œ ì ‘ê·¼ ì‹¤íŒ¨(%s): %s",
                                response.status,
                                feed_url,
                            )

                except Exception as e:
                    logger.warning(f"RSS í”¼ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜ {feed_url}: {e}")

            if not successful_response:
                logger.info("ì‹ ê·œ RSS í”¼ë“œì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í•´ í´ë°± í”¼ë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")

                for feed_url in self.legacy_rss_feeds:
                    if feed_url in attempted_feeds:
                        continue

                    try:
                        logger.info(f"ë„¤ëœë€ë“œ ë ˆê±°ì‹œ RSS í”¼ë“œ í¬ë¡¤ë§: {feed_url}")

                        async with session.get(feed_url, headers=rss_headers) as response:
                            if response.status == 200:
                                content = await response.text()
                                feed_results = await self._parse_rss_feed(content, keywords)
                                results.extend(feed_results)
                                logger.info(f"ë ˆê±°ì‹œ RSSì—ì„œ {len(feed_results)}ê±´ ìˆ˜ì§‘")
                            elif response.status == 404:
                                logger.info(
                                    "ë ˆê±°ì‹œ RSS í”¼ë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ(404): %s",
                                    feed_url,
                                )
                            else:
                                logger.warning(
                                    "ë ˆê±°ì‹œ RSS í”¼ë“œ ì ‘ê·¼ ì‹¤íŒ¨(%s): %s",
                                    response.status,
                                    feed_url,
                                )

                    except Exception as e:
                        logger.warning(f"ë ˆê±°ì‹œ RSS í”¼ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜ {feed_url}: {e}")

        return results

    async def _crawl_web_search(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ì›¹ ê²€ìƒ‰ì„ í†µí•œ ê³µê³  ìˆ˜ì§‘"""
        results = []

        connector = aiohttp.TCPConnector(ssl=create_ssl_context())
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=45),
            connector=connector
        ) as session:

            for keyword in keywords[:3]:  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
                try:
                    logger.info(f"ë„¤ëœë€ë“œ ì›¹ ê²€ìƒ‰: {keyword}")

                    # TenderNed ê²€ìƒ‰ í˜ì´ì§€
                    search_params = {
                        "query": keyword,
                        "type": "all",
                        "status": "open",
                        "sortBy": "publicationDate",
                        "sortOrder": "desc"
                    }

                    async with session.get(self.search_url, params=search_params) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            search_results = await self._parse_search_results_nl(html_content, keyword)
                            results.extend(search_results)
                            logger.info(f"ì›¹ ê²€ìƒ‰ì—ì„œ {len(search_results)}ê±´ ìˆ˜ì§‘")
                        else:
                            logger.warning(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status}")

                    # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                    await asyncio.sleep(3)

                except Exception as e:
                    logger.warning(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ {keyword}: {e}")

        return results

    async def _crawl_api_search(self, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """API ê²€ìƒ‰ì„ í†µí•œ ê³µê³  ìˆ˜ì§‘"""
        results = []

        if not keywords:
            return results

        connector = aiohttp.TCPConnector(ssl=create_ssl_context())
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=45),
            connector=connector
        ) as session:

            for keyword in keywords[:2]:  # APIëŠ” ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œë§Œ
                try:
                    logger.info(f"ë„¤ëœë€ë“œ API ê²€ìƒ‰: {keyword}")

                    # API ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
                    api_params = {
                        "searchText": keyword,
                        "pageSize": 20,
                        "pageNumber": 0,
                        "sortField": "publicationDate",
                        "sortDirection": "DESC"
                    }

                    headers = {
                        "Accept": "application/json",
                        "Content-Type": "application/json",
                        "User-Agent": "TenderBot/1.0"
                    }

                    async with session.get(self.api_url, params=api_params, headers=headers) as response:
                        if response.status == 200:
                            try:
                                json_data = await response.json()
                                api_results = await self._parse_api_results(json_data, keyword)
                                results.extend(api_results)
                                logger.info(f"APIì—ì„œ {len(api_results)}ê±´ ìˆ˜ì§‘")
                            except json.JSONDecodeError:
                                logger.warning("API ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨")
                        else:
                            logger.warning(f"API ê²€ìƒ‰ ì‹¤íŒ¨: {response.status}")

                    await asyncio.sleep(2)

                except Exception as e:
                    logger.warning(f"API ê²€ìƒ‰ ì˜¤ë¥˜ {keyword}: {e}")

        return results

    async def _crawl_main_portal(self, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """ë©”ì¸ í¬í„¸ í¬ë¡¤ë§"""
        results = []

        try:
            logger.info("ë„¤ëœë€ë“œ TenderNed ë©”ì¸ í¬í„¸ í¬ë¡¤ë§")

            connector = aiohttp.TCPConnector(ssl=create_ssl_context())
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=45),
                connector=connector
            ) as session:

                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }

                # ë©”ì¸ í˜ì´ì§€
                async with session.get(self.tenderned_base_url, headers=headers) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        portal_results = await self._parse_main_page(html_content, keywords)
                        results.extend(portal_results)
                        logger.info(f"ë©”ì¸ í¬í„¸ì—ì„œ {len(portal_results)}ê±´ ìˆ˜ì§‘")

        except Exception as e:
            logger.warning(f"ë©”ì¸ í¬í„¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

        return results

    async def _parse_rss_feed(self, content: str, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        results = []

        try:
            # XML íŒŒì‹±
            root = ET.fromstring(content)
            items = root.findall(".//item")

            for item in items:
                try:
                    title = item.find("title")
                    title_text = title.text if title is not None else ""

                    description = item.find("description")
                    description_text = description.text if description is not None else ""

                    link = item.find("link")
                    link_url = link.text if link is not None else ""

                    pub_date = item.find("pubDate")
                    pub_date_text = pub_date.text if pub_date is not None else ""

                    # í‚¤ì›Œë“œ í•„í„°ë§ (ë„¤ëœë€ë“œì–´ í¬í•¨)
                    if keywords and not self._matches_keywords_nl(title_text + " " + description_text, keywords):
                        continue

                    # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ê³µê³  ì •ë³´ êµ¬ì„±
                    tender_info = {
                        "title": title_text.strip()[:500],  # ê¸¸ì´ ì œí•œ
                        "organization": self._extract_organization_nl(description_text) or "Nederlandse Overheid",
                        "bid_number": f"NL-RSS-{datetime.now().strftime('%Y%m%d')}-{len(results)+1:03d}",
                        "announcement_date": self._parse_date_nl(pub_date_text),
                        "deadline_date": self._extract_deadline_nl(description_text) or self._estimate_deadline_date_nl(),
                        "estimated_price": str(self._extract_value_nl(description_text)) if self._extract_value_nl(description_text) else "",
                        "currency": "EUR",
                        "source_url": link_url.strip(),
                        "source_site": "NL_TENDERNED",
                        "country": "NL",
                        "keywords": keywords or [],
                        "relevance_score": self._calculate_relevance_score_nl(title_text, keywords[0] if keywords else ""),
                        "urgency_level": "medium",
                        "status": "active",
                        "extra_data": {
                            "description": description_text.strip()[:1000],  # ê¸¸ì´ ì œí•œ
                            "tender_type": self._determine_tender_type_nl(title_text),
                            "cpv_codes": self._extract_cpv_codes(description_text),
                            "notice_type": "RSS",
                            "language": "nl",
                            "crawled_at": datetime.now().isoformat()
                        }
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í•„í„°ë§
                    if self._is_healthcare_related_nl(tender_info):
                        results.append(tender_info)

                except Exception as e:
                    logger.warning(f"RSS ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except ET.ParseError as e:
            logger.warning(f"RSS XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    async def _parse_search_results_nl(self, html_content: str, keyword: str) -> List[Dict[str, Any]]:
        """ë„¤ëœë€ë“œì–´ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
        results = []

        try:
            import re

            # ë„¤ëœë€ë“œì–´ ê³µê³  ì œëª© íŒ¨í„´
            title_patterns = [
                r'<h[2-4][^>]*>([^<]*(?:aanbesteding|inschrijving|tender)[^<]*)</h[2-4]>',
                r'title="([^"]*(?:aanbesteding|inschrijving|tender)[^"]*)"',
                r'<a[^>]*>([^<]*(?:medisch|ziekenhuis|gezondheidszorg)[^<]*)</a>'
            ]

            # ë§í¬ íŒ¨í„´
            link_patterns = [
                r'href="([^"]*(?:tender|aanbesteding)[^"]*)"',
                r'href="([^"]*tenderned[^"]*)"'
            ]

            titles = []
            for pattern in title_patterns:
                titles.extend(re.findall(pattern, html_content, re.IGNORECASE))

            links = []
            for pattern in link_patterns:
                links.extend(re.findall(pattern, html_content))

            # ì œëª©ê³¼ ë§í¬ ë§¤ì¹­
            for i, title in enumerate(titles[:8]):  # ìµœëŒ€ 8ê°œ
                try:
                    link_url = ""
                    if i < len(links):
                        link_url = urljoin(self.tenderned_base_url, links[i])

                    tender_info = {
                        "title": title.strip()[:500],
                        "organization": self._extract_organization_from_title_nl(title) or "Nederlandse Overheid",
                        "bid_number": f"NL-WEB-{datetime.now().strftime('%Y%m%d')}-{i+1:03d}",
                        "announcement_date": datetime.now().date().isoformat(),
                        "deadline_date": self._estimate_deadline_date_nl(),
                        "estimated_price": "",
                        "currency": "EUR",
                        "source_url": link_url,
                        "source_site": "NL_TENDERNED",
                        "country": "NL",
                        "keywords": [keyword],
                        "relevance_score": self._calculate_relevance_score_nl(title, keyword),
                        "urgency_level": "medium",
                        "status": "active",
                        "extra_data": {
                            "description": f"Zoekwoord: {keyword}",
                            "tender_type": self._determine_tender_type_nl(title),
                            "notice_type": "WEB_SEARCH",
                            "language": "nl",
                            "crawled_at": datetime.now().isoformat()
                        }
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í™•ì¸
                    if self._is_healthcare_related_nl(tender_info):
                        results.append(tender_info)

                except Exception as e:
                    logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
            logger.warning(f"HTML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    async def _parse_api_results(self, json_data: Dict[str, Any], keyword: str) -> List[Dict[str, Any]]:
        """API ì‘ë‹µ íŒŒì‹±"""
        results = []

        try:
            # API ì‘ë‹µ êµ¬ì¡° ê°€ì •
            tenders = json_data.get("results", [])
            if not tenders:
                tenders = json_data.get("data", [])
            if not tenders:
                tenders = json_data.get("content", [])

            for tender in tenders[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    title = tender.get("title", tender.get("name", ""))
                    description = tender.get("description", tender.get("summary", ""))
                    tender_id = tender.get("id", tender.get("tenderId", ""))

                    # URL êµ¬ì„±
                    detail_url = ""
                    if tender_id:
                        detail_url = f"{self.tenderned_base_url}/tender/{tender_id}"

                    tender_info = {
                        "title": title.strip()[:500],
                        "organization": tender.get("organization", "Nederlandse Overheid"),
                        "bid_number": f"NL-API-{datetime.now().strftime('%Y%m%d')}-{tender_id or len(results)+1:03d}",
                        "announcement_date": self._parse_date_nl(tender.get("publicationDate", "")),
                        "deadline_date": self._parse_date_nl(tender.get("deadlineDate", "")) or self._estimate_deadline_date_nl(),
                        "estimated_price": str(tender.get("estimatedValue")) if tender.get("estimatedValue") else "",
                        "currency": "EUR",
                        "source_url": detail_url,
                        "source_site": "NL_TENDERNED",
                        "country": "NL",
                        "keywords": [keyword],
                        "relevance_score": self._calculate_relevance_score_nl(title, keyword),
                        "urgency_level": "medium",
                        "status": "active",
                        "extra_data": {
                            "description": description.strip()[:1000],
                            "tender_type": self._determine_tender_type_nl(title),
                            "tender_id": tender_id,
                            "notice_type": "API",
                            "language": "nl",
                            "crawled_at": datetime.now().isoformat()
                        }
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í™•ì¸
                    if self._is_healthcare_related_nl(tender_info):
                        results.append(tender_info)

                except Exception as e:
                    logger.warning(f"API ê²°ê³¼ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
            logger.warning(f"API JSON íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    async def _parse_main_page(self, html_content: str, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """ë©”ì¸ í˜ì´ì§€ íŒŒì‹±"""
        results = []

        try:
            import re

            # ë©”ì¸ í˜ì´ì§€ ê³µê³  íŒ¨í„´
            title_patterns = [
                r'<a[^>]*>([^<]*(?:aanbesteding|tender)[^<]*)</a>',
                r'<div[^>]*>([^<]*(?:medisch|ziekenhuis|gezondheidszorg)[^<]*)</div>',
                r'<h[2-4][^>]*>([^<]*(?:UMC|academisch ziekenhuis)[^<]*)</h[2-4]>'
            ]

            titles = []
            for pattern in title_patterns:
                titles.extend(re.findall(pattern, html_content, re.IGNORECASE))

            for title in titles[:6]:  # ìµœëŒ€ 6ê°œ
                try:
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if keywords and not self._matches_keywords_nl(title, keywords):
                        continue

                    tender_info = {
                        "title": title.strip()[:500],
                        "organization": "Nederlandse Overheid",
                        "bid_number": f"NL-PORTAL-{datetime.now().strftime('%Y%m%d')}-{len(results)+1:03d}",
                        "announcement_date": datetime.now().date().isoformat(),
                        "deadline_date": self._estimate_deadline_date_nl(),
                        "estimated_price": "",
                        "currency": "EUR",
                        "source_url": self.tenderned_base_url,
                        "source_site": "NL_TENDERNED",
                        "country": "NL",
                        "keywords": keywords or [],
                        "relevance_score": self._calculate_relevance_score_nl(title, keywords[0] if keywords else ""),
                        "urgency_level": "medium",
                        "status": "active",
                        "extra_data": {
                            "description": "TenderNed hoofdportaal",
                            "tender_type": self._determine_tender_type_nl(title),
                            "notice_type": "MAIN_PORTAL",
                            "language": "nl",
                            "crawled_at": datetime.now().isoformat()
                        }
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í™•ì¸
                    if self._is_healthcare_related_nl(tender_info):
                        results.append(tender_info)

                except Exception as e:
                    logger.warning(f"ë©”ì¸ í˜ì´ì§€ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
            logger.warning(f"ë©”ì¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def _matches_keywords_nl(self, text: str, keywords: List[str]) -> bool:
        """ë„¤ëœë€ë“œì–´ í‚¤ì›Œë“œ ë§¤ì¹­"""
        if not keywords:
            return True

        text_lower = text.lower()

        # ì˜ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return True

        # ë„¤ëœë€ë“œì–´ ì˜ë£Œ í‚¤ì›Œë“œ ë§¤ì¹­
        for med_keyword in self.medical_keywords_nl:
            if med_keyword in text_lower:
                return True

        return False

    def _determine_tender_type_nl(self, title: str) -> str:
        """ë„¤ëœë€ë“œì–´ ê³µê³  ìœ í˜• íŒë‹¨"""
        title_lower = title.lower()

        if "openbare" in title_lower or "open" in title_lower:
            return "OPEN"
        elif "beperkte" in title_lower or "gesloten" in title_lower:
            return "RESTRICTED"
        elif "onderhandse" in title_lower:
            return "NEGOTIATED"
        elif "raamovereenkomst" in title_lower:
            return "FRAMEWORK"
        else:
            return "OTHER"

    def _extract_organization_nl(self, text: str) -> str:
        """ë„¤ëœë€ë“œì–´ ë°œì£¼ê¸°ê´€ ì¶”ì¶œ"""
        import re

        org_patterns = [
            r"(Ministerie[^,\n]+)",
            r"(Gemeente[^,\n]+)",
            r"(Provincie[^,\n]+)",
            r"(Ziekenhuis[^,\n]+)",
            r"(UMC[^,\n]*)",
            r"(Academisch Ziekenhuis[^,\n]+)",
            r"(Universiteit[^,\n]+)",
            r"(GGD[^,\n]*)",
            r"(Waternet[^,\n]*)"
        ]

        for pattern in org_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return "Nederlandse Overheid"

    def _extract_organization_from_title_nl(self, title: str) -> str:
        """ì œëª©ì—ì„œ ë°œì£¼ê¸°ê´€ ì¶”ì¶œ"""
        title_lower = title.lower()

        if "ziekenhuis" in title_lower or "umc" in title_lower:
            return "Nederlands Ziekenhuis"
        elif "universiteit" in title_lower:
            return "Nederlandse Universiteit"
        elif "gemeente" in title_lower:
            return "Nederlandse Gemeente"
        elif "ministerie" in title_lower:
            return "Nederlands Ministerie"
        elif "provincie" in title_lower:
            return "Nederlandse Provincie"
        elif "ggd" in title_lower:
            return "GGD Nederland"
        else:
            return "Nederlandse Overheid"

    def _extract_value_nl(self, text: str) -> Optional[float]:
        """ë„¤ëœë€ë“œì–´ ì¶”ì •ê°€ê²© ì¶”ì¶œ"""
        import re

        # ë„¤ëœë€ë“œ ê¸ˆì•¡ íŒ¨í„´
        value_patterns = [
            r"â‚¬\s*(\d+(?:\.\d+)*(?:,\d+)?)",
            r"(\d+(?:\.\d+)*(?:,\d+)?)\s*â‚¬",
            r"(\d+(?:\.\d+)*(?:,\d+)?)\s*euro",
            r"waarde[:\s]*â‚¬?\s*(\d+(?:\.\d+)*(?:,\d+)?)",
            r"bedrag[:\s]*â‚¬?\s*(\d+(?:\.\d+)*(?:,\d+)?)",
            r"raming[:\s]*â‚¬?\s*(\d+(?:\.\d+)*(?:,\d+)?)"
        ]

        for pattern in value_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    value_str = match.group(1).replace(".", "").replace(",", ".")
                    return float(value_str)
                except ValueError:
                    continue

        return None

    def _extract_deadline_nl(self, text: str) -> Optional[str]:
        """ë„¤ëœë€ë“œì–´ ë§ˆê°ì¼ ì¶”ì¶œ"""
        import re

        # ë„¤ëœë€ë“œ ë‚ ì§œ íŒ¨í„´
        date_patterns = [
            r"(\d{1,2}/\d{1,2}/\d{4})",
            r"(\d{1,2}-\d{1,2}-\d{4})",
            r"(\d{4}-\d{1,2}-\d{1,2})",
            r"inschrijftermijn[:\s]*(\d{1,2}/\d{1,2}/\d{4})",
            r"deadline[:\s]*(\d{1,2}/\d{1,2}/\d{4})",
            r"uiterlijk[:\s]*(\d{1,2}/\d{1,2}/\d{4})"
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)

        return None

    def _parse_date_nl(self, date_str: str) -> str:
        """ë„¤ëœë€ë“œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
        try:
            from datetime import datetime

            if not date_str or date_str.strip() == "":
                return datetime.now().date().isoformat()

            # ë„¤ëœë€ë“œì–´ ë‚ ì§œ í˜•ì‹ë“¤
            formats = [
                "%a, %d %b %Y %H:%M:%S %Z",
                "%a, %d %b %Y %H:%M:%S %z",
                "%d/%m/%Y %H:%M:%S",
                "%d/%m/%Y",
                "%d-%m-%Y %H:%M:%S",
                "%d-%m-%Y",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d",
            ]

            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.date().isoformat()
                except ValueError:
                    continue

            return datetime.now().date().isoformat()

        except Exception:
            return datetime.now().date().isoformat()

    def _is_healthcare_related_nl(self, tender_info: Dict[str, Any]) -> bool:
        """ë„¤ëœë€ë“œì–´ ì˜ë£Œê¸°ê¸° ê´€ë ¨ ê³µê³  í™•ì¸"""
        # CPV ì½”ë“œ í™•ì¸
        cpv_codes = tender_info.get("cpv_codes", [])
        if any(cpv.startswith(hc) for cpv in cpv_codes for hc in ["331", "336", "337"]):
            return True

        # ë„¤ëœë€ë“œì–´ ì˜ë£Œ í‚¤ì›Œë“œ í™•ì¸
        text = f"{tender_info.get('title', '')} {tender_info.get('description', '')}".lower()

        return any(keyword in text for keyword in self.medical_keywords_nl)

    def _extract_cpv_codes(self, text: str) -> List[str]:
        """CPV ì½”ë“œ ì¶”ì¶œ"""
        import re

        cpv_pattern = r"CPV[:\s]*(\d{8})"
        matches = re.findall(cpv_pattern, text, re.IGNORECASE)

        return matches if matches else []

    def _remove_duplicates(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±°"""
        seen_urls = set()
        unique_results = []

        for result in results:
            url = result.get("source_url", "")
            title = result.get("title", "")

            key = url if url else title
            if key and key not in seen_urls:
                seen_urls.add(key)
                unique_results.append(result)

        return unique_results

    async def login(self) -> bool:
        """ë¡œê·¸ì¸ - ë„¤ëœë€ë“œ TenderNedëŠ” ê³µê°œ ì‚¬ì´íŠ¸ì´ë¯€ë¡œ ë¡œê·¸ì¸ ë¶ˆí•„ìš”"""
        return True

    async def search_bids(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ì…ì°° ì •ë³´ ê²€ìƒ‰ - crawl ë©”ì„œë“œë¥¼ í˜¸ì¶œ"""
        result = await self.crawl(keywords)
        return result.get("results", [])

    def _estimate_deadline_date_nl(self) -> str:
        """ë§ˆê°ì¼ ì¶”ì • (ë„¤ëœë€ë“œ ê¸°ì¤€ 30ì¼ í›„)"""
        try:
            estimated_date = datetime.now() + timedelta(days=30)
            return estimated_date.date().isoformat()
        except Exception:
            return datetime.now().date().isoformat()

    def _calculate_relevance_score_nl(self, title: str, keyword: str) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë„¤ëœë€ë“œì–´)"""
        if not keyword or not title:
            return 5.0

        title_lower = title.lower()
        keyword_lower = keyword.lower()

        # ì™„ì „ ì¼ì¹˜
        if keyword_lower in title_lower:
            return 8.0

        # ë„¤ëœë€ë“œì–´ ì˜ë£Œ í‚¤ì›Œë“œ ë¶€ë¶„ ì¼ì¹˜
        dutch_medical_keywords = [
            "medisch", "medische", "ziekenhuis", "kliniek", "diagnostiek",
            "laboratoire", "medische apparatuur", "gezondheidszorg", "zorg",
            "therapie", "chirurgie", "radiologie", "cardiologie", "oncologie"
        ]

        for medical_kw in dutch_medical_keywords:
            if medical_kw.lower() in title_lower:
                return 7.0

        return 5.0