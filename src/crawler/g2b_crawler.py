"""
G2B (ë‚˜ë¼ì¥í„°) API Crawler
ì¡°ë‹¬ì²­ ê³µê³µë°ì´í„° í¬í„¸ Open APIë¥¼ í†µí•œ ë¯¼ê°„ì…ì°°ê³µê³  í¬ë¡¤ëŸ¬
"""

import asyncio
import aiohttp
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urlencode

from src.crawler.base import BaseCrawler
from src.config import settings
from src.utils.logger import get_logger

logger = get_logger(__name__)


class G2BCrawler(BaseCrawler):
    """ë‚˜ë¼ì¥í„°(G2B) API í¬ë¡¤ëŸ¬"""

    def __init__(self):
        super().__init__("G2B", "KR")
        self.api_base_url = "http://apis.data.go.kr/1230000/ao/PubDataOpnStdService"
        self.api_key = settings.G2B_API_KEY

        # ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤ ì˜¤í¼ë ˆì´ì…˜
        self.operation = "getDataSetOpnStdBidPblancInfo"  # ì…ì°°ê³µê³ ì •ë³´

    async def login(self) -> bool:
        """API ê¸°ë°˜ì´ë¯€ë¡œ ë¡œê·¸ì¸ ë¶ˆí•„ìš”"""
        if not self.api_key:
            logger.warning("G2B API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            logger.warning("data.go.krì—ì„œ 'ëˆ„ë¦¬ì¥í„° ë¯¼ê°„ì…ì°°ê³µê³ ì„œë¹„ìŠ¤' API í‚¤ë¥¼ ë°œê¸‰ë°›ì•„ .env íŒŒì¼ì˜ G2B_API_KEYì— ì„¤ì •í•˜ì„¸ìš”.")
            logger.warning("ë”ë¯¸ ëª¨ë“œë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
            self.dummy_mode = True
            return False

        logger.info("G2B API í‚¤ ì¸ì¦ ì¤€ë¹„ ì™„ë£Œ")
        return True

    def setup_driver(self):
        """API ê¸°ë°˜ì´ë¯€ë¡œ WebDriver ë¶ˆí•„ìš”"""
        logger.info("G2B API í¬ë¡¤ëŸ¬ - WebDriver ì„¤ì • ìŠ¤í‚µ")
        self.dummy_mode = not bool(self.api_key)

    def teardown_driver(self):
        """API ê¸°ë°˜ì´ë¯€ë¡œ ì •ë¦¬ ì‘ì—… ë¶ˆí•„ìš”"""
        logger.info("G2B API í¬ë¡¤ëŸ¬ - WebDriver ì •ë¦¬ ìŠ¤í‚µ")

    async def search_bids(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ì…ì°° ì •ë³´ ê²€ìƒ‰"""
        if self.dummy_mode:
            logger.info("G2B API í‚¤ê°€ ì—†ì–´ ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰")
            dummy_data = self.generate_dummy_data(keywords)
            # ë”ë¯¸ ë°ì´í„°ë„ ê²°ê³¼ì— í¬í•¨ì‹œì¼œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•˜ë„ë¡
            await self._save_dummy_results(dummy_data)
            return dummy_data

        all_results = []

        try:
            # Seegene í‚¤ì›Œë“œ ì¶”ê°€
            from src.config import crawler_config
            seegene_keywords = []
            seegene_keywords.extend(crawler_config.SEEGENE_KEYWORDS['korean'][:3])  # ìƒìœ„ 3ê°œ í•œêµ­ì–´ í‚¤ì›Œë“œ
            seegene_keywords.extend(crawler_config.SEEGENE_KEYWORDS['english'][:3])  # ìƒìœ„ 3ê°œ ì˜ì–´ í‚¤ì›Œë“œ

            # ì‚¬ìš©ì í‚¤ì›Œë“œì™€ Seegene í‚¤ì›Œë“œ ê²°í•©
            search_keywords = keywords + seegene_keywords
            logger.info(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")

            # ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤ API ê²€ìƒ‰
            results = await self._search_standard_api(search_keywords)
            all_results.extend(results)

            # ì¤‘ë³µ ì œê±°
            unique_results = self._remove_duplicates(all_results)

            logger.info(f"G2B API ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(unique_results)}ê±´")
            return unique_results

        except Exception as e:
            logger.error(f"G2B API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return all_results

    async def _search_standard_api(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤ API ê²€ìƒ‰"""
        results = []

        try:
            # ê²€ìƒ‰ ê¸°ê°„ ì„¤ì • (ìµœê·¼ 30ì¼, API ì œí•œ: 1ê°œì›”)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=30)

            # í‘œì¤€ API íŒŒë¼ë¯¸í„° êµ¬ì„±
            params = {
                'ServiceKey': self.api_key,  # ëŒ€ë¬¸ì S ì£¼ì˜
                'type': 'json',
                'numOfRows': 100,
                'pageNo': 1,
                'bidNtceBgnDt': start_date.strftime('%Y%m%d%H%M'),  # ì…ì°°ê³µê³ ì‹œì‘ì¼ì‹œ
                'bidNtceEndDt': end_date.strftime('%Y%m%d%H%M')     # ì…ì°°ê³µê³ ì¢…ë£Œì¼ì‹œ
            }

            logger.info(f"ğŸ” í‘œì¤€ API ê²€ìƒ‰ - ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")

            # API í˜¸ì¶œ
            async with aiohttp.ClientSession() as session:
                url = f"{self.api_base_url}/{self.operation}"

                async with session.get(url, params=params) as response:
                    if response.status != 200:
                        logger.error(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status}")
                        return results

                    data = await response.text()
                    logger.info(f"API ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì): {data[:500]}")

                    if not data.strip():
                        logger.warning("APIì—ì„œ ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ")
                        return results

                    # XML ì˜¤ë¥˜ ì‘ë‹µ í™•ì¸
                    if data.strip().startswith('<OpenAPI_ServiceResponse>'):
                        logger.error("G2B API ì¸ì¦ ì˜¤ë¥˜ - XML ì˜¤ë¥˜ ì‘ë‹µ ìˆ˜ì‹ ")
                        if 'SERVICE_ACCESS_DENIED_ERROR' in data:
                            logger.error("ğŸš« G2B API í‚¤ ì¸ì¦ ì‹¤íŒ¨ (ì˜¤ë¥˜ì½”ë“œ: 20)")
                            logger.error("ğŸ“‹ í•´ê²° ë°©ë²•:")
                            logger.error("   1. data.go.kr ê³µê³µë°ì´í„°í¬í„¸ ì ‘ì†")
                            logger.error("   2. 'ë‚˜ë¼ì¥í„° ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤' ê²€ìƒ‰ ë° í™œìš©ì‹ ì²­")
                            logger.error("   3. ìŠ¹ì¸ëœ API í‚¤ë¥¼ .env íŒŒì¼ì˜ G2B_API_KEYì— ì„¤ì •")
                            logger.error(f"   4. í˜„ì¬ ì„¤ì •ëœ í‚¤: {self.api_key[:10]}...{self.api_key[-10:]}")
                        logger.error(f"ğŸ“„ ì „ì²´ ì˜¤ë¥˜ ì‘ë‹µ: {data}")
                        return results

                    try:
                        json_data = json.loads(data)
                        logger.info(f"JSON íŒŒì‹± ì„±ê³µ. ì‘ë‹µ êµ¬ì¡°: {list(json_data.keys()) if isinstance(json_data, dict) else type(json_data)}")
                    except json.JSONDecodeError as e:
                        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        logger.error(f"ì‘ë‹µ ë‚´ìš©: {data}")
                        return results

                    # ì‘ë‹µ ë°ì´í„° íŒŒì‹±
                    results = await self._parse_standard_api_response(json_data, keywords)

        except Exception as e:
            logger.error(f"í‘œì¤€ API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")

        return results

    async def _parse_standard_api_response(self, json_data: Dict, keywords: List[str]) -> List[Dict[str, Any]]:
        """API ì‘ë‹µ ë°ì´í„° íŒŒì‹±"""
        results = []

        try:
            # í‘œì¤€ API ì‘ë‹µ êµ¬ì¡° í™•ì¸
            if 'response' not in json_data:
                logger.warning("API ì‘ë‹µì— 'response' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return results

            response = json_data['response']

            # ê²°ê³¼ ì½”ë“œ í™•ì¸
            header = response.get('header', {})
            if header.get('resultCode') != '00':
                logger.warning(f"API ì˜¤ë¥˜: {header.get('resultMsg', 'Unknown error')}")
                return results

            # ë°ì´í„° ì¶”ì¶œ
            body = response.get('body', {})
            items = body.get('items', [])
            total_count = body.get('totalCount', 0)

            logger.info(f"ğŸ“Š ì „ì²´ ê²°ê³¼ ìˆ˜: {total_count}ê±´")
            logger.info(f"ğŸ” items íƒ€ì…: {type(items)}, ê¸¸ì´: {len(items) if isinstance(items, list) else 'N/A'}")

            if not items:
                logger.info("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return results

            # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (ë‹¨ì¼ ì•„ì´í…œì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
            if isinstance(items, dict):
                items = [items]

            for item in items:
                try:
                    # í‚¤ì›Œë“œ í•„í„°ë§ (í‘œì¤€ API í•„ë“œ ì‚¬ìš©)
                    title = item.get('ntceNm', '')  # ì…ì°°ê³µê³ ëª…
                    organization = item.get('ntceInsttNm', '')  # ê³µê³ ê¸°ê´€ëª…

                    # ë””ë²„ê¹…: ì…ì°° ì œëª© ë¡œê·¸
                    logger.info(f"ğŸ“‹ ì…ì°°ì œëª©: {title}")

                    if not self._matches_keywords(title, organization, keywords):
                        logger.info(f"âŒ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨: {title[:50]}...")
                        continue

                    logger.info(f"âœ… í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ: {title[:50]}...")

                    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                    relevance_score = self.calculate_relevance_score(title, organization)

                    # ê¸´ê¸‰ë„ ë ˆë²¨ ê³„ì‚°
                    deadline_date = item.get('bidClseDate', '')  # ì…ì°°ë§ˆê°ì¼ì
                    urgency_level = self.determine_urgency_level(deadline_date)

                    # ì…ì°°ì •ë³´ êµ¬ì„± (í‘œì¤€ API í•„ë“œ ë§¤í•‘)
                    bid_info = {
                        "title": title,
                        "organization": organization,
                        "bid_number": item.get('bidNtceNo', ''),  # ì…ì°°ê³µê³ ë²ˆí˜¸
                        "announcement_date": item.get('nticeDt', ''),  # ì…ì°°ê³µê³ ì¼ì
                        "deadline_date": deadline_date,
                        "estimated_price": self._format_price(item.get('presmptPrce', '')),  # ì¶”ì •ê°€ê²©
                        "currency": "KRW",
                        "source_url": item.get('bidNtceUrl', ''),  # ì…ì°°ê³µê³ URL
                        "source_site": "G2B",
                        "country": "KR",
                        "keywords": self._extract_keywords(title, organization),
                        "relevance_score": relevance_score,
                        "urgency_level": urgency_level,
                        "status": "active",
                        "extra_data": {
                            "crawled_at": datetime.now().isoformat(),
                            "bid_order": item.get('bidNtceOrd', ''),  # ì…ì°°ê³µê³ ì°¨ìˆ˜
                            "business_division": item.get('bsnsDivNm', ''),  # ì—…ë¬´êµ¬ë¶„ëª…
                            "contract_method": item.get('cntrctCnclsMthdNm', ''),  # ê³„ì•½ì²´ê²°ë°©ë²•ëª…
                            "contract_type": item.get('cntrctCnclsSttusNm', ''),  # ê³„ì•½ì²´ê²°í˜•íƒœëª…
                            "decision_method": item.get('bidwinrDcsnMthdNm', ''),  # ë‚™ì°°ìê²°ì •ë°©ë²•ëª…
                            "opening_date": item.get('opengDate', ''),  # ê°œì°°ì¼ì
                            "opening_time": item.get('opengTm', ''),  # ê°œì°°ì‹œê°
                            "opening_place": item.get('opengPlce', ''),  # ê°œì°°ì¥ì†Œ
                            "budget_amount": self._format_price(item.get('asignBdgtAmt', '')),  # ë°°ì •ì˜ˆì‚°ê¸ˆì•¡
                            "international_bid": item.get('intrntnlBidYn', ''),  # êµ­ì œì…ì°°ì—¬ë¶€
                            "electronic_bid": item.get('elctrnBidYn', ''),  # ì „ìì…ì°°ì—¬ë¶€
                            "demand_institution": item.get('dmndInsttNm', ''),  # ìˆ˜ìš”ê¸°ê´€ëª…
                            "notice_status": item.get('bidNtceSttusNm', ''),  # ì…ì°°ê³µê³ ìƒíƒœëª…
                            "region_limit": item.get('rgnLmtYn', ''),  # ì§€ì—­ì œí•œì—¬ë¶€
                            "industry_limit": item.get('indstrytyLmtYn', ''),  # ì—…ì¢…ì œí•œì—¬ë¶€
                            "api_data": True,
                            "api_version": "standard"
                        }
                    }

                    results.append(bid_info)

                except Exception as e:
                    logger.warning(f"ê°œë³„ ì•„ì´í…œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
            logger.error(f"API ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")

        return results

    def _matches_keywords(self, title: str, organization: str, keywords: List[str]) -> bool:
        """í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸"""
        from src.config import crawler_config

        text = f"{title} {organization}".lower()

        # Seegene í‚¤ì›Œë“œ í™•ì¸
        all_keywords = []
        all_keywords.extend(crawler_config.SEEGENE_KEYWORDS['korean'])
        all_keywords.extend(crawler_config.SEEGENE_KEYWORDS['english'])

        for keyword in all_keywords:
            if keyword.lower() in text:
                return True

        # ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¸
        for keyword in keywords:
            if keyword.lower() in text:
                return True

        return False

    def _extract_keywords(self, title: str, organization: str = "") -> List[str]:
        """ì œëª©ê³¼ ê¸°ê´€ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        text_lower = f"{title} {organization}".lower()

        from src.config import crawler_config
        for keyword in crawler_config.SEEGENE_KEYWORDS['korean']:
            if keyword.lower() in text_lower:
                keywords.append(keyword)
        for keyword in crawler_config.SEEGENE_KEYWORDS['english']:
            if keyword.lower() in text_lower:
                keywords.append(keyword)

        return list(set(keywords))  # ì¤‘ë³µ ì œê±°

    def _format_date(self, date_str: str) -> str:
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜"""
        if not date_str:
            return ""

        try:
            # API ì‘ë‹µ í˜•ì‹: "2025-01-15 14:30:00"
            if len(date_str) >= 10:
                return date_str[:10]  # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            return date_str
        except:
            return date_str

    def _format_price(self, price_str: str) -> str:
        """ê°€ê²© í˜•ì‹ ë³€í™˜"""
        if not price_str:
            return ""

        try:
            # ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ ì›í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            price_num = int(price_str)
            return f"{price_num:,}ì›"
        except:
            return price_str

    def _generate_detail_url(self, bid_number: str) -> str:
        """ìƒì„¸ í˜ì´ì§€ URL ìƒì„±"""
        if not bid_number:
            return ""
        return f"https://www.g2b.go.kr/ep/invitation/publish/bidInfoDtl/bidInfoDtl.do?bidNo={bid_number}"

    def _remove_duplicates(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±°"""
        seen_bid_numbers = set()
        unique_results = []

        for result in results:
            bid_number = result.get('bid_number', '')
            if bid_number and bid_number not in seen_bid_numbers:
                seen_bid_numbers.add(bid_number)
                unique_results.append(result)
            elif not bid_number:  # bid_numberê°€ ì—†ëŠ” ê²½ìš° URLë¡œ ì¤‘ë³µ ì²´í¬
                source_url = result.get('source_url', '')
                if source_url not in [r.get('source_url', '') for r in unique_results]:
                    unique_results.append(result)

        return unique_results

    def parse_deadline(self, deadline_str: str) -> Optional[datetime]:
        """G2B API ë§ˆê°ì¼ íŒŒì‹±"""
        try:
            if not deadline_str or deadline_str.strip() == "":
                return None

            # API ì‘ë‹µ í˜•ì‹: "2025-01-15 14:30:00"
            for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d"]:
                try:
                    return datetime.strptime(deadline_str.strip(), fmt)
                except ValueError:
                    continue

            return None

        except Exception:
            return None

    def generate_dummy_data(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ë”ë¯¸ ë°ì´í„° ìƒì„± (API í‚¤ê°€ ì—†ì„ ë•Œ)"""
        dummy_bids = []

        categories = ["ìš©ì—­", "ë¬¼í’ˆ", "ê³µì‚¬", "ê¸°íƒ€"]

        for i, keyword in enumerate(keywords[:2]):  # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œ
            for j, category in enumerate(categories):
                dummy_bid = {
                    "title": f"[API í…ŒìŠ¤íŠ¸] {keyword} ê´€ë ¨ {category} ì…ì°°ê³µê³  {i+1}-{j+1}",
                    "organization": f"í…ŒìŠ¤íŠ¸ê¸°ê´€{i+1}-{j+1}",
                    "bid_number": f"API-TEST-{datetime.now().strftime('%Y%m%d')}-{i+1:02d}{j+1:02d}",
                    "announcement_date": datetime.now().strftime("%Y-%m-%d"),
                    "deadline_date": (datetime.now() + timedelta(days=7+i+j)).strftime("%Y-%m-%d"),
                    "estimated_price": f"{(i+j+1)*15000000:,}ì›",
                    "currency": "KRW",
                    "source_url": f"https://test.g2b.go.kr/bid/{i+1}{j+1}",
                    "source_site": "G2B",
                    "country": "KR",
                    "keywords": [keyword],
                    "relevance_score": 8.5 - (i+j)*0.5,
                    "urgency_level": "medium",
                    "status": "active",
                    "extra_data": {
                        "crawled_at": datetime.now().isoformat(),
                        "category": category,
                        "api_data": False,
                        "dummy_data": True,
                        "note": "G2B API í‚¤ ì—†ìŒìœ¼ë¡œ ì¸í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°"
                    }
                }
                dummy_bids.append(dummy_bid)

        logger.info(f"G2B API ë”ë¯¸ ë°ì´í„° {len(dummy_bids)}ê±´ ìƒì„±")
        return dummy_bids