"""
French BOAMP/PLACE í¬ë¡¤ëŸ¬
í”„ë‘ìŠ¤ ê³µê³µì¡°ë‹¬ í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import ssl
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import quote, urljoin

import aiohttp
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta

from ..config import settings
from ..crawler.base import BaseCrawler
from ..models.tender_notice import (
    TenderNotice, TenderStatus, TenderType, ProcurementMethod,
    TenderValue, Organization, Classification, TenderDocument,
    CurrencyCode,
)
from ..utils.cpv_filter import cpv_filter
from ..utils.logger import get_logger

logger = get_logger(__name__)


DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": (
        "text/html,application/xhtml+xml,application/xml;q=0.9,"
        "application/rss+xml;q=0.9,*/*;q=0.8"
    ),
    "Accept-Language": "en-US,en;q=0.9",
    "Connection": "keep-alive",
}


def create_ssl_context():
    """SSL ê²€ì¦ ìš°íšŒë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE

    custom_ca_bundle = getattr(settings, "SSL_CUSTOM_CA_BUNDLE", None)
    if custom_ca_bundle:
        ca_path = Path(custom_ca_bundle)
        if not ca_path.is_absolute():
            ca_path = Path.cwd() / ca_path

        if ca_path.exists():
            try:
                ssl_context.load_verify_locations(cafile=str(ca_path))
                ssl_context.check_hostname = True
                ssl_context.verify_mode = ssl.CERT_REQUIRED
                logger.info(f"ì»¤ìŠ¤í…€ CA ë²ˆë“¤ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {ca_path}")
            except Exception as exc:
                logger.warning(f"ì»¤ìŠ¤í…€ CA ë²ˆë“¤ ë¡œë“œ ì‹¤íŒ¨: {exc}")
        else:
            logger.warning(f"ì§€ì •ëœ CA ë²ˆë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ca_path}")

    return ssl_context


class FranceBOAMPCrawler(BaseCrawler):
    """í”„ë‘ìŠ¤ BOAMP/PLACE ê³µê³µì¡°ë‹¬ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        super().__init__("FR_BOAMP", "FR")

        # BOAMP API ì„¤ì •
        self.boamp_base_url = "https://www.boamp.fr"
        self.place_base_url = "https://www.marches-publics.gouv.fr"

        # OpenDataSoft API ì—”ë“œí¬ì¸íŠ¸ (BOAMPëŠ” OpenDataSoft í”Œë«í¼ ì‚¬ìš©)
        self.api_base_url = f"{self.boamp_base_url}/api"
        self.records_api = f"{self.api_base_url}/records/1.0/search/"

        # RSS/XML í”¼ë“œ URLë“¤ (OpenDataSoft í˜•ì‹)
        self.rss_feeds = [
            f"{self.api_base_url}/records/1.0/search/?format=rss",
            f"{self.api_base_url}/feeds/rss"
        ]

        # ê²€ìƒ‰ í˜ì´ì§€ URL
        self.search_page_url = f"{self.boamp_base_url}/pages/recherche/"

        # CPV ì½”ë“œ í•„í„° (ì˜ë£Œê¸°ê¸° ê´€ë ¨)
        self.healthcare_cpv_codes = [
            "33100000",  # ì˜ë£Œê¸°ê¸°
            "33140000",  # ì˜ë£Œìš©í’ˆ
            "33183000",  # ì§„ë‹¨ê¸°ê¸°
            "33184000",  # ì‹¤í—˜ì‹¤ ê¸°ê¸°
            "33600000",  # ì˜ì•½í’ˆ
            "33700000",  # ê°œì¸ë³´í˜¸ì¥ë¹„
        ]

    async def crawl(self, keywords: List[str] = None) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"í”„ë‘ìŠ¤ BOAMP í¬ë¡¤ë§ ì‹œì‘ - í‚¤ì›Œë“œ: {keywords}")

        results = []

        try:
            # OpenDataSoft API ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„)
            if keywords:
                api_results = await self._crawl_api_search(keywords)
                results.extend(api_results)

            # API ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ RSS í”¼ë“œ ì‹œë„
            if not results:
                rss_results = await self._crawl_rss_feeds(keywords)
                results.extend(rss_results)

            # ì—¬ì „íˆ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰ ì‹œë„
            if not results and keywords:
                web_results = await self._crawl_web_search(keywords)
                results.extend(web_results)

            # ê²°ê³¼ ì¤‘ë³µ ì œê±°
            unique_results = self._remove_duplicates(results)

            logger.info(f"í”„ë‘ìŠ¤ BOAMP í¬ë¡¤ë§ ì™„ë£Œ - ì´ {len(unique_results)}ê±´ ìˆ˜ì§‘")

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if unique_results:
                try:
                    await DatabaseManager.save_bid_info(unique_results)
                    logger.info(f"ğŸ’¾ FR_BOAMP ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(unique_results)}ê±´")
                except Exception as e:
                    logger.error(f"âŒ FR_BOAMP ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            else:
                logger.info("ğŸ“ FR_BOAMP ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

            return {
                "success": True,
                "total_collected": len(unique_results),
                "results": unique_results,
                "source": "FR_BOAMP",
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"í”„ë‘ìŠ¤ BOAMP í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": results,
                "source": "FR_BOAMP",
                "total_collected": 0,
                "timestamp": datetime.now().isoformat()
            }

    async def _crawl_api_search(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """OpenDataSoft APIë¥¼ í†µí•œ ê²€ìƒ‰"""
        results = []

        connector = aiohttp.TCPConnector(ssl=create_ssl_context())
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=45),
            connector=connector,
            headers=DEFAULT_HEADERS,
        ) as session:

            for keyword in keywords[:3]:  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
                try:
                    logger.info(f"API ê²€ìƒ‰: {keyword}")

                    # OpenDataSoft API íŒŒë¼ë¯¸í„°
                    api_params = {
                        "dataset": "boamp",  # ì¶”ì •ë˜ëŠ” ë°ì´í„°ì…‹ ì´ë¦„
                        "q": keyword,
                        "rows": 20,
                        "start": 0,
                        "format": "json",
                        "facet": ["type_de_marche", "procedure", "cpv"]
                    }

                    async with session.get(
                        self.records_api,
                        params=api_params,
                        headers=DEFAULT_HEADERS,
                    ) as response:
                        if response.status == 200:
                            try:
                                data = await response.json()
                                api_results = await self._parse_api_response(data, keyword)
                                results.extend(api_results)
                                logger.info(f"APIì—ì„œ {len(api_results)}ê±´ ìˆ˜ì§‘")
                            except json.JSONDecodeError as e:
                                logger.warning(f"API ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        else:
                            logger.warning(f"API ê²€ìƒ‰ ì‹¤íŒ¨: {response.status}")

                    # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                    await asyncio.sleep(2)

                except Exception as e:
                    logger.warning(f"API ê²€ìƒ‰ ì˜¤ë¥˜ {keyword}: {e}")

        return results

    async def _parse_api_response(self, data: Dict[str, Any], keyword: str) -> List[Dict[str, Any]]:
        """API ì‘ë‹µ ë°ì´í„° íŒŒì‹±"""
        results = []

        try:
            records = data.get("records", [])
            total_hits = data.get("nhits", 0)
            logger.info(f"API ì‘ë‹µ: ì´ {total_hits}ê±´ ì¤‘ {len(records)}ê±´ ì²˜ë¦¬")

            for record in records:
                try:
                    fields = record.get("fields", {})
                    record_id = record.get("recordid", "")

                    # BOAMP ë°ì´í„°ëŠ” 'donnees' í•„ë“œì— JSON ë¬¸ìì—´ë¡œ ì €ì¥ë¨
                    donnees_str = fields.get("donnees", "")
                    nature_libelle = fields.get("nature_libelle", "")

                    title = ""
                    organization = ""
                    estimated_value = None
                    cpv_codes = []
                    description = f"í‚¤ì›Œë“œ: {keyword}"

                    # JSON ë°ì´í„° íŒŒì‹±
                    if donnees_str:
                        try:
                            donnees = json.loads(donnees_str)

                            # ì œëª© ì¶”ì¶œ
                            if "OBJET" in donnees:
                                objet = donnees["OBJET"]
                                title = objet.get("TITRE_MARCHE", "")
                                if "OBJET_COMPLET" in objet:
                                    description = objet["OBJET_COMPLET"]

                                # CPV ì½”ë“œ
                                if "CPV" in objet and "PRINCIPAL" in objet["CPV"]:
                                    cpv_codes = [objet["CPV"]["PRINCIPAL"]]

                                # ê°€ê²© ì •ë³´
                                if "CARACTERISTIQUES" in objet and "VALEUR" in objet["CARACTERISTIQUES"]:
                                    valeur = objet["CARACTERISTIQUES"]["VALEUR"]
                                    if isinstance(valeur, dict) and "#text" in valeur:
                                        estimated_value = self._parse_value(valeur["#text"])
                                    elif isinstance(valeur, str):
                                        estimated_value = self._parse_value(valeur)

                            # ê¸°ê´€ëª… ì¶”ì¶œ
                            if "IDENTITE" in donnees:
                                identite = donnees["IDENTITE"]
                                organization = identite.get("DENOMINATION", "")

                        except json.JSONDecodeError as e:
                            logger.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                            title = f"BOAMP ê³µê³  - {keyword}"

                    # ê¸°ë³¸ê°’ ì„¤ì •
                    if not title:
                        title = f"BOAMP ê³µê³  - {keyword}"
                    if not organization:
                        organization = "í”„ë‘ìŠ¤ ê³µê³µê¸°ê´€"

                    # URL êµ¬ì„±
                    source_url = f"{self.boamp_base_url}/avis/{record_id}" if record_id else ""

                    tender_info = {
                        "title": title[:200].strip(),
                        "description": description[:500] if description else f"í‚¤ì›Œë“œ: {keyword}",
                        "organization": organization.strip(),
                        "source_url": source_url,
                        "publication_date": "",  # API ì‘ë‹µì—ì„œ ì§ì ‘ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•¨
                        "deadline_date": "",
                        "estimated_value": estimated_value,
                        "currency": "EUR",
                        "source_site": "BOAMP",
                        "country": "FR",
                        "cpv_codes": cpv_codes,
                        "keywords": [keyword],
                        "tender_type": self._determine_tender_type(title),
                        "notice_type": "API",
                        "language": "fr",
                        "record_id": record_id,
                        "nature": nature_libelle
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í•„í„°ë§
                    if self._is_healthcare_related(tender_info):
                        results.append(tender_info)
                        logger.debug(f"ì˜ë£Œê¸°ê¸° ê´€ë ¨ ê³µê³  ë°œê²¬: {title[:100]}")

                except Exception as e:
                    logger.warning(f"API ë ˆì½”ë“œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
            logger.warning(f"API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def _parse_value(self, value_str: str) -> Optional[float]:
        """ê°€ê²© ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜"""
        if not value_str:
            return None

        try:
            # ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ì ì œê±°
            import re
            numeric_str = re.sub(r'[^\d.,]', '', str(value_str))
            numeric_str = numeric_str.replace(',', '.')

            if numeric_str:
                return float(numeric_str)
        except (ValueError, TypeError):
            pass

        return None

    async def _crawl_rss_feeds(self, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œì—ì„œ ê³µê³  ìˆ˜ì§‘"""
        results = []

        connector = aiohttp.TCPConnector(ssl=create_ssl_context())
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=connector,
            headers=DEFAULT_HEADERS,
        ) as session:

            for feed_url in self.rss_feeds:
                try:
                    logger.info(f"RSS í”¼ë“œ í¬ë¡¤ë§: {feed_url}")

                    async with session.get(feed_url, headers=DEFAULT_HEADERS) as response:
                        if response.status == 200:
                            content = await response.text()
                            feed_results = await self._parse_rss_feed(content, keywords)
                            results.extend(feed_results)
                            logger.info(f"RSSì—ì„œ {len(feed_results)}ê±´ ìˆ˜ì§‘")
                        else:
                            logger.warning(f"RSS í”¼ë“œ ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")

                except Exception as e:
                    logger.warning(f"RSS í”¼ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜ {feed_url}: {e}")

        return results

    async def _parse_rss_feed(self, content: str, keywords: List[str] = None) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        results = []

        try:
            # XML íŒŒì‹± ì‹œë„
            root = ET.fromstring(content)

            # RSS 2.0 í˜•ì‹ ì²˜ë¦¬
            items = root.findall(".//item")

            for item in items:
                try:
                    title = item.find("title")
                    title_text = title.text if title is not None else ""

                    description = item.find("description")
                    description_text = description.text if description is not None else ""

                    link = item.find("link")
                    link_url = link.text if link is not None else ""

                    pub_date = item.find("pubDate")
                    pub_date_text = pub_date.text if pub_date is not None else ""

                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if keywords and not self._matches_keywords(title_text + " " + description_text, keywords):
                        continue

                    # ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ì— ë§ëŠ” ê³µê³  ì •ë³´ êµ¬ì„±
                    tender_info = {
                        "title": title_text.strip()[:500],  # ê¸¸ì´ ì œí•œ
                        "organization": self._extract_organization(description_text),
                        "bid_number": f"FR-RSS-{datetime.now().strftime('%Y%m%d')}-{len(results)+1:03d}",
                        "announcement_date": self._parse_date(pub_date_text),
                        "deadline_date": self._extract_deadline(description_text) or self._estimate_deadline_date(),
                        "estimated_price": str(self._extract_value(description_text)) if self._extract_value(description_text) else "",
                        "currency": "EUR",
                        "source_url": link_url.strip(),
                        "source_site": "FR_BOAMP",
                        "country": "FR",
                        "keywords": keywords or [],
                        "relevance_score": self._calculate_relevance_score(title_text, keywords[0] if keywords else ""),
                        "urgency_level": "medium",
                        "status": "active",
                        "extra_data": {
                            "description": description_text.strip()[:1000],  # ê¸¸ì´ ì œí•œ
                            "tender_type": self._determine_tender_type(title_text),
                            "cpv_codes": self._extract_cpv_codes(description_text),
                            "notice_type": "RSS",
                            "language": "fr",
                            "crawled_at": datetime.now().isoformat()
                        }
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í•„í„°ë§
                    if self._is_healthcare_related(tender_info):
                        results.append(tender_info)

                except Exception as e:
                    logger.warning(f"RSS ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except ET.ParseError as e:
            logger.warning(f"RSS XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    async def _crawl_web_search(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ì›¹ ê²€ìƒ‰ì„ í†µí•œ ê³µê³  ìˆ˜ì§‘"""
        results = []

        connector = aiohttp.TCPConnector(ssl=create_ssl_context())
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=45),
            connector=connector,
            headers=DEFAULT_HEADERS,
        ) as session:

            for keyword in keywords[:3]:  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
                try:
                    logger.info(f"ì›¹ ê²€ìƒ‰: {keyword}")

                    # BOAMP ê²€ìƒ‰ í˜ì´ì§€
                    search_url = self.search_page_url
                    search_params = {
                        "q": keyword,
                        "search": keyword,
                        "type": "all"
                    }

                    async with session.get(
                        search_url,
                        params=search_params,
                        headers=DEFAULT_HEADERS,
                    ) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            search_results = await self._parse_search_results(html_content, keyword)
                            results.extend(search_results)
                            logger.info(f"ì›¹ ê²€ìƒ‰ì—ì„œ {len(search_results)}ê±´ ìˆ˜ì§‘")
                        else:
                            logger.warning(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status}")

                    # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                    await asyncio.sleep(2)

                except Exception as e:
                    logger.warning(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ {keyword}: {e}")

        return results

    async def _parse_search_results(self, html_content: str, keyword: str) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ HTML íŒŒì‹±"""
        results = []

        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')

            # OpenDataSoft ê¸°ë°˜ ê²°ê³¼ ê²€ìƒ‰
            result_items = soup.find_all(['div', 'article'], class_=lambda x: x and ('record' in x.lower() or 'result' in x.lower() or 'notice' in x.lower()))

            if not result_items:
                # ì¼ë°˜ì ì¸ HTML êµ¬ì¡°ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì°¾ê¸°
                result_items = soup.find_all(['div', 'article', 'li'], class_=lambda x: x and any(term in x.lower() for term in ['item', 'entry', 'card', 'box']))

            if not result_items:
                # ì œëª© íƒœê·¸ë¡œ ê²€ìƒ‰ ì‹œë„
                result_items = soup.find_all(['h2', 'h3', 'h4'])

            logger.info(f"HTMLì—ì„œ {len(result_items)}ê°œ ìš”ì†Œ ë°œê²¬")

            for item in result_items[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    title = ""
                    link_url = ""
                    organization = ""
                    description = ""

                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.find(['h1', 'h2', 'h3', 'h4', 'h5'])
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                    elif item.name in ['h1', 'h2', 'h3', 'h4', 'h5']:
                        title = item.get_text(strip=True)

                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.find('a', href=True)
                    if link_elem:
                        link_url = urljoin(self.boamp_base_url, link_elem['href'])
                    elif item.name == 'a' and item.get('href'):
                        link_url = urljoin(self.boamp_base_url, item['href'])

                    # ê¸°ê´€ëª… ì¶”ì¶œ
                    org_elem = item.find(string=lambda text: text and ('ministÃ¨re' in text.lower() or 'mairie' in text.lower() or 'conseil' in text.lower()))
                    if org_elem:
                        organization = org_elem.strip()

                    # ì„¤ëª… ì¶”ì¶œ
                    desc_elem = item.find(['p', 'div'], class_=lambda x: x and 'description' in x.lower())
                    if desc_elem:
                        description = desc_elem.get_text(strip=True)

                    # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ì²˜ë¦¬
                    full_text = f"{title} {description}".lower()
                    if keyword.lower() not in full_text:
                        continue

                    if title:  # ì œëª©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
                        tender_info = {
                            "title": title[:200],  # ì œëª© ê¸¸ì´ ì œí•œ
                            "description": description[:500] if description else f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}",
                            "organization": organization if organization else "í”„ë‘ìŠ¤ ê³µê³µê¸°ê´€",
                            "source_url": link_url,
                            "publication_date": datetime.now().date().isoformat(),
                            "source_site": "BOAMP",
                            "country": "FR",
                            "currency": "EUR",
                            "tender_type": self._determine_tender_type(title),
                            "keywords": [keyword],
                            "notice_type": "WEB_SEARCH",
                            "language": "fr"
                        }

                        # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í•„í„°ë§
                        if self._is_healthcare_related(tender_info):
                            results.append(tender_info)

                except Exception as e:
                    logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            # BeautifulSoupê°€ ì—†ëŠ” ê²½ìš° ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©
        except ImportError:
            logger.warning("BeautifulSoup4ê°€ ì—†ì–´ ì •ê·œí‘œí˜„ì‹ íŒŒì‹± ì‚¬ìš©")
            results = await self._parse_search_results_regex(html_content, keyword)
        except Exception as e:
            logger.warning(f"HTML íŒŒì‹± ì˜¤ë¥˜: {e}")
            # ì •ê·œí‘œí˜„ì‹ íŒŒì‹±ìœ¼ë¡œ í´ë°±
            results = await self._parse_search_results_regex(html_content, keyword)

        return results

    async def _parse_search_results_regex(self, html_content: str, keyword: str) -> List[Dict[str, Any]]:
        """ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± (í´ë°±)"""
        results = []

        try:
            import re

            # ê°œì„ ëœ íŒ¨í„´ë“¤
            patterns = [
                # í”„ë‘ìŠ¤ ê³µê³µì¡°ë‹¬ ê´€ë ¨ ì œëª© íŒ¨í„´
                r'<h[2-4][^>]*>([^<]*(?:marchÃ©|appel|consultation|avis|offre)[^<]*)</h[2-4]>',
                # ì¼ë°˜ì ì¸ ì œëª© íŒ¨í„´
                r'<h[2-4][^>]*class="[^"]*title[^"]*"[^>]*>([^<]+)</h[2-4]>',
                # data-* ì†ì„±ì´ ìˆëŠ” ì œëª©
                r'data-title="([^"]*)',
                # aria-label ì†ì„±
                r'aria-label="([^"]*(?:marchÃ©|appel|consultation)[^"]*)"'
            ]

            all_titles = []
            for pattern in patterns:
                titles = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
                all_titles.extend(titles)

            # ë§í¬ íŒ¨í„´ (ë” í¬ê´„ì )
            link_patterns = [
                r'href="([^"]*(?:avis|notice|marche)[^"]*)"',
                r'href="(/[^"]*detail[^"]*)"',
                r'href="(/[^"]*record[^"]*)"'
            ]

            all_links = []
            for pattern in link_patterns:
                links = re.findall(pattern, html_content, re.IGNORECASE)
                all_links.extend(links)

            # ì œëª©ê³¼ ë§í¬ ë§¤ì¹­
            for i, title in enumerate(all_titles[:10]):
                try:
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if keyword.lower() not in title.lower():
                        continue

                    link_url = ""
                    if i < len(all_links):
                        link_url = urljoin(self.boamp_base_url, all_links[i])

                    title_clean = re.sub(r'<[^>]+>', '', title).strip()

                    # ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ì— ë§ëŠ” êµ¬ì¡°ë¡œ ìˆ˜ì •
                    tender_info = {
                        "title": title_clean[:200],
                        "organization": self._extract_organization(title_clean),
                        "bid_number": f"FR-{datetime.now().strftime('%Y%m%d')}-{i+1:03d}",
                        "announcement_date": datetime.now().date().isoformat(),
                        "deadline_date": self._estimate_deadline_date(),
                        "estimated_price": "",  # BOAMPì—ì„œëŠ” ê°€ê²© ì •ë³´ ì œí•œì 
                        "currency": "EUR",
                        "source_url": link_url or f"https://www.boamp.fr/search?q={keyword}",
                        "source_site": "FR_BOAMP",
                        "country": "FR",
                        "keywords": [keyword],
                        "relevance_score": self._calculate_relevance_score(title_clean, keyword),
                        "urgency_level": "medium",
                        "status": "active",
                        "extra_data": {
                            "description": f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}",
                            "tender_type": self._determine_tender_type(title_clean),
                            "notice_type": "WEB_SEARCH",
                            "language": "fr",
                            "crawled_at": datetime.now().isoformat()
                        }
                    }

                    # ì˜ë£Œê¸°ê¸° ê´€ë ¨ í•„í„°ë§
                    if self._is_healthcare_related(tender_info):
                        results.append(tender_info)

                except Exception as e:
                    logger.warning(f"ì •ê·œí‘œí˜„ì‹ íŒŒì‹± ì•„ì´í…œ ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
            logger.warning(f"ì •ê·œí‘œí˜„ì‹ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def _matches_keywords(self, text: str, keywords: List[str]) -> bool:
        """í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸"""
        if not keywords:
            return True

        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in keywords)

    def _determine_tender_type(self, title: str) -> str:
        """ê³µê³  ìœ í˜• íŒë‹¨"""
        title_lower = title.lower()

        if "appel" in title_lower or "offres" in title_lower:
            return "OPEN"
        elif "consultation" in title_lower:
            return "RESTRICTED"
        elif "marchÃ©" in title_lower:
            return "CONTRACT"
        else:
            return "OTHER"

    def _extract_organization(self, text: str) -> str:
        """ë°œì£¼ê¸°ê´€ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ê¸°ê´€ëª… ì¶”ì¶œ ì‹œë„
        import re

        org_patterns = [
            r"(MinistÃ¨re[^,\n]+)",
            r"(Conseil [^,\n]+)",
            r"(Mairie [^,\n]+)",
            r"(HÃ´pital [^,\n]+)",
            r"(CHU [^,\n]+)",
            r"(APHP[^,\n]*)",
        ]

        for pattern in org_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return "í”„ë‘ìŠ¤ ê³µê³µê¸°ê´€"

    def _extract_organization(self, title: str) -> str:
        """ì œëª©ì—ì„œ ê¸°ê´€ëª… ì¶”ì¶œ"""
        import re

        # í”„ë‘ìŠ¤ ê¸°ê´€ëª… íŒ¨í„´ë“¤
        org_patterns = [
            r"(CHU [^,\n\-]+)",
            r"(HÃ´pital [^,\n\-]+)",
            r"(APHP[^,\n\-]*)",
            r"(Centre [^,\n\-]+)",
            r"(UniversitÃ© [^,\n\-]+)",
            r"(MinistÃ¨re [^,\n\-]+)",
        ]

        for pattern in org_patterns:
            match = re.search(pattern, title, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return "í”„ë‘ìŠ¤ ê³µê³µê¸°ê´€"

    def _estimate_deadline_date(self) -> str:
        """ë§ˆê°ì¼ ì¶”ì • (BOAMPì—ì„œ ì •í™•í•œ ë‚ ì§œë¥¼ ì–»ê¸° ì–´ë ¤ìš°ë¯€ë¡œ)"""
        from datetime import timedelta
        estimated_deadline = datetime.now() + timedelta(days=30)
        return estimated_deadline.date().isoformat()

    def _calculate_relevance_score(self, title: str, keyword: str) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not title or not keyword:
            return 5.0

        title_lower = title.lower()
        keyword_lower = keyword.lower()

        # ê¸°ë³¸ ì ìˆ˜
        score = 5.0

        # í‚¤ì›Œë“œê°€ ì œëª©ì— ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
        if keyword_lower in title_lower:
            score += 2.0

        # ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
        medical_keywords = ['medical', 'health', 'hospital', 'clinical', 'diagnostic']
        for med_keyword in medical_keywords:
            if med_keyword in title_lower:
                score += 1.0
                break

        return min(score, 10.0)  # ìµœëŒ€ 10ì 

    def _extract_cpv_codes(self, text: str) -> List[str]:
        """CPV ì½”ë“œ ì¶”ì¶œ"""
        import re

        # CPV ì½”ë“œ íŒ¨í„´ (8ìë¦¬ ìˆ«ì)
        cpv_pattern = r"CPV\s*:?\s*(\d{8})"
        matches = re.findall(cpv_pattern, text, re.IGNORECASE)

        return matches if matches else []

    def _extract_value(self, text: str) -> Optional[float]:
        """ì¶”ì •ê°€ê²© ì¶”ì¶œ"""
        import re

        # ê¸ˆì•¡ íŒ¨í„´ (ìœ ë¡œ)
        value_patterns = [
            r"(\d+(?:\s*\d+)*(?:,\d+)?)\s*â‚¬",
            r"â‚¬\s*(\d+(?:\s*\d+)*(?:,\d+)?)",
            r"(\d+(?:\s*\d+)*(?:,\d+)?)\s*euros?",
        ]

        for pattern in value_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    value_str = match.group(1).replace(" ", "").replace(",", ".")
                    return float(value_str)
                except ValueError:
                    continue

        return None

    def _extract_deadline(self, text: str) -> Optional[str]:
        """ë§ˆê°ì¼ ì¶”ì¶œ"""
        import re

        # ë‚ ì§œ íŒ¨í„´
        date_patterns = [
            r"(\d{1,2}/\d{1,2}/\d{4})",
            r"(\d{1,2}-\d{1,2}-\d{4})",
            r"(\d{4}-\d{1,2}-\d{1,2})",
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)

        return None

    def _parse_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            # RSS pubDate í˜•ì‹ íŒŒì‹±
            from datetime import datetime

            # ì¼ë°˜ì ì¸ RSS ë‚ ì§œ í˜•ì‹ë“¤
            formats = [
                "%a, %d %b %Y %H:%M:%S %Z",
                "%a, %d %b %Y %H:%M:%S %z",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d",
            ]

            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.date().isoformat()
                except ValueError:
                    continue

            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜
            return datetime.now().date().isoformat()

        except Exception:
            return datetime.now().date().isoformat()

    def _is_healthcare_related(self, tender_info: Dict[str, Any]) -> bool:
        """ì˜ë£Œê¸°ê¸° ê´€ë ¨ ê³µê³ ì¸ì§€ í™•ì¸"""
        # CPV ì½”ë“œ í™•ì¸
        cpv_codes = tender_info.get("cpv_codes", [])
        if any(cpv.startswith(hc) for cpv in cpv_codes for hc in ["331", "336", "337"]):
            return True

        # í‚¤ì›Œë“œ í™•ì¸
        text = f"{tender_info.get('title', '')} {tender_info.get('description', '')}".lower()

        healthcare_keywords = [
            "mÃ©dical", "mÃ©decin", "santÃ©", "hÃ´pital", "clinique",
            "diagnostic", "laboratoire", "Ã©quipement mÃ©dical",
            "dispositif mÃ©dical", "matÃ©riel mÃ©dical", "chu", "aphp"
        ]

        return any(keyword in text for keyword in healthcare_keywords)

    def _remove_duplicates(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±°"""
        seen_urls = set()
        unique_results = []

        for result in results:
            url = result.get("source_url", "")
            title = result.get("title", "")

            # URL ë˜ëŠ” ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            key = url if url else title
            if key and key not in seen_urls:
                seen_urls.add(key)
                unique_results.append(result)

        return unique_results

    async def login(self) -> bool:
        """ë¡œê·¸ì¸ - í”„ë‘ìŠ¤ BOAMPëŠ” ê³µê°œ ì‚¬ì´íŠ¸ì´ë¯€ë¡œ ë¡œê·¸ì¸ ë¶ˆí•„ìš”"""
        return True

    async def search_bids(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ì…ì°° ì •ë³´ ê²€ìƒ‰ - crawl ë©”ì„œë“œë¥¼ í˜¸ì¶œ"""
        result = await self.crawl(keywords)
        return result.get("results", [])