"""
Crawler Manager
í¬ë¡¤ëŸ¬ ê´€ë¦¬ ë° ìŠ¤ì¼€ì¤„ë§
"""

import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

from src.crawler.g2b_crawler import G2BCrawler
from src.crawler.samgov_crawler import SAMGovCrawler
from src.crawler.ted_crawler import TEDCrawler
from src.crawler.uk_fts_crawler import UKFTSCrawler
from src.crawler.fr_boamp_crawler import FranceBOAMPCrawler
from src.crawler.de_vergabestellen_crawler import GermanyVergabestellenCrawler
from src.crawler.it_mepa_crawler import ItalyMEPACrawler
from src.crawler.es_pcsp_crawler import SpainPCSPCrawler
from src.crawler.nl_tenderned_crawler import NetherlandsTenderNedCrawler
from src.config import settings, crawler_config
from src.utils.logger import get_logger

logger = get_logger(__name__)


class CrawlerManager:
    """í¬ë¡¤ëŸ¬ ê´€ë¦¬ì"""

    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self.crawlers = {
            "G2B": G2BCrawler(),
            "SAM.gov": SAMGovCrawler(),
            "TED": TEDCrawler(),
            "UK_FTS": UKFTSCrawler(),
            "FR_BOAMP": FranceBOAMPCrawler(),
            "DE_VERGABESTELLEN": GermanyVergabestellenCrawler(),
            "IT_MEPA": ItalyMEPACrawler(),
            "ES_PCSP": SpainPCSPCrawler(),
            "NL_TENDERNED": NetherlandsTenderNedCrawler()
        }
        self.is_running = False
        self.last_run_results = {}

    async def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        try:
            if not self.is_running:
                # ê¸°ë³¸ ìŠ¤ì¼€ì¤„ ì„¤ì •
                await self._setup_default_schedules()

                self.scheduler.start()
                self.is_running = True
                logger.info("í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨")

        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹¤íŒ¨: {e}")

    async def stop_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        try:
            if self.is_running:
                self.scheduler.shutdown()
                self.is_running = False
                logger.info("í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ë¨")

        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì‹¤íŒ¨: {e}")

    async def _setup_default_schedules(self):
        """ê¸°ë³¸ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        # G2B - ë§¤ì¼ ì˜¤ì „ 9ì‹œ, ì˜¤í›„ 6ì‹œ
        self.scheduler.add_job(
            self._run_g2b_crawler,
            CronTrigger(hour=9, minute=0),
            id="g2b_morning",
            name="G2B Morning Crawl"
        )

        self.scheduler.add_job(
            self._run_g2b_crawler,
            CronTrigger(hour=18, minute=0),
            id="g2b_evening",
            name="G2B Evening Crawl"
        )

        # SAM.gov - ë§¤ì¼ ì˜¤ì „ 10ì‹œ, ì˜¤í›„ 7ì‹œ (ì‹œì°¨ ê³ ë ¤)
        self.scheduler.add_job(
            self._run_samgov_crawler,
            CronTrigger(hour=10, minute=0),
            id="samgov_morning",
            name="SAM.gov Morning Crawl"
        )

        self.scheduler.add_job(
            self._run_samgov_crawler,
            CronTrigger(hour=19, minute=0),
            id="samgov_evening",
            name="SAM.gov Evening Crawl"
        )

        # TED - ë§¤ì¼ ì˜¤ì „ 11ì‹œ, ì˜¤í›„ 8ì‹œ (ìœ ëŸ½ ì‹œê°„ ê³ ë ¤)
        self.scheduler.add_job(
            self._run_ted_crawler,
            CronTrigger(hour=11, minute=0),
            id="ted_morning",
            name="TED Morning Crawl"
        )

        self.scheduler.add_job(
            self._run_ted_crawler,
            CronTrigger(hour=20, minute=0),
            id="ted_evening",
            name="TED Evening Crawl"
        )

        # UK FTS - ë§¤ì¼ ì˜¤ì „ 12ì‹œ, ì˜¤í›„ 9ì‹œ (ì˜êµ­ ì‹œê°„ ê³ ë ¤)
        self.scheduler.add_job(
            self._run_uk_fts_crawler,
            CronTrigger(hour=12, minute=0),
            id="uk_fts_morning",
            name="UK FTS Morning Crawl"
        )

        self.scheduler.add_job(
            self._run_uk_fts_crawler,
            CronTrigger(hour=21, minute=0),
            id="uk_fts_evening",
            name="UK FTS Evening Crawl"
        )

        # ìœ ëŸ½ í¬ë¡¤ëŸ¬ë“¤ - ë§¤ì¼ ì˜¤ì „ 1ì‹œë¶€í„° 5ë¶„ ê°„ê²©ìœ¼ë¡œ
        european_crawlers = ["FR_BOAMP", "DE_VERGABESTELLEN", "IT_MEPA", "ES_PCSP", "NL_TENDERNED"]
        for i, crawler_name in enumerate(european_crawlers):
            self.scheduler.add_job(
                lambda name=crawler_name: self._run_european_crawler(name),
                CronTrigger(hour=1, minute=i*5),
                id=f"{crawler_name.lower()}_daily",
                name=f"{crawler_name} Daily Crawl"
            )

        logger.info("ê¸°ë³¸ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ")

    async def _run_g2b_crawler(self):
        """G2B í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            logger.info("ì˜ˆì•½ëœ G2B í¬ë¡¤ë§ ì‹œì‘")
            result = await self.run_crawler("G2B")
            self.last_run_results["G2B"] = {
                **result,
                "scheduled_run": True,
                "run_time": datetime.now().isoformat()
            }
            logger.info(f"G2B í¬ë¡¤ë§ ì™„ë£Œ: {result}")

        except Exception as e:
            logger.error(f"ì˜ˆì•½ëœ G2B í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    async def _run_samgov_crawler(self):
        """SAM.gov í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            logger.info("ì˜ˆì•½ëœ SAM.gov í¬ë¡¤ë§ ì‹œì‘")
            result = await self.run_crawler("SAM.gov")
            self.last_run_results["SAM.gov"] = {
                **result,
                "scheduled_run": True,
                "run_time": datetime.now().isoformat()
            }
            logger.info(f"SAM.gov í¬ë¡¤ë§ ì™„ë£Œ: {result}")

        except Exception as e:
            logger.error(f"ì˜ˆì•½ëœ SAM.gov í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    async def _run_ted_crawler(self):
        """TED í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            logger.info("ì˜ˆì•½ëœ TED í¬ë¡¤ë§ ì‹œì‘")
            result = await self.run_crawler("TED")
            self.last_run_results["TED"] = {
                **result,
                "scheduled_run": True,
                "run_time": datetime.now().isoformat()
            }
            logger.info(f"TED í¬ë¡¤ë§ ì™„ë£Œ: {result}")

        except Exception as e:
            logger.error(f"ì˜ˆì•½ëœ TED í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    async def _run_uk_fts_crawler(self):
        """UK FTS í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            logger.info("ì˜ˆì•½ëœ UK FTS í¬ë¡¤ë§ ì‹œì‘")
            result = await self.run_crawler("UK_FTS")
            self.last_run_results["UK_FTS"] = {
                **result,
                "scheduled_run": True,
                "run_time": datetime.now().isoformat()
            }
            logger.info(f"UK FTS í¬ë¡¤ë§ ì™„ë£Œ: {result}")

        except Exception as e:
            logger.error(f"ì˜ˆì•½ëœ UK FTS í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    async def _run_european_crawler(self, crawler_name: str):
        """ìœ ëŸ½ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            logger.info(f"ì˜ˆì•½ëœ {crawler_name} í¬ë¡¤ë§ ì‹œì‘")
            result = await self.run_crawler(crawler_name)
            self.last_run_results[crawler_name] = {
                **result,
                "scheduled_run": True,
                "run_time": datetime.now().isoformat()
            }
            logger.info(f"{crawler_name} í¬ë¡¤ë§ ì™„ë£Œ: {result}")

        except Exception as e:
            logger.error(f"ì˜ˆì•½ëœ {crawler_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    async def run_crawler(self, site_name: str, keywords: Optional[List[str]] = None) -> Dict[str, Any]:
        """íŠ¹ì • í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        if site_name not in self.crawlers:
            return {
                "success": False,
                "error": f"ì•Œ ìˆ˜ ì—†ëŠ” ì‚¬ì´íŠ¸: {site_name}",
                "site": site_name,
                "total_found": 0
            }

        crawler = self.crawlers[site_name]

        try:
            # ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©
            if not keywords:
                keywords = (
                    crawler_config.SEEGENE_KEYWORDS['korean']
                    if site_name == "G2B"
                    else crawler_config.SEEGENE_KEYWORDS['english']
                )

            logger.info(f"ğŸš€ {site_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘ - í‚¤ì›Œë“œ: {keywords}")

            # í¬ë¡¤ëŸ¬ ì‹¤í–‰ (í¬ë¡¤ëŸ¬ë³„ ë©”ì„œë“œ êµ¬ë¶„)
            if site_name in ["FR_BOAMP", "DE_VERGABESTELLEN", "IT_MEPA", "ES_PCSP", "NL_TENDERNED"]:
                logger.info(f"ğŸ“¡ {site_name} crawl() ë©”ì„œë“œ í˜¸ì¶œ")
                result = await crawler.crawl(keywords)
                # ìƒˆ í¬ë¡¤ëŸ¬ì˜ ê²°ê³¼ í•„ë“œëª…ì„ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                if "total_collected" in result:
                    result["total_found"] = result["total_collected"]
                if "source" in result:
                    result["site"] = result["source"]
                logger.info(f"âœ… {site_name} crawl() ì™„ë£Œ: {result.get('total_found', 0)}ê±´")
            elif site_name == "G2B":
                # G2B í¬ë¡¤ëŸ¬ëŠ” search_bids ë©”ì„œë“œ ì‚¬ìš©
                logger.info(f"ğŸ“¡ {site_name} search_bids() ë©”ì„œë“œ í˜¸ì¶œ")
                bids = await crawler.search_bids(keywords)
                logger.info(f"ğŸ“‹ {site_name} search_bids() ë°˜í™˜: {len(bids)}ê±´")
                result = {
                    "success": True,
                    "site": site_name,
                    "total_found": len(bids),
                    "results": bids
                }
                logger.info(f"âœ… {site_name} search_bids() ì™„ë£Œ: {len(bids)}ê±´")
            else:
                logger.info(f"ğŸ“¡ {site_name} run_crawler() ë©”ì„œë“œ í˜¸ì¶œ")
                result = await crawler.run_crawler(keywords)
                logger.info(f"âœ… {site_name} run_crawler() ì™„ë£Œ: {result.get('total_found', 0)}ê±´")

            # ê²°ê³¼ ê¸°ë¡
            self.last_run_results[site_name] = {
                **result,
                "manual_run": True,
                "run_time": datetime.now().isoformat()
            }

            logger.info(f"ğŸ¯ {site_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ - ìµœì¢… ê²°ê³¼: {result}")
            return result

        except Exception as e:
            import traceback
            logger.error(f"âŒ {site_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            logger.error(f"ğŸ“Š ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return {
                "success": False,
                "site": site_name,
                "error": str(e),
                "total_found": 0
            }

    async def run_all_crawlers(self, keywords: Optional[List[str]] = None) -> Dict[str, Any]:
        """ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        results = {}

        for site_name in self.crawlers.keys():
            logger.info(f"{site_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘...")
            result = await self.run_crawler(site_name, keywords)
            results[site_name] = result

            # í¬ë¡¤ëŸ¬ ê°„ ê°„ê²©
            await asyncio.sleep(5)

        total_found = sum(r.get('total_found', 0) for r in results.values())
        success_count = sum(1 for r in results.values() if r.get('success', False))

        return {
            "success": success_count > 0,
            "total_crawlers": len(self.crawlers),
            "successful_crawlers": success_count,
            "total_found": total_found,
            "results": results,
            "run_time": datetime.now().isoformat()
        }

    def get_crawler_status(self) -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        status = {
            "scheduler_running": self.is_running,
            "crawlers": {}
        }

        for site_name, crawler in self.crawlers.items():
            # ë¡œê·¸ì¸ ì •ë³´ í™•ì¸
            has_credentials = False
            if site_name == "G2B":
                has_credentials = bool(settings.G2B_USERNAME and settings.G2B_PASSWORD)
            elif site_name == "SAM.gov":
                has_credentials = bool(settings.SAMGOV_USERNAME and settings.SAMGOV_PASSWORD)

            # ë§ˆì§€ë§‰ ì‹¤í–‰ ê²°ê³¼
            last_result = self.last_run_results.get(site_name, {})

            status["crawlers"][site_name] = {
                "has_credentials": has_credentials,
                "can_make_requests": True,  # WebDriver ê¸°ë°˜ì´ë¯€ë¡œ í•­ìƒ ê°€ëŠ¥
                "status": "configured" if has_credentials else "partial",
                "last_run": last_result.get("run_time"),
                "last_success": last_result.get("success", False),
                "last_found": last_result.get("total_found", 0)
            }

        return status

    def get_scheduled_jobs(self) -> List[Dict[str, Any]]:
        """ì˜ˆì•½ëœ ì‘ì—… ëª©ë¡"""
        jobs = []

        for job in self.scheduler.get_jobs():
            next_run = job.next_run_time
            jobs.append({
                "id": job.id,
                "name": job.name,
                "next_run": next_run.isoformat() if next_run else None,
                "trigger": str(job.trigger)
            })

        return jobs

    async def add_custom_schedule(self, site_name: str, cron_expression: str, job_id: str = None) -> bool:
        """ì‚¬ìš©ì ì •ì˜ ìŠ¤ì¼€ì¤„ ì¶”ê°€"""
        try:
            if site_name not in self.crawlers:
                return False

            if not job_id:
                job_id = f"{site_name}_custom_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            # í¬ë¡  í‘œí˜„ì‹ íŒŒì‹±
            parts = cron_expression.split()
            if len(parts) != 5:
                return False

            minute, hour, day, month, day_of_week = parts

            # ì‹¤í–‰ í•¨ìˆ˜ ì„ íƒ
            func = self._run_g2b_crawler if site_name == "G2B" else self._run_samgov_crawler

            # ì‘ì—… ì¶”ê°€
            self.scheduler.add_job(
                func,
                CronTrigger(
                    minute=minute,
                    hour=hour,
                    day=day,
                    month=month,
                    day_of_week=day_of_week
                ),
                id=job_id,
                name=f"{site_name} Custom Schedule"
            )

            logger.info(f"ì‚¬ìš©ì ì •ì˜ ìŠ¤ì¼€ì¤„ ì¶”ê°€: {job_id}")
            return True

        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì •ì˜ ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def remove_scheduled_job(self, job_id: str) -> bool:
        """ì˜ˆì•½ëœ ì‘ì—… ì œê±°"""
        try:
            self.scheduler.remove_job(job_id)
            logger.info(f"ì˜ˆì•½ëœ ì‘ì—… ì œê±°: {job_id}")
            return True
        except Exception as e:
            logger.error(f"ì‘ì—… ì œê±° ì‹¤íŒ¨: {e}")
            return False


# ì „ì—­ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
crawler_manager = CrawlerManager()