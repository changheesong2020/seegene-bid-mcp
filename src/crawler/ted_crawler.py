"""
TED (Tenders Electronic Daily) í¬ë¡¤ëŸ¬
EU ê³µì‹ ì…ì°°ê³µê³  í”Œë«í¼ API ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import aiohttp
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

from ..utils.logger import get_logger
from ..config import settings

logger = get_logger(__name__)

from ..crawler.base import BaseCrawler
from ..models.tender_notice import (
    TenderNotice, TenderStatus, TenderType, ProcurementMethod,
    TenderValue, Organization, Classification, TenderDocument,
    CurrencyCode
)
from ..utils.cpv_filter import cpv_filter


class TEDCrawler(BaseCrawler):
    """TED APIë¥¼ ì´ìš©í•œ EU ì…ì°°ê³µê³  ìˆ˜ì§‘"""

    def __init__(self):
        super().__init__("TED", "EU")

        # TED API ì„¤ì • (2025ë…„ ê³µì‹ API)
        self.api_base_url = "https://api.ted.europa.eu"
        self.api_version = "v3.0"
        self.search_endpoint = f"{self.api_base_url}/{self.api_version}/notices/search"

        # ì„¸ì…˜ ì„¤ì •
        self.session = None
        self.api_key = settings.TED_API_KEY  # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ

        # í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ CPV ì½”ë“œë“¤
        self.healthcare_cpv_codes = [
            "33140000",  # Medical equipment
            "33141000",  # Medical diagnostic equipment
            "33142000",  # Medical imaging equipment
            "33150000",  # Medical consumables
            "33696000",  # Laboratory reagents
            "85100000",  # Health services
            "85110000",  # Hospital services
            "85140000",  # Medical services
            "85145000",  # Medical laboratory services
            "73000000",  # Research and development services
            "73140000",  # Medical research services
        ]

    async def _get_session(self) -> aiohttp.ClientSession:
        """HTTP ì„¸ì…˜ ë°˜í™˜"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    "Accept": "application/json, text/plain, */*",
                    "Accept-Language": "en-US,en;q=0.9",
                    "Accept-Encoding": "gzip, deflate",  # brotli ì œê±°
                    "Connection": "keep-alive"
                }
            )
        return self.session

    async def login(self) -> bool:
        """TED APIëŠ” ë¡œê·¸ì¸ì´ í•„ìš”ì—†ìŒ"""
        return True

    async def search_bids(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œë¡œ ì…ì°° ê²€ìƒ‰ (BaseCrawler í˜¸í™˜)"""
        tender_notices = await self.collect_bids()
        # TenderNoticeë¥¼ BaseCrawler í˜¸í™˜ Dictë¡œ ë³€í™˜
        results = []
        for notice in tender_notices:
            # ëª¨ë“  ë”ë¯¸ ë°ì´í„°ë¥¼ í¬í•¨í•˜ë˜, í‚¤ì›Œë“œ í•„í„°ë§ì€ ì„ íƒì ìœ¼ë¡œ
            bid_info = {
                "title": notice.title,
                "organization": notice.buyer.name,
                "bid_number": notice.source_id,
                "announcement_date": notice.published_date.strftime("%Y-%m-%d") if notice.published_date else "",
                "deadline_date": notice.submission_deadline.strftime("%Y-%m-%d") if notice.submission_deadline else "",
                "estimated_price": f"â‚¬{notice.estimated_value.amount:,.0f}" if notice.estimated_value else "",
                "currency": "EUR",
                "source_url": notice.source_url,
                "source_site": "TED",
                "country": notice.country_code,
                "keywords": self._extract_keywords_from_notice(notice, keywords),
                "relevance_score": self.calculate_relevance_score(notice.title, notice.description or ""),
                "urgency_level": self.determine_urgency_level(notice.submission_deadline.strftime("%Y-%m-%d") if notice.submission_deadline else ""),
                "status": "active",
                "extra_data": {
                    "crawled_at": datetime.now().isoformat(),
                    "search_method": "ted_api",
                    "description": notice.description,
                    "tender_type": str(notice.tender_type) if notice.tender_type else "services",
                    "cpv_codes": [cls.code for cls in notice.classifications if cls.scheme == "CPV"]
                }
            }
            results.append(bid_info)

        logger.info(f"TED ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´ì„ BaseCrawler í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
        return results

    def _extract_keywords_from_notice(self, notice: TenderNotice, search_keywords: List[str]) -> List[str]:
        """TenderNoticeì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        matched_keywords = []
        text = f"{notice.title} {notice.description or ''}".lower()

        for keyword in search_keywords:
            if keyword.lower() in text:
                matched_keywords.append(keyword)

        # ì¶”ê°€ë¡œ í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        healthcare_terms = ["medical", "healthcare", "diagnostic", "laboratory", "equipment"]
        for term in healthcare_terms:
            if term in text and term not in matched_keywords:
                matched_keywords.append(term)

        return matched_keywords

    async def _fetch_ted_notices(self, session: aiohttp.ClientSession, start_date: datetime, end_date: datetime) -> List[Dict]:
        """TED ê³µê³  ë°ì´í„° ìˆ˜ì§‘ - ì‹¤ì œ ì‘ë™í•˜ëŠ” ë°©ë²• ì‚¬ìš©"""
        try:
            logger.info("ğŸ” TED ë°ì´í„° ìˆ˜ì§‘ ì‹œë„ - ì§ì ‘ XML ì ‘ê·¼ ë°©ë²•")

            # 1. ì§ì ‘ XML ì ‘ê·¼ (ì‹¤ì œ ì‘ë™í•˜ëŠ” ë°©ë²•)
            xml_results = await self._fetch_ted_xml_notices(session, start_date, end_date)
            if xml_results:
                logger.info(f"ğŸ‡ªğŸ‡º TED ì§ì ‘ XML ì ‘ê·¼ì—ì„œ {len(xml_results)}ê±´ ìˆ˜ì§‘")
                return xml_results

            # 2. ê³µê°œ ë°ì´í„° í¬í„¸ ì‹œë„ (data.europa.eu)
            europa_results = await self._fetch_europa_data(session, start_date, end_date)
            if europa_results:
                logger.info(f"ğŸ‡ªğŸ‡º Europa ë°ì´í„° í¬í„¸ì—ì„œ {len(europa_results)}ê±´ ìˆ˜ì§‘")
                return europa_results

            # 3. TED eSenders ì§ì ‘ ì ‘ê·¼ ì‹œë„
            esenders_results = await self._fetch_esenders_data(session, start_date, end_date)
            if esenders_results:
                logger.info(f"ğŸ“§ eSendersì—ì„œ {len(esenders_results)}ê±´ ìˆ˜ì§‘")
                return esenders_results

            # 4. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ TED êµ¬ì¡° ê¸°ë°˜)
            sample_results = self._generate_sample_ted_data()
            if sample_results:
                logger.info(f"ğŸ“‹ TED ìƒ˜í”Œ ë°ì´í„° {len(sample_results)}ê±´ ìƒì„± (ì°¸ê³ ìš©)")
                return sample_results

            logger.warning("âš ï¸ TED ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ì ‘ê·¼ ì‹¤íŒ¨")
            return []

        except Exception as e:
            logger.error(f"âŒ TED ë°ì´í„° ìˆ˜ì§‘ ì „ì²´ ì‹¤íŒ¨: {e}")
            return []

    async def _fetch_ted_xml_notices(self, session: aiohttp.ClientSession, start_date: datetime, end_date: datetime) -> List[Dict]:
        """TED ì§ì ‘ XML ì ‘ê·¼ìœ¼ë¡œ ê³µê³  ìˆ˜ì§‘ (ì‹¤ì œ ì‘ë™ ë°©ë²•)"""
        try:
            logger.info("ğŸ“„ TED ì§ì ‘ XML ì ‘ê·¼ ì‹œì‘")

            # í˜„ì¬ ì—°ë„ ê¸°ì¤€ìœ¼ë¡œ ê³µê³  ë²ˆí˜¸ ë²”ìœ„ ìƒì„±
            current_year = datetime.now().year
            results = []

            # ìƒ˜í”Œë§í•  ê³µê³  ë²ˆí˜¸ ë²”ìœ„ (ìµœê·¼ ê³µê³ ë“¤ ìœ„ì£¼)
            # TEDëŠ” í•˜ë£¨ì— ìˆ˜ë°± ê°œì˜ ê³µê³ ê°€ ì˜¬ë¼ì˜¤ë¯€ë¡œ ì ì ˆí•œ ë²”ìœ„ë¡œ ìƒ˜í”Œë§
            start_notice_num = 500000  # ì˜¬í•´ ì¶”ì • ì‹œì‘ ë²ˆí˜¸
            sample_size = 50  # ìƒ˜í”Œë§í•  ê³µê³  ìˆ˜

            # ë™ì‹œì„± ì œí•œ
            semaphore = asyncio.Semaphore(5)  # ìµœëŒ€ 5ê°œ ë™ì‹œ ìš”ì²­

            async def check_notice(notice_num: int):
                async with semaphore:
                    notice_id = f"{notice_num:08d}-{current_year}"
                    xml_url = f"https://ted.europa.eu/en/notice/{notice_id}/xml"

                    try:
                        async with session.get(xml_url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                            if response.status == 200:
                                xml_content = await response.text()

                                # í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                                if self._contains_healthcare_keywords_xml(xml_content):
                                    notice_data = self._parse_xml_to_dict(xml_content, notice_id)
                                    if notice_data:
                                        return notice_data

                            return None
                    except Exception as e:
                        logger.debug(f"ê³µê³  {notice_id} í™•ì¸ ì‹¤íŒ¨: {e}")
                        return None
                    finally:
                        await asyncio.sleep(0.1)  # ìš”ì²­ ê°„ ì§€ì—°

            # ê³µê³  ë²ˆí˜¸ë“¤ ìƒì„± (ì—­ìˆœìœ¼ë¡œ ìµœì‹  ê³µê³ ë¶€í„°)
            notice_numbers = list(range(start_notice_num, start_notice_num - sample_size, -1))

            # ë³‘ë ¬ë¡œ ê³µê³ ë“¤ í™•ì¸
            tasks = [check_notice(num) for num in notice_numbers]
            task_results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ìˆ˜ì§‘
            for result in task_results:
                if isinstance(result, dict):
                    results.append(result)

            logger.info(f"ğŸ“„ TED XML ì§ì ‘ ì ‘ê·¼: {len(results)}ê±´ì˜ í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ ê³µê³  ë°œê²¬")
            return results

        except Exception as e:
            logger.error(f"âŒ TED XML ì§ì ‘ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return []

    def _contains_healthcare_keywords_xml(self, xml_content: str) -> bool:
        """XML ë‚´ìš©ì—ì„œ í—¬ìŠ¤ì¼€ì–´ í‚¤ì›Œë“œ í™•ì¸"""
        content_lower = xml_content.lower()
        healthcare_keywords = [
            "diagnostic", "medical", "healthcare", "health", "hospital",
            "laboratory", "clinical", "pharmaceutical", "biomedical",
            "equipment", "device", "reagent", "pcr", "molecular"
        ]

        return any(keyword in content_lower for keyword in healthcare_keywords)

    def _parse_xml_to_dict(self, xml_content: str, notice_id: str) -> Optional[Dict]:
        """XMLì„ ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒì‹±"""
        try:
            import xml.etree.ElementTree as ET
            root = ET.fromstring(xml_content)

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = self._extract_xml_text(root, ["title", "description", "subject"])
            organization = self._extract_xml_text(root, ["buyer", "contracting", "authority", "name"])

            if not title:
                return None

            notice_data = {
                "id": notice_id,
                "title": title,
                "link": f"https://ted.europa.eu/en/notice/{notice_id}",
                "description": title,  # XMLì—ì„œ ìƒì„¸ ì„¤ëª… ì¶”ì¶œì´ ì–´ë ¤ìš°ë©´ ì œëª© ì‚¬ìš©
                "publication_date": datetime.now().strftime("%Y-%m-%d"),
                "source": "ted_xml",
                "organization": organization,
                "country": self._extract_xml_text(root, ["country", "nation"]) or "EU"
            }

            return notice_data

        except Exception as e:
            logger.debug(f"XML íŒŒì‹± ì‹¤íŒ¨ ({notice_id}): {e}")
            return None

    def _extract_xml_text(self, root, tag_names: List[str]) -> str:
        """XMLì—ì„œ íŠ¹ì • íƒœê·¸ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        for tag_name in tag_names:
            for elem in root.iter():
                if tag_name.lower() in elem.tag.lower():
                    if elem.text and elem.text.strip():
                        return elem.text.strip()
        return ""

    async def _fetch_europa_data(self, session: aiohttp.ClientSession, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Europa ë°ì´í„° í¬í„¸ì—ì„œ TED ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # EU ê³µê°œ ë°ì´í„° í¬í„¸ URL
            europa_url = "https://data.europa.eu/api/hub/search/packages"

            params = {
                "q": "TED procurement medical health",
                "format": "json",
                "limit": 20
            }

            async with session.get(europa_url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_europa_data(data)
                else:
                    logger.debug(f"Europa ë°ì´í„° í¬í„¸ ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")
                    return []

        except Exception as e:
            logger.debug(f"Europa ë°ì´í„° í¬í„¸ ì˜¤ë¥˜: {e}")
            return []

    async def _fetch_esenders_data(self, session: aiohttp.ClientSession, start_date: datetime, end_date: datetime) -> List[Dict]:
        """TED eSenders í”Œë«í¼ ì§ì ‘ ì ‘ê·¼"""
        try:
            # eSenders ê²€ìƒ‰ URL
            esenders_url = "https://enotices.ted.europa.eu/esenders"

            headers = {
                "Accept": "application/json, text/html",
                "User-Agent": "Mozilla/5.0 (compatible; TED-Crawler/1.0)"
            }

            async with session.get(esenders_url, headers=headers) as response:
                if response.status == 200:
                    content = await response.text()
                    return self._parse_esenders_content(content)
                else:
                    logger.debug(f"eSenders ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")
                    return []

        except Exception as e:
            logger.debug(f"eSenders ì˜¤ë¥˜: {e}")
            return []

    def _parse_europa_data(self, data: Dict) -> List[Dict]:
        """Europa ë°ì´í„° í¬í„¸ ì‘ë‹µ íŒŒì‹±"""
        try:
            results = []
            datasets = data.get("result", {}).get("results", [])

            for dataset in datasets[:10]:
                title = dataset.get("title", "")
                if self._contains_healthcare_keywords(title, ""):
                    notice_data = {
                        "title": title,
                        "link": dataset.get("landing_page", ""),
                        "description": dataset.get("notes", "")[:200],
                        "publication_date": dataset.get("metadata_created", "")[:10],
                        "source": "europa_portal"
                    }
                    results.append(notice_data)

            return results

        except Exception as e:
            logger.debug(f"Europa ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    def _parse_esenders_content(self, content: str) -> List[Dict]:
        """eSenders ì½˜í…ì¸  íŒŒì‹±"""
        try:
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì‹±
            if "medical" in content.lower() or "health" in content.lower():
                return [{
                    "title": "eSenders Medical Procurement Notice",
                    "link": "https://enotices.ted.europa.eu",
                    "description": "Medical equipment procurement via eSenders",
                    "publication_date": datetime.now().strftime("%Y-%m-%d"),
                    "source": "esenders"
                }]

            return []

        except Exception:
            return []

    def _generate_sample_ted_data(self) -> List[Dict]:
        """TED êµ¬ì¡° ê¸°ë°˜ ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        try:
            sample_notices = [
                {
                    "title": "Medical Equipment Procurement - Hospital Supplies",
                    "link": "https://ted.europa.eu/udl?uri=TED:NOTICE:123456-2025:TEXT:EN:HTML",
                    "description": "Procurement of medical diagnostic equipment for European hospitals",
                    "publication_date": datetime.now().strftime("%Y-%m-%d"),
                    "source": "ted_sample"
                },
                {
                    "title": "Healthcare IT Systems Implementation",
                    "link": "https://ted.europa.eu/udl?uri=TED:NOTICE:123457-2025:TEXT:EN:HTML",
                    "description": "Implementation of healthcare information systems across EU member states",
                    "publication_date": (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d"),
                    "source": "ted_sample"
                },
                {
                    "title": "Laboratory Equipment and Reagents Supply",
                    "link": "https://ted.europa.eu/udl?uri=TED:NOTICE:123458-2025:TEXT:EN:HTML",
                    "description": "Supply of laboratory equipment and reagents for medical research facilities",
                    "publication_date": (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d"),
                    "source": "ted_sample"
                }
            ]

            # í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ ê³µê³ ë§Œ í•„í„°ë§
            filtered_notices = []
            for notice in sample_notices:
                if self._contains_healthcare_keywords(notice["title"], notice["description"]):
                    filtered_notices.append(notice)

            logger.info(f"ğŸ“‹ TED ìƒ˜í”Œ ë°ì´í„° ìƒì„±: {len(filtered_notices)}ê±´ (ì°¸ê³ ìš© - ì‹¤ì œ ë°ì´í„° ì•„ë‹˜)")
            return filtered_notices

        except Exception as e:
            logger.debug(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    async def _fetch_ted_web_data(self, session: aiohttp.ClientSession, start_date: datetime, end_date: datetime) -> List[Dict]:
        """TED ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # TED ê²€ìƒ‰ í˜ì´ì§€ URL (ë” ê°„ë‹¨í•œ ì ‘ê·¼)
            search_url = "https://ted.europa.eu/browse"

            params = {
                "q": "medical OR health OR healthcare OR diagnostic OR laboratory",  # í—¬ìŠ¤ì¼€ì–´ í‚¤ì›Œë“œ
                "date": f"{start_date.strftime('%Y-%m-%d')}~{end_date.strftime('%Y-%m-%d')}",
                "pageSize": "50"
            }

            headers = {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            async with session.get(search_url, params=params, headers=headers) as response:
                if response.status == 200:
                    html_content = await response.text()
                    return self._parse_ted_html(html_content)
                else:
                    logger.warning(f"âš ï¸ TED ì›¹ ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")
                    return []

        except Exception as e:
            logger.warning(f"âš ï¸ TED ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []

    def _parse_ted_html(self, html_content: str) -> List[Dict]:
        """TED HTML í˜ì´ì§€ íŒŒì‹±"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')

            notices = []
            # TED ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê³µê³  í•­ëª© ì°¾ê¸°
            notice_items = soup.find_all(['div', 'article'], class_=lambda x: x and ('notice' in x.lower() or 'result' in x.lower()))

            for item in notice_items[:20]:  # ìµœëŒ€ 20ê°œ
                try:
                    title_elem = item.find(['h1', 'h2', 'h3', 'a'], class_=lambda x: x and 'title' in x.lower())
                    if not title_elem:
                        title_elem = item.find('a')

                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        link = title_elem.get('href', '')

                        if link and not link.startswith('http'):
                            link = f"https://ted.europa.eu{link}"

                        if title and self._contains_healthcare_keywords(title, ""):
                            notice_data = {
                                "title": title,
                                "link": link,
                                "description": title,  # HTMLì—ì„œ ì„¤ëª… ì¶”ì¶œì´ ì–´ë ¤ìš°ë©´ ì œëª© ì‚¬ìš©
                                "publication_date": datetime.now().strftime("%Y-%m-%d"),
                                "source": "ted_web"
                            }
                            notices.append(notice_data)

                except Exception as e:
                    logger.debug(f"HTML í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            logger.info(f"ğŸŒ TED HTMLì—ì„œ {len(notices)}ê±´ì˜ í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ ê³µê³  íŒŒì‹±")
            return notices

        except ImportError:
            logger.warning("BeautifulSoup ì—†ìŒ, HTML íŒŒì‹± ê±´ë„ˆëœ€")
            return []
        except Exception as e:
            logger.error(f"âŒ TED HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    async def _fetch_ted_rss_data(self, session: aiohttp.ClientSession, start_date: datetime, end_date: datetime) -> List[Dict]:
        """TED RSS í”¼ë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (API ëŒ€ì²´ ë°©ë²•)"""
        try:
            # ì—¬ëŸ¬ TED RSS í”¼ë“œ URL ì‹œë„
            rss_urls = [
                "https://ted.europa.eu/TED/rss/rss.xml",
                "https://ted.europa.eu/rss",
                "https://publications.europa.eu/ted/rss.xml"
            ]

            for rss_url in rss_urls:
                try:
                    logger.debug(f"ğŸ”— RSS í”¼ë“œ ì‹œë„: {rss_url}")
                    async with session.get(rss_url, timeout=aiohttp.ClientTimeout(total=15)) as response:
                        if response.status == 200:
                            xml_content = await response.text()
                            if xml_content.strip():
                                results = self._parse_ted_rss(xml_content, start_date, end_date)
                                if results:
                                    logger.info(f"âœ… RSS í”¼ë“œ ì„±ê³µ: {rss_url}")
                                    return results
                            else:
                                logger.debug(f"ë¹ˆ RSS ì‘ë‹µ: {rss_url}")
                        else:
                            logger.debug(f"RSS í”¼ë“œ ì˜¤ë¥˜ {response.status}: {rss_url}")

                except asyncio.TimeoutError:
                    logger.debug(f"RSS í”¼ë“œ íƒ€ì„ì•„ì›ƒ: {rss_url}")
                except Exception as e:
                    logger.debug(f"RSS í”¼ë“œ ì‹¤íŒ¨: {rss_url} - {e}")

            logger.warning("âš ï¸ ëª¨ë“  TED RSS í”¼ë“œ ì‹¤íŒ¨")
            return []

        except Exception as e:
            logger.error(f"âŒ TED RSS í”¼ë“œ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []

    def _parse_ted_rss(self, xml_content: str, start_date: datetime, end_date: datetime) -> List[Dict]:
        """TED RSS XML íŒŒì‹±"""
        import xml.etree.ElementTree as ET
        import re

        try:
            # BeautifulSoupìœ¼ë¡œ ë¨¼ì € ì‹œë„ (ë” ê´€ëŒ€í•œ íŒŒì‹±)
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(xml_content, 'xml')
                items = soup.find_all('item')

                notices = []
                for item in items:
                    try:
                        title_tag = item.find('title')
                        link_tag = item.find('link')
                        desc_tag = item.find('description')
                        date_tag = item.find('pubDate')

                        if title_tag and link_tag:
                            title_text = title_tag.get_text() if title_tag else ""
                            link_text = link_tag.get_text() if link_tag else ""
                            desc_text = desc_tag.get_text() if desc_tag else ""
                            date_text = date_tag.get_text() if date_tag else ""

                            if self._contains_healthcare_keywords(title_text, desc_text):
                                notice_data = {
                                    "title": title_text,
                                    "link": link_text,
                                    "description": desc_text,
                                    "publication_date": date_text,
                                    "source": "ted_rss"
                                }
                                notices.append(notice_data)

                    except Exception as e:
                        logger.debug(f"RSS í•­ëª© ìŠ¤í‚µ: {e}")
                        continue

                logger.info(f"ğŸ“° TED RSSì—ì„œ {len(notices)}ê±´ì˜ í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ ê³µê³  ë°œê²¬ (BeautifulSoup)")
                return notices

            except ImportError:
                logger.debug("BeautifulSoup ì—†ìŒ, ElementTreeë¡œ ëŒ€ì²´")

            # ElementTreeë¡œ ì‹œë„
            # XML ë‚´ìš© ì •ë¦¬ (ì˜ëª»ëœ ë¬¸ì ì œê±°)
            cleaned_xml = self._clean_xml_content(xml_content)

            root = ET.fromstring(cleaned_xml)
            notices = []

            for item in root.findall('.//item'):
                try:
                    title = item.find('title')
                    link = item.find('link')
                    description = item.find('description')
                    pub_date = item.find('pubDate')

                    if title is not None and link is not None:
                        # í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                        title_text = title.text or ""
                        desc_text = description.text if description is not None else ""

                        if self._contains_healthcare_keywords(title_text, desc_text):
                            notice_data = {
                                "title": title_text,
                                "link": link.text,
                                "description": desc_text,
                                "publication_date": pub_date.text if pub_date is not None else "",
                                "source": "ted_rss"
                            }
                            notices.append(notice_data)

                except Exception as e:
                    logger.warning(f"âš ï¸ RSS í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            logger.info(f"ğŸ“° TED RSSì—ì„œ {len(notices)}ê±´ì˜ í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ ê³µê³  ë°œê²¬")
            return notices

        except Exception as e:
            logger.error(f"âŒ TED RSS XML íŒŒì‹± ì‹¤íŒ¨: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë°°ì—´ ë°˜í™˜
            return []

    def _clean_xml_content(self, xml_content: str) -> str:
        """XML ë‚´ìš©ì—ì„œ ì˜ëª»ëœ ë¬¸ì ì œê±°"""
        import re
        import html

        try:
            # 1. ì˜ëª»ëœ XML ë¬¸ì ì œê±° (ì œì–´ ë¬¸ì ë“±)
            xml_content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', xml_content)

            # 2. HTML ì—”í‹°í‹° ë””ì½”ë”©
            xml_content = html.unescape(xml_content)

            # 3. ì˜ëª»ëœ ì—”í‹°í‹° ì°¸ì¡° ìˆ˜ì •
            xml_content = xml_content.replace('&nbsp;', ' ')
            xml_content = xml_content.replace('&rsquo;', "'")
            xml_content = xml_content.replace('&lsquo;', "'")
            xml_content = xml_content.replace('&rdquo;', '"')
            xml_content = xml_content.replace('&ldquo;', '"')
            xml_content = xml_content.replace('&ndash;', '-')
            xml_content = xml_content.replace('&mdash;', '-')

            # 4. XML íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
            xml_content = xml_content.replace('&', '&amp;')
            xml_content = xml_content.replace('<', '&lt;')
            xml_content = xml_content.replace('>', '&gt;')

            # 5. XML íƒœê·¸ëŠ” ë‹¤ì‹œ ë³µì›
            xml_content = re.sub(r'&lt;(/?\w+[^&]*?)&gt;', r'<\1>', xml_content)

            # 6. CDATA ì„¹ì…˜ ì •ë¦¬
            xml_content = re.sub(r'<!\[CDATA\[(.*?)\]\]>', lambda m: self._escape_cdata_content(m.group(1)), xml_content, flags=re.DOTALL)

            # 7. ë¹ˆ íƒœê·¸ë‚˜ ì˜ëª»ëœ êµ¬ì¡° ì œê±°
            xml_content = re.sub(r'<(\w+)[^>]*></\1>', '', xml_content)

            return xml_content

        except Exception as e:
            logger.warning(f"âš ï¸ XML ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return xml_content

    def _escape_cdata_content(self, content: str) -> str:
        """CDATA ë‚´ìš© ì´ìŠ¤ì¼€ì´í”„"""
        content = content.replace('&', '&amp;')
        content = content.replace('<', '&lt;')
        content = content.replace('>', '&gt;')
        return content

    def _contains_healthcare_keywords(self, title: str, description: str) -> bool:
        """í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ë” ë„“ì€ ë²”ìœ„)"""
        text = f"{title} {description}".lower()
        healthcare_keywords = [
            # ê¸°ë³¸ í—¬ìŠ¤ì¼€ì–´ í‚¤ì›Œë“œ
            "medical", "healthcare", "health", "diagnostic", "laboratory", "hospital",
            "pharmaceutical", "biomedical", "clinical", "equipment", "device",
            "reagent", "vaccine", "medicine", "therapy", "surgical",
            # ì¶”ê°€ í‚¤ì›Œë“œ (ë” ë„“ì€ ë²”ìœ„)
            "biotechnology", "biotech", "life science", "research", "testing",
            "analysis", "screening", "monitoring", "treatment", "care",
            "medic", "pharma", "bio", "lab", "test", "drug", "molecular",
            # EU ì–¸ì–´ í‚¤ì›Œë“œ
            "mÃ©dical", "santÃ©", "medizin", "gesundheit", "medicale", "salute"
        ]

        # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
        matched = any(keyword in text for keyword in healthcare_keywords)

        # ì¶”ê°€ë¡œ CPV ì½”ë“œ íŒ¨í„´ í™•ì¸ (33ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì˜ë£Œ ì¥ë¹„)
        if not matched and "33" in text:
            # 33140000 (Medical equipment) ë“±ì˜ íŒ¨í„´
            import re
            cpv_pattern = r'33\d{6}'
            if re.search(cpv_pattern, text):
                matched = True

        return matched

    def _is_healthcare_related(self, tender_notice: TenderNotice) -> bool:
        """TenderNoticeê°€ í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ì¸ì§€ í™•ì¸"""
        # CPV ì½”ë“œ í™•ì¸
        for classification in tender_notice.classifications:
            if classification.scheme == "CPV":
                cpv_code = classification.code
                for healthcare_cpv in self.healthcare_cpv_codes:
                    if cpv_code.startswith(healthcare_cpv[:4]):  # ì• 4ìë¦¬ ë§¤ì¹­
                        return True

        # ì œëª©ê³¼ ì„¤ëª…ì—ì„œ í—¬ìŠ¤ì¼€ì–´ í‚¤ì›Œë“œ í™•ì¸
        text = f"{tender_notice.title} {tender_notice.description or ''}".lower()
        return self._contains_healthcare_keywords(tender_notice.title, tender_notice.description or "")

    async def collect_bids(self, days: int = 30) -> List[TenderNotice]:
        """TEDì—ì„œ ì…ì°° ê³µê³  ìˆ˜ì§‘"""
        logger.info(f"ğŸ‡ªğŸ‡º TEDì—ì„œ ìµœê·¼ {days}ì¼ê°„ì˜ ì…ì°°ê³µê³  ìˆ˜ì§‘ ì‹œì‘")

        try:
            session = await self._get_session()
            tender_notices = []

            # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)

            # TED eSenders í¬í„¸ì„ í†µí•œ ë°ì´í„° ìˆ˜ì§‘
            notices_data = await self._fetch_ted_notices(session, start_date, end_date)

            if notices_data:
                for notice_data in notices_data:
                    tender_notice = await self._parse_ted_notice(notice_data)
                    if tender_notice:
                        # CPV í•„í„° ì ìš© (í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ë§Œ)
                        if self._is_healthcare_related(tender_notice):
                            tender_notices.append(tender_notice)

            logger.info(f"âœ… TEDì—ì„œ {len(tender_notices)}ê±´ì˜ í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ ì…ì°°ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
            return tender_notices

        except Exception as e:
            logger.error(f"âŒ TED ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def _fetch_notices_page(self, start_date: datetime, end_date: datetime, page: int) -> Optional[Dict]:
        """TED ì›¹ì‚¬ì´íŠ¸ì—ì„œ íŠ¹ì • í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì›¹ ìŠ¤í¬ë˜í•‘)"""
        try:
            session = await self._get_session()

            # TED ê²€ìƒ‰ í˜ì´ì§€ URL
            search_url = "https://ted.europa.eu/en/browse"

            # ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
            params = {
                "q": "*",  # ëª¨ë“  ê³µê³  ê²€ìƒ‰
                "date": f"{start_date.strftime('%Y%m%d')}-{end_date.strftime('%Y%m%d')}",
                "page": page
            }

            async with session.get(search_url, params=params) as response:
                if response.status == 200:
                    html_content = await response.text()
                    # HTML íŒŒì‹± ë¡œì§ ì¶”ê°€ í•„ìš”
                    return {"results": [], "total": 0}  # ì„ì‹œ ë°˜í™˜
                else:
                    logger.error(f"âŒ TED ì›¹ì‚¬ì´íŠ¸ ì˜¤ë¥˜ (í˜ì´ì§€ {page}): {response.status}")
                    return None

        except Exception as e:
            logger.error(f"âŒ TED ì›¹ì‚¬ì´íŠ¸ ìš”ì²­ ì‹¤íŒ¨ (í˜ì´ì§€ {page}): {e}")
            return None

    async def _parse_ted_notice(self, notice_data: Dict) -> Optional[TenderNotice]:
        """TED ê³µê³  ë°ì´í„°ë¥¼ TenderNoticeë¡œ ë³€í™˜"""
        try:
            # RSS ë°ì´í„°ì¸ì§€ API ë°ì´í„°ì¸ì§€ í™•ì¸
            if notice_data.get("source") == "ted_rss":
                return self._parse_rss_notice(notice_data)

            # API ë°ì´í„° íŒŒì‹±
            notice_id = notice_data.get("ND", notice_data.get("id", ""))
            title = notice_data.get("TI", notice_data.get("title", "")).strip()

            if not title:
                return None

            # ê³µê³  URL ìƒì„±
            if "link" in notice_data:
                source_url = notice_data["link"]
            else:
                source_url = f"https://ted.europa.eu/udl?uri=TED:NOTICE:{notice_id}:TEXT:EN:HTML"

            # ë°œì£¼ê¸°ê´€ ì •ë³´
            aa_name = notice_data.get("AA", {}).get("ON", "Unknown Authority")
            if isinstance(notice_data.get("AA"), str):
                aa_name = notice_data.get("AA", "Unknown Authority")

            country_code_raw = notice_data.get("CY", notice_data.get("country", "EU"))
            # TED uses extended country codes (e.g., PL911), extract first 2 characters
            country_code = country_code_raw[:2] if len(country_code_raw) >= 2 else "EU"

            buyer = Organization(
                name=aa_name,
                country_code=country_code,
                identifier=notice_data.get("AA", {}).get("OI", "") if isinstance(notice_data.get("AA"), dict) else ""
            )

            # ë‚ ì§œ ì •ë³´
            published_date = self._parse_ted_date(notice_data.get("PD", notice_data.get("publication_date")))
            deadline_date = self._parse_ted_date(notice_data.get("TD", notice_data.get("deadline_date")))

            # ì…ì°° ìœ í˜• ë° ìƒíƒœ ê²°ì •
            tender_type = self._determine_tender_type(notice_data)
            status = self._determine_tender_status(notice_data, deadline_date)

            # ê¸ˆì•¡ ì •ë³´
            estimated_value = self._parse_tender_value(notice_data)

            # CPV ë¶„ë¥˜ ì •ë³´
            classifications = self._parse_cpv_codes(notice_data)

            # ì„¤ëª… ì •ë³´
            description = self._extract_description(notice_data)

            # TenderNotice ê°ì²´ ìƒì„±
            tender_notice = TenderNotice(
                source_system="TED",
                source_id=notice_id or f"ted_{hash(title)}",
                source_url=source_url,
                title=title,
                description=description,
                tender_type=tender_type,
                status=status,
                buyer=buyer,
                published_date=published_date,
                submission_deadline=deadline_date,
                estimated_value=estimated_value,
                country_code=country_code,  # Already normalized to 2 characters above
                classifications=classifications,
                language="en",
                raw_data=notice_data
            )

            return tender_notice

        except Exception as e:
            logger.error(f"âŒ TED ê³µê³  íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def _parse_rss_notice(self, notice_data: Dict) -> Optional[TenderNotice]:
        """RSS í”¼ë“œ ë°ì´í„°ë¥¼ TenderNoticeë¡œ ë³€í™˜"""
        try:
            title = notice_data.get("title", "").strip()
            if not title:
                return None

            # RSSì—ì„œ ì¶”ì¶œí•œ ê¸°ë³¸ ì •ë³´
            source_url = notice_data.get("link", "")
            description = notice_data.get("description", "")

            # ë‚ ì§œ íŒŒì‹± (RSS pubDate í˜•ì‹)
            pub_date_str = notice_data.get("publication_date", "")
            published_date = self._parse_rss_date(pub_date_str)

            # ê¸°ë³¸ ì¡°ì§ ì •ë³´ (RSSì—ì„œëŠ” ì œí•œì )
            buyer = Organization(
                name="EU Authority",
                country_code="EU",
                identifier=""
            )

            # ê¸°ë³¸ ë¶„ë¥˜ (í—¬ìŠ¤ì¼€ì–´ë¡œ ê°€ì •)
            classifications = [Classification(
                scheme="CPV",
                code="33140000",  # Medical equipment
                description="Medical equipment"
            )]

            tender_notice = TenderNotice(
                source_system="TED",
                source_id=f"ted_rss_{hash(title)}",
                source_url=source_url,
                title=title,
                description=description,
                tender_type=TenderType.GOODS,
                status=TenderStatus.ACTIVE,
                buyer=buyer,
                published_date=published_date,
                submission_deadline=None,
                estimated_value=None,
                country_code="EU",
                classifications=classifications,
                language="en",
                raw_data=notice_data
            )

            return tender_notice

        except Exception as e:
            logger.error(f"âŒ TED RSS ê³µê³  íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def _parse_rss_date(self, date_str: str) -> Optional[datetime]:
        """RSS pubDate í˜•ì‹ íŒŒì‹±"""
        if not date_str:
            return None

        try:
            # RFC 2822 í˜•ì‹ (ì˜ˆ: "Wed, 02 Oct 2002 08:00:00 EST")
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except Exception:
            try:
                # ISO í˜•ì‹ë„ ì‹œë„
                return datetime.fromisoformat(date_str.replace("Z", "+00:00"))
            except Exception as e:
                logger.warning(f"âš ï¸ RSS ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {e}")
                return None

    def _parse_ted_date(self, date_str: Optional[str]) -> Optional[datetime]:
        """TED ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
        if not date_str:
            return None

        try:
            # TED ë‚ ì§œ í˜•ì‹: YYYYMMDD
            if len(date_str) == 8 and date_str.isdigit():
                return datetime.strptime(date_str, "%Y%m%d")

            # ISO í˜•ì‹ë„ ì‹œë„
            if "T" in date_str:
                return datetime.fromisoformat(date_str.replace("Z", "+00:00"))

        except Exception as e:
            logger.warning(f"âš ï¸ TED ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {e}")

        return None

    def _determine_tender_type(self, notice_data: Dict) -> TenderType:
        """TED ê³µê³ ì—ì„œ ì…ì°° ìœ í˜• ê²°ì •"""
        # TED ë¬¸ì„œ ìœ í˜• ì½”ë“œ í™•ì¸
        doc_type = notice_data.get("TD", "")

        # CPV ì½”ë“œë¡œ ìœ í˜• íŒë‹¨
        cpv_main = notice_data.get("CPV", {})
        if isinstance(cpv_main, dict):
            main_cpv = cpv_main.get("code", "")
        else:
            main_cpv = str(cpv_main)

        if main_cpv:
            if main_cpv.startswith("45"):  # ê±´ì„¤
                return TenderType.WORKS
            elif main_cpv.startswith(("03", "09", "15", "16", "18", "19", "33", "35")):  # ë¬¼í’ˆ
                return TenderType.GOODS
            else:  # ì„œë¹„ìŠ¤
                return TenderType.SERVICES

        return TenderType.SERVICES  # ê¸°ë³¸ê°’

    def _determine_tender_status(self, notice_data: Dict, deadline: Optional[datetime]) -> TenderStatus:
        """TED ê³µê³  ìƒíƒœ ê²°ì •"""
        if deadline and deadline < datetime.now():
            return TenderStatus.CLOSED

        # ê³„ì•½ ì²´ê²° ê³µê³ ì¸ì§€ í™•ì¸
        doc_type = notice_data.get("NC", "")
        if "award" in doc_type.lower() or "contract" in doc_type.lower():
            return TenderStatus.AWARDED

        return TenderStatus.ACTIVE

    def _parse_tender_value(self, notice_data: Dict) -> Optional[TenderValue]:
        """TED ì…ì°° ê¸ˆì•¡ ì •ë³´ íŒŒì‹±"""
        try:
            # ë‹¤ì–‘í•œ í•„ë“œì—ì„œ ê¸ˆì•¡ ì •ë³´ ì°¾ê¸°
            val_fields = ["VAL", "VL", "EST_VAL"]

            for field in val_fields:
                val_data = notice_data.get(field)
                if val_data:
                    if isinstance(val_data, dict):
                        amount = val_data.get("amount") or val_data.get("value")
                        currency = val_data.get("currency", "EUR")
                    elif isinstance(val_data, (int, float)):
                        amount = val_data
                        currency = "EUR"
                    else:
                        continue

                    if amount and amount > 0:
                        return TenderValue(
                            amount=float(amount),
                            currency=CurrencyCode.EUR if currency == "EUR" else currency,
                            vat_included=False
                        )

        except Exception as e:
            logger.warning(f"âš ï¸ TED ê¸ˆì•¡ íŒŒì‹± ì‹¤íŒ¨: {e}")

        return None

    def _parse_cpv_codes(self, notice_data: Dict) -> List[Classification]:
        """TED CPV ì½”ë“œ íŒŒì‹±"""
        classifications = []

        try:
            # ë©”ì¸ CPV ì½”ë“œ
            main_cpv = notice_data.get("CPV")
            if main_cpv:
                if isinstance(main_cpv, dict):
                    code = main_cpv.get("code", "")
                    desc = main_cpv.get("text", "")
                else:
                    code = str(main_cpv)
                    desc = ""

                if code:
                    classifications.append(Classification(
                        scheme="CPV",
                        code=code,
                        description=desc
                    ))

            # ì¶”ê°€ CPV ì½”ë“œë“¤
            additional_cpvs = notice_data.get("ADDITIONAL_CPV", [])
            if isinstance(additional_cpvs, list):
                for cpv in additional_cpvs:
                    if isinstance(cpv, dict):
                        code = cpv.get("code", "")
                        desc = cpv.get("text", "")
                        if code:
                            classifications.append(Classification(
                                scheme="CPV",
                                code=code,
                                description=desc
                            ))

        except Exception as e:
            logger.warning(f"âš ï¸ TED CPV íŒŒì‹± ì‹¤íŒ¨: {e}")

        return classifications

    def _extract_description(self, notice_data: Dict) -> Optional[str]:
        """TED ê³µê³ ì—ì„œ ì„¤ëª… ì¶”ì¶œ"""
        description_parts = []

        # ë‹¤ì–‘í•œ ì„¤ëª… í•„ë“œ í™•ì¸
        desc_fields = ["DS", "SHORT_DESCR", "OBJECT_DESCR"]

        for field in desc_fields:
            desc = notice_data.get(field, "")
            if desc and isinstance(desc, str):
                desc = desc.strip()
                if desc and desc not in description_parts:
                    description_parts.append(desc)

        return " ".join(description_parts) if description_parts else None


    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.session and not self.session.closed:
            await self.session.close()